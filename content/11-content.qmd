---
title: "Measurement from a Causal Perspective"
date: "2024-MAY-20"
bibliography: /Users/joseph/GIT/templates/bib/references.bib
editor_options: 
  chunk_output_type: console
format:
  html:
    warnings: FALSE
    error: FALSE
    messages: FALSE
    code-overflow: scroll
    highlight-style: oblivion
    code-tools:
      source: true
      toggle: FALSE
html-math-method: katex
reference-location: margin
citation-location: margin
cap-location: margin
code-block-border-left: true
---


```{r}
#| include: false
#| echo: false
#read libraries

library("tinytex")
library(extrafont)
loadfonts(device = "all")

library("margot")
library("tidyverse")

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
source("/Users/joseph/GIT/templates/functions/libs2.R")

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
#source("/Users/joseph/GIT/templates/functions/funs.R")

# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI
source("/Users/joseph/GIT/templates/functions/experimental_funs.R")
```


::: {.callout-note}
**Required**
- [@fischer2019primer] [link](https://www.dropbox.com/scl/fi/1h8slzy3vzscvbtp6yrjh/FischeKarlprimer.pdf?rlkey=xl93d5y7280c1qjhn3k2g8qls&dl=0)

##### Optional Readings 
- [@Vijver2021CulturePsychology] [link](https://doi.org/10.1017/9781107415188)
- [@he2012] [link](https://www.dropbox.com/scl/fi/zuv4odmxbz8dbtdjfap3e/He-BiasandEquivalence.pdf?rlkey=wezprklb4jm6rgvvx0g58nw1n&dl=0ā)
- [@Harkness2003TRANSALTION] [link](https://www.dropbox.com/scl/fi/hmmje9vbunmcu3oiahaa5/Harkness_CC_translation.pdf?rlkey=6vqq3ap5n52qp7t1e570ubpgt&dl=0)
:::

::: {.callout-important}
## Key concepts
- EFA
- CFA
- Multigroup CFA
- Invariance Testing
:::

::: {.callout-important}
- You need to know these measurement concepts
:::


#### Readings

- [@fischer2019primer] [link](https://www.dropbox.com/scl/fi/1h8slzy3vzscvbtp6yrjh/FischeKarlprimer.pdf?rlkey=xl93d5y7280c1qjhn3k2g8qls&dl=0)

##### Optional Readings 

- [@Vijver2021CulturePsychology] [link](https://doi.org/10.1017/9781107415188)
- [@he2012] [link](https://www.dropbox.com/scl/fi/zuv4odmxbz8dbtdjfap3e/He-BiasandEquivalence.pdf?rlkey=wezprklb4jm6rgvvx0g58nw1n&dl=0ā)
- [@Harkness2003TRANSALTION] [link](https://www.dropbox.com/scl/fi/hmmje9vbunmcu3oiahaa5/Harkness_CC_translation.pdf?rlkey=6vqq3ap5n52qp7t1e570ubpgt&dl=0)

#### Lab

- R exercises focusing on measurement theory applications and graphing


## Overview
By the conclusion of our session, you will gain proficiency in key areas of factor analysis.

Specifically, you will learn how to perform:

- Exploratory factor analysis,
- Confirmatory factor analysis (CFA),
- Multigroup CFA,
- Assess partial invariance,
- And understand configural, metric, and scalar equivalence through a practical example.



## Focus on Kessler-6 Anxiety

The code below will:

-   Load required packages.
-   Select the Kessler 6 items
-   Check whether there is sufficient correlation among the variables to support factor analysis.

#### 1. Select Scale


```{r}
#| label: cfa_factor_structure

# get synthetic data
library(margot)
library(tidyverse)
library(performance)
# select the columns we need. 
dt_only_k6 <- df_nz |> 
  filter(wave == 2018) |> 
  select(
    kessler_depressed,
    kessler_effort,
    kessler_hopeless,
    kessler_worthless,
    kessler_nervous,
    kessler_restless
  )


# check factor structure
performance::check_factorstructure(dt_only_k6)

```


- start with `df_nz` synthetic dataset. 
- take items from the Kessler-6 (K6) scale: depressed, effort, hopeless, worthless, nervous, and restless.
- Do the items in this scale cohere? If so, how?

##### Code and Analysis:

We employ `performance::check_factorstructure()` to evaluate the data's suitability for factor analysis. Two tests are reported:

a. **Bartlett's Test of Sphericity**

This test verifies if the correlation matrix significantly deviates from an identity matrix, an indicator that the variables are interrelated and suitable for factor analysis. The outcome:

- **Chi-square (Chisq)**: 50402.32 with **Degrees of Freedom (15)** and a **p-value** < .001

This highly reliable result (p < .001) confirms that the observed correlation matrix is not an identity matrix, substantiating the factorability of the dataset.

b. **Kaiser-Meyer-Olkin (KMO) Measure**

The KMO test measures sampling adequacy by comparing the magnitudes of observed correlation coefficients to those of partial correlation coefficients. A KMO value nearing 1 indicates appropriateness for factor analysis. The results are:

- **Overall KMO**: 0.87
  - This value suggests good sampling adequacy, indicating that the sum of partial correlations is relatively low compared to the sum of correlations, thus supporting the potential for distinct and reliable factors.

Each item’s KMO value exceeds the acceptable threshold of 0.5,so suitabile for factor analysis.


### 2. Explore Factor Structure 

The following R code allows us to perform exploratory factor analysis (EFA) on the Kessler 6 (K6) scale data, assuming three latent factors. 
```{r}
#| label: efa_made_easy
# exploratory factor analysis
# explore a factor structure made of 3 latent variables
library(psych)
efa <- psych::fa(dt_only_k6, nfactors = 3) |>
  model_parameters(sort = TRUE, threshold = "max")

print( efa )
```

### 2. Explore Factor Structure

```{r}
library(psych)
efa <- psych::fa(dt_only_k6, nfactors = 3) |>
  model_parameters(sort = TRUE, threshold = "max")
print(efa)
```

#### Concept of Rotation in Factor Analysis

In factor analysis, rotation is a mathematical technique applied to the factor solution to make it more interpretable. 

The ourpose of rotation is to redistribute the variance among the factors, aiming to achieve a simpler and more meaningful structure that aligns more closely with theoretical or empirical expectations. There are two types:

**Orthogonal rotations** (such as Varimax), which assume that the factors are uncorrelated and keep the axes at 90 degrees to each other. This is useful when we assume that the underlying factors are independent.

**Oblique rotations** (such as Oblimin), which allow the factors to correlate. This is more realistic in psychological and social sciences, and that's what we use...

#### Results 


This factor analysis uses oblimin rotation, and the items loaded as follows on the three factors:

- **MR3**: Strongly associated with 'kessler_hopeless' (0.79) and 'kessler_worthless' (0.79). This factor might be capturing aspects related to feelings of hopelessness and worthlessness, often linked with depressive affect.
- **MR1**: Mostly linked with 'kessler_depressed' (0.99), suggesting this factor represents core depressive symptoms.
- **MR2**: Includes 'kessler_restless' (0.72), 'kessler_nervous' (0.43), and 'kessler_effort' (0.38). This factor seems to encompass symptoms related to anxiety and agitation.

The **complexity** values indicate the number of factors each item loads on "significantly."  A complexity near 1.00 suggests that the item predominantly loads on a single factor, which is seen with most of the items except for 'kessler_nervous' and 'kessler_effort', which show higher complexity and thus share variance with more than one factor.

**Uniqueness** values represent the variance in each item not explained by the common factors. Lower uniqueness values for items like 'kessler_depressed' indicate that the factor explains most of the variance for that item.

#### Variance Explained

The three factors together account for 62.94% of the total variance in the data, distributed as follows:
- **MR3**: 28.20%
- **MR1**: 17.56%
- **MR2**: 17.18%

This indicates a substantial explanation of the data’s variance by the model, with the highest contribution from the factor associated with hopelessness and worthlessness.


So, the results indicate that the first two factors (MR1 and MR2) capture the dominant themes of psychological distress measured by the K6 scale, with depressive symptoms and anxiety being particularly prominent. The third factor (MR3), although explaining a smaller portion of the variance, suggests other dimensions of distress that may be relevant in specific contexts.

#### Consensus View?

There are different algorithms for assessing the factor structure.  The performance package allows us to consider a 'consensus' view.


```{r}
#| label: plot_factors
#| code-fold: true
#| column: page-right


n <-n_factors(dt_only_k6)
n
# plot
plot(n) + theme_classic()
```

Output: 

> The choice of 1 dimensions is supported by 8 (50.00%) methods out of 16 (Optimal coordinates, Acceleration factor, Parallel analysis, Kaiser criterion, Scree (SE), Scree (R2), VSS complexity 1, Velicer's MAP). 


The result indicates that a single dimension is supported by half of the methods used (8 out of 16).  However science isn't a matter of voting, so let's press on. 


### Confirmatory Factor Analysis (ignoring groups)

CFA to validate the hypothesised factor structures derived from EFA. 

- **One-factor model**: assumes all items measure a single underlying construct.
- **Two-factor model**: assumes two distinct constructs measured by the items.
- **Three-factor model**: assumes three distinct constructs measured by the items.

his is what we do:

#### 1. Data Partition

First, we take the dataset (`dt_only_k6`) and partition it into training and testing sets. This division helps in validating the model built on the training data against an unseen test set; this enhances robustness for the factor analysis findings.

```{r}
part_data <- datawizard::data_partition(dt_only_k6, training_proportion = .7, seed = 123)
training <- part_data$p_0.7
test <- part_data$test
```

#### 2. Model Setup for CFA

Bases on the EFA results, we consider three different factor structures

- **One-factor model**: assumes all items measure a single underlying construct.
- **Two-factor model**: assumes two distinct constructs measured by the items.
- **Three-factor model**: assumes three distinct constructs measured by the items.

We fit each model to the training data:

```{r}
# One-factor model
structure_k6_one <- psych::fa(training, nfactors = 1) |>
  efa_to_cfa()

# Two-factor model
structure_k6_two <- psych::fa(training, nfactors = 2) |>
  efa_to_cfa()

# Three-factor model
structure_k6_three <- psych::fa(training, nfactors = 3) %>%
  efa_to_cfa()
```


Then we split our data for cross-validation

```{r}
#| label: cfa
# first partition the data 
part_data <- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = 123)


# set up training data
training <- part_data$p_0.7
test <- part_data$test


# one factor model
structure_k6_one <- psych::fa(training, nfactors = 1) |>
  efa_to_cfa()

# two factor model model
structure_k6_two <- psych::fa(training, nfactors = 2) |>
  efa_to_cfa()

# three factor model
structure_k6_three <- psych::fa(training, nfactors = 3) %>%
  efa_to_cfa()

# inspect models
structure_k6_one
structure_k6_two
structure_k6_three
```

- **One-Factor Model**: All items are linked to a single factor (`MR1`).
  
- **Two-Factor Model**: 
  - `MR1` is linked with `kessler_depressed`, `kessler_hopeless`, and `kessler_worthless`, suggesting these items might represent a more depressive aspect of **distress.**
  - `MR2` is associated with `kessler_effort`, `kessler_nervous`, and `kessler_restless`, which could indicate a different aspect, perhaps related to **anxiety or agitation.**

- **Three-Factor Model**: 
  - `MR1` includes `kessler_depressed`, `kessler_effort`, `kessler_hopeless`, and `kessler_worthless`, indicating a broad factor possibly encompassing overall distress.
  - `MR2` consists solely of `kessler_effort`.
  - `MR3` includes `kessler_nervous` + `kessler_restless`, which might imply these are distinctivene from other distress components.
  

Do these results make sense?
  
  
Next we perform the confirmatory factor analysis itself...

```{r}
#| label: fit-models
# fit and compare models

# one latent model
one_latent <-
  suppressWarnings(lavaan::cfa(structure_k6_one, data = test))

# two latents model
two_latents <-
  suppressWarnings(lavaan::cfa(structure_k6_two, data = test))

# three latents model
three_latents <-
  suppressWarnings(lavaan::cfa(structure_k6_three, data = test))


# compare models
compare <-
  performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)

# select cols we want
key_columns_df <- compare[, c("Model", "Chi2", "Chi2_df", "CFI", "RMSEA", "RMSEA_CI_low", "RMSEA_CI_high", "AIC", "BIC")]

# view as html table
as.data.frame(key_columns_df) |>
  kbl(format = "markdown")
```

####  Metrics:

- **Chi2 (Chi-Square Test)**: A lower Chi2 value indicates a better fit of the model to the data.
- **df (Degrees of Freedom)**: Reflects the model complexity.
- **CFI (Comparative Fit Index)**: Values closer to 1 indicate a better fit. A value above 0.95 is generally considered to indicate a good fit.
- **RMSEA (Root Mean Square Error of Approximation)**: values less than 0.05 indicate a good fit, and values up to 0.08 are acceptable.
- **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: lower values are better, indicating a more parsimonious model, with the BIC imposing a penalty for model complexity.

#### Model Selection:

What do you think? 

- The **Three Latents Model** shows the best fit across all indicators, having the lowest Chi2, RMSEA, and the highest CFI. It also has the lowest AIC and BIC scores, suggesting it not only fits well but is also the most parsimonious among the tested models. 

But... 

- **The CFI** for the two-factor model is 0.990, which is close to 1 and suggests a very good fit to the data. This is superior to the one-factor model (CFI = 0.9468) and slightly less than the three-factor model (CFI = 0.9954). A CFI value closer to 1 indicates a better fit of the model to the data.

- **Root Mean Square Error of Approximation (RMSEA):** The two-factor model has an RMSEA of 0.0443, which is within the excellent fit range (below 0.05). It significantly improves upon the one-factor model's RMSEA of 0.0985 and is only slightly worse than the three-factor model's 0.0313.

- **BIC* isn't much different, So

We might say the two-factor model strikes a balance between simplicity and model fit. It has fewer factors than the three-factor model, making it potentially easier to interpret while still capturing the variance in the data.

Look at the items. What do you think? 

Does Anxiety appear to differ from Depression? 

### Multi-group Confirmatory Factor Analysis

This script runs multi-group confirmatory factor analysis (MG-CFA) to assess the invariance of the Kessler 6 (K6) distress scale across two ethnic groups: European New Zealanders and Māori.

```{r}
#| label: group_by_cfa

# select needed columns plus 'ethnicity'
# filter dataset for only 'euro' and 'maori' ethnic categories
dt_eth_k6_eth <- df_nz |> 
  filter(wave == 2018) |> 
  filter(eth_cat == "euro" | eth_cat == "maori") |> 
  select(kessler_depressed, kessler_effort, kessler_hopeless,
         kessler_worthless, kessler_nervous, kessler_restless, eth_cat)

# partition the dataset into training and test subsets
# stratify by ethnic category to ensure balanced representation
part_data_eth <- datawizard::data_partition(dt_eth_k6_eth, training_proportion = .7, seed = 123, group = "eth_cat")

training_eth <- part_data_eth$p_0.7
test_eth <- part_data_eth$test

# configural invariance models
#run CFA models specifying one, two, and three latent variables without constraining across groups
one_latent_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_one, group = "eth_cat", data = test_eth))
two_latents_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_two, group = "eth_cat", data = test_eth))
three_latents_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_three, group = "eth_cat", data = test_eth))

# compare model performances for configural invariance
compare_eth_configural <- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)
compare_eth_configural
# metric invariance models
# run CFA models holding factor loadings equal across groups
one_latent_eth_metric <- suppressWarnings(lavaan::cfa(structure_k6_one, group = "eth_cat", group.equal = "loadings", data = test_eth))
two_latents_eth_metric  <- suppressWarnings(lavaan::cfa(structure_k6_two, group = "eth_cat", group.equal = "loadings", data = test_eth))
three_latents_eth_metric  <- suppressWarnings(lavaan::cfa(structure_k6_three, group = "eth_cat",group.equal = "loadings", data = test_eth))

# compare model performances for metric invariance
compare_eth_metric  <- performance::compare_performance(one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric, verbose = FALSE)

# scalar invariance models
# run CFA models holding factor loadings and intercepts equal across groups
one_latent_eth_scalar <- suppressWarnings(lavaan::cfa(structure_k6_one, group = "eth_cat", group.equal = c("loadings","intercepts"), data = test_eth))
two_latents_eth_scalar  <- suppressWarnings(lavaan::cfa(structure_k6_two, group = "eth_cat", group.equal =  c("loadings","intercepts"), data = test_eth))
three_latents_eth_scalar  <- suppressWarnings(lavaan::cfa(structure_k6_three, group = "eth_cat",group.equal =  c("loadings","intercepts"), data = test_eth))

# Compare model performances for scalar invariance
compare_eth_scalar <- compare_eth_scalar  <- performance::compare_performance(one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar, verbose = FALSE)
```
The concepts of *configural*, *metric*, and *scalar invariance* relate to the comparability of a measurement instrument, such as a survey or test, across different groups.


1.  **Configural invariance** refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying or "latent" constructs (factors) are defined the same way for different groups. This doesn't mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.

In the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.

2.  **Metric invariance** (also known as "weak invariance") refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.

If metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.

3.  **Scalar invariance** (also known as "strong invariance") involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.

In the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.

Note: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.

### Configural invariance

```{r}
compare_eth_configural

compare_eth_configural_key <- compare_eth_configural[, c("Name", "Chi2", "Chi2_df","RFI", "NNFI", "CFI","GFI","RMSEA", "RMSEA_CI_low", "RMSEA_CI_high", "AIC", "BIC")]

as.data.frame(compare_eth_configural_key)|>
  kbl(format = "markdown")

```

The table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It's the most basic form of measurement invariance.

Looking at the results, we can draw the following conclusions:

1.  **Chi2 (Chi-square)**: a lower value suggests a better model fit. In this case, the three looks best 

2.  **GFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index)**: These values range from 0 to 1, with values closer to 1 suggesting a better fit. All models are close. 

3.  **NFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index)**: These range from 0 to 1, with values closer to 1 suggesting a better fit. The two and three factors models have the highest values.

4.  **RMSEA (Root Mean Square Error of Approximation)**: lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable.Only two and three meet this threshold.

5.  **RMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual)**: three is best.

6.  **RFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index)**: These range from 0 to 1, with values closer to 1 suggesting a better fit. Again three is the winner.

7.  **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: Three again....

### Analysis of the Results:
- **One Latent Model**: shows the poorest fit among the models with a high RMSEA and the lowest CFI. The model's Chi-squared value is also significantly high, indicating a substantial misfit with the observed data.

- **Two Latents Model**: displays a much improved fit compared to the one latent model, as evident from its much lower Chi-squared value, lower RMSEA, and higher CFI. This suggests that two factors might be necessary to adequately represent the underlying structure in the data.

- **Three Latents Model**: provides the best fit metrics among the three configurations...

### Metric Equivalence

```{r}
compare_eth_metric <- compare_eth_metric[, c("Name", "Chi2", "Chi2_df","RFI", "NNFI", "CFI","GFI","RMSEA", "RMSEA_CI_low", "RMSEA_CI_high", "AIC", "BIC")]

as.data.frame(compare_eth_metric)|>
  kbl(format = "markdown")
```

This table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as weak measurement invariance) across different ethnic categories (eth_cat).  

Three wins again...but not by much.


### Scalar Equivalence

```{r}
# view as html table

compare_eth_scalar <- compare_eth_scalar[, c("Name", "Chi2", "Chi2_df","RFI", "NNFI", "CFI","GFI","RMSEA", "RMSEA_CI_low", "RMSEA_CI_high", "AIC", "BIC")]

as.data.frame(compare_eth_scalar)|>
  kbl(format = "markdown")
```

Overall it seems that we have good evidence for the three-factor model of Kessler-6, but two-factor is close.


### Lab assignment

Using the code above, perform MG-CFA on personality measures using the `df_nz` data set. 




### Packages

```{r}
report::cite_packages()
```


