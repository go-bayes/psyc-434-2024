{
  "hash": "8d41ecc332a08786b2af392e2cdf799b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Template: Stratified Causal Estimatation\"\ndate: \"2023-JUNE-06\"\nauthor:\n    name: Joseph Bulbulia\n    orcid: 0000-0002-5861-2056\n    affiliation: Victoria University of Wellington, New Zealand\n    email: joseph.bulbulia@vuw.ac.nz\n    corresponding: yes\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\nformat:\n  html:\n    warnings: FALSE\n    error: FALSE\n    messages: FALSE\n    code-overflow: scroll\n    highlight-style: oblivion\n    code-tools:\n      source: true\n      toggle: FALSE\nhtml-math-method: katex\nreference-location: margin\ncitation-location: margin\ncap-location: margin\ncode-block-border-left: true\n---\n\n\n\n\nYour step-by-step guide to reporting is as follows: \n\n\n## Intoduction\n\nAnswer the following:\n\n-   State the Question: is my question clearly stated? If not, state it.\n-   Relevance of the Question: Have I explained its importance? If not, explain.\n-   Subgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\n-   Causality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\n-   State how you will use time-series data to address causality.\n-   Define your exposure.\n-   Define your outcome(s)\n-   Explain how the the exposure and outcome is relevant to your question.\n-   Define your causal estimand (see: lecture 8). Hint: it is ATE_g\\_risk difference = E\\[Y(1)-(0)\\|G,L\\], where G is your multiple-group indicator and L is your set of baseline confounders.\n\n## Method\n-   Consider any ethical implications.\n-   Explain the sample. Provide descriptive statistics\n-   Discuss inclusion criteria.\n-   Discuss how your sample relates to the \"source population\" (lecture 8.)\n-   Explain NZAVS measures. State the questions used in the items\n-   In your own words describe how the data meet the following assumptions required for causal inference:\n-   Positivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\n-   Consistency: Can I interpret what it means to intervene on the exposure?\n-   Exchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, **you must explain why the baseline measure of your exposure and outcome are included as potential confounders.**\n-   Note: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\n-   Draw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\n-   Measurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\n-   State how you will handle missing data\n-   State what your estimator will be. Note I've given you the following text to modify:\n\n> The Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n> Step 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n> Step 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n> Step 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n> Step 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n> Step 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\nOR if you decide to use machine learning: \n\n> We perform statistical estimation using semi-parametric Targeted Learning, specifically a Targeted Minimum Loss-based Estimation (TMLE) estimator. TMLE is a robust method that combines machine learning techniques with traditional statistical models to estimate causal effects while providing valid statistical uncertainty measures for these estimates [@van2014targeted; @van2012targeted].\n\n> TMLE operates through a two-step process that involves modelling both the outcome and treatment (exposure). Initially, TMLE employs machine learning algorithms to flexibly model the relationship between treatments, covariates, and outcomes. This flexibility allows TMLE to account for complex, high-dimensional covariate spaces efficiently without imposing restrictive model assumptions [@van2014discussion; @vanderlaan2011; @vanderlaan2018]. The outcome of this step is a set of initial estimates for these relationships.\n\n> The second step of TMLE involves \"targeting\" these initial estimates by incorporating information about the observed data distribution to improve the accuracy of the causal effect estimate. TMLE achieves this precision through an iterative updating process, which adjusts the initial estimates towards the true causal effect. This updating process is guided by the efficient influence function, ensuring that the final TMLE estimate is as close as possible, given the measures and data, to the targeted causal effect while still being robust to model-misspecification in either the outcome or the treatment model [@van2014discussion].\n\n> Again, a central feature of TMLE is its double-robustness property. If either the treatment model or the outcome model is correctly specified, the TMLE estimator will consistently estimate the causal effect. Additionally, we used cross-validation to avoid over-fitting, following the pre-stated protocols in @bulbulia2024PRACTICAL.  The integration of TMLE and machine learning technologies reduces the dependence on restrictive modelling assumptions and introduces an additional layer of robustness. For further details of the specific targeted learning strategy we favour, see [@hoffman2022; @hoffman2023; @d√≠az2021]. We perform estimation using the `lmtp` package [@williams2021]. We used the `superlearner` library for semi-parametric estimation with the predefined libraries `SL.ranger`, `SL.glmnet`, and `SL.xgboost` [@polley2023; @xgboost2023; @Ranger2017]. We created graphs, tables and output reports using the `margot` package [@margot2024]. \n\n\n\n<!-- Also see the appendix [here](https://go-bayes.github.io/psych-434-2023/content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2) -->\n\n\n\n\n-   State what E-values are (lecture 9) and how you will use Evalues to clarify the risk of unmeasured confounding.\n\n\n\n## Results\n\n-   Use the scripts I have provided as a template for your analysis.\n-   Propensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: `A ~ x1 + x2 + x3 + ....` using logistic regression, all continuous predictors were transformed to z-scores\n    -   WeightIt Package Utilisation: Explicitly mention the use of the 'WeightIt' package in R, including any specific options or parameters used in the propensity score estimation process [@greifer2023a].\n    -   Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as 'ebal', 'energy', and 'ps'.\n    -   If your exposure is continuous only the 'energy' option was used for propensity score estimation.\n    -   Subgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\n    -   Covariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\n    -   Weighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\nExample:\n\n> We estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the `subgroup` command of the `WeightIt` package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: \"ebal\" or entropy balancing, \"energy\" or energy balancing, and \"ps\" or traditional inverse probability of weighting balancing. Of these methods \"ebal\" performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n-   Interpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\n\n-   Outcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n> We fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup, which was also interacted with the exposure and baseline covariates. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the *Method* section. We calculated confidence intervals and standard errors, using the `clarify` package in R, which relies on simulation based inference for these quantities of interest [@greifer2023]\n\n-   Report the causal estimates.\n    -   ATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a\\*\n    -   differences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n-   Report the E-value: how sensitive are your results to unmeasured confounding? Hint: see the code below. I've substantially automated this task.\n\n\n## Discussion\n\nMake sure to hit these points:\n\nConsider the following ideas about what to discuss in one's findings. The order of exposition might be different.\n\n1.  **Summary of results**: What did you find?\n\n2.  **Interpretation of E-values:** Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\n\n3.  **Causal Effect Interpretation:** What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\n\n4.  **Comparison of Subgroups:** Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\n\n5.  **Uncertainty and Confidence Intervals:** Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\n\n6.  **Generalisability and Transportability:** Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\n\n7.  **Assumptions and Limitations:** Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\n\n8.  **Theoretical Relevance**: How are these findings relevant to existing theories.\n\n9.  **Replication and Future Research:** Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\n\n10. **Real-world Implications:** Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research.\n\n## Example anlaysis (week 10)\n\n## Load Libraries\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions\nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# # experimental functions (more functions)\n# source(\n#   \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n# )\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n```\n:::\n\n\n\n<!-- ### Import data -->\n\n<!-- ```{r} -->\n<!-- #| label: import_data -->\n<!-- # This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data. -->\n\n<!-- nzavs_synth <- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\")) -->\n\n<!-- ``` -->\n\n<!-- ### Find column names -->\n\n<!-- ```{r} -->\n<!-- #| label: colnames -->\n<!-- #| echo: false -->\n<!-- #| eval: false -->\n<!-- #| code-fold: true -->\n<!-- ## use colnames to inspect the variables -->\n\n<!-- colnames(nzavs_synth) -->\n\n<!-- # more about the variables here:  -->\n<!-- # https://github.com/go-bayes/psych-434-2023/blob/main/data/readme.qmd -->\n<!-- ``` -->\n\n<!-- ### Transform indicators -->\n\n<!-- -   What is the effect of exercise as measured by the NZAVS exercise scale on (1) depression and (2) anxiety. -->\n\n<!-- ```{r} -->\n<!-- #| label: data_wrangling -->\n\n<!-- # questions are: -->\n<!--       # kessler_depressed, -->\n<!--       # # During the last 30 days, how often did you feel so depressed that nothing could cheer you up? -->\n<!--       # kessler_hopeless, -->\n<!--       # # During the last 30 days, how often did you feel hopeless? -->\n<!--       # kessler_nervous, -->\n<!--       # # During the last 30 days, how often did you feel nervous? -->\n<!--       # kessler_effort, -->\n<!--       # # During the last 30 days, how often did you feel that everything was an effort? -->\n<!--       # kessler_restless, -->\n<!--       # # During the last 30 days, how often did you feel restless or fidgety ? -->\n<!--       # kessler_worthless  # During the last 30 days, how often did you feel worthless? -->\n<!-- dt_start <- nzavs_synth %>% -->\n<!--   arrange(id, wave) %>% -->\n<!--   rowwise() %>% -->\n<!--   mutate( -->\n<!--     # see week 10 appendix 1 for a detailed explanation of how we obtain these 2 x factors -->\n<!--     kessler_latent_depression = mean( -->\n<!--       c(kessler_depressed, kessler_hopeless, kessler_effort), -->\n<!--       na.rm = TRUE -->\n<!--     ), -->\n<!--     kessler_latent_anxiety  = mean(c( -->\n<!--       kessler_effort, kessler_nervous, kessler_restless -->\n<!--     ), na.rm = TRUE) -->\n<!--   ) |> -->\n<!--   ungroup() |> -->\n<!--   # Coarsen 'hours_exercise' into categories -->\n<!--   mutate( -->\n<!--     hours_exercise_coarsen = cut( -->\n<!--       hours_exercise, -->\n<!--       # Hours spent exercising/ physical activity -->\n<!--       breaks = c(-1, 3, 8, 200), -->\n<!--       labels = c(\"inactive\", -->\n<!--                  \"active\", -->\n<!--                  \"very_active\"), -->\n<!--       # Define thresholds for categories -->\n<!--       levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"), -->\n<!--       ordered = TRUE -->\n<!--     ) -->\n<!--   ) |> -->\n<!--   # Create a binary 'urban' variable based on the 'rural_gch2018' variable -->\n<!--   mutate(urban = factor( -->\n<!--     ifelse( -->\n<!--       rural_gch2018 == \"medium_urban_accessibility\" | -->\n<!--         # Define urban condition -->\n<!--         rural_gch2018 == \"high_urban_accessibility\", -->\n<!--       \"urban\", -->\n<!--       # Label 'urban' if condition is met -->\n<!--       \"rural\"  # Label 'rural' if condition is not met -->\n<!--     ) -->\n<!--   )) |>   # for continuous exposure -- see appendix -->\n<!--   mutate(hours_exercise = ifelse( hours_exercise < 0, 0, hours_exercise)) |>  -->\n<!--   mutate(hours_exercise_log = log( hours_exercise + 1 )) |>  -->\n<!--   mutate(hours_exercise_log_round = round(hours_exercise_log, 0)) -->\n\n<!-- ``` -->\n\n<!-- ### Inspect your data -->\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- #| code-fold: true -->\n\n<!-- # do some checks -->\n<!-- levels(dt_start$hours_exercise_coarsen) -->\n<!-- table(dt_start$hours_exercise_coarsen) -->\n<!-- max(dt_start$hours_exercise_log) -->\n<!-- min(dt_start$hours_exercise) -->\n\n<!-- # checks -->\n\n\n<!-- # justification for transforming exercise\" has a very long tail -->\n<!-- hist(dt_start$hours_exercise, breaks = 1000) -->\n<!-- # consider only those cases below < or = to 20 -->\n<!-- hist(subset(dt_start, hours_exercise <= 20)$hours_exercise) -->\n<!-- hist(as.numeric(dt_start$hours_exercise_coarsen)) -->\n\n\n<!-- # suppose you use a continuous variable -->\n<!-- mean(dt_start$hours_exercise, na.rm = TRUE) -->\n<!-- max(dt_start$hours_exercise, na.rm = TRUE) -->\n<!-- min(dt_start$hours_exercise, na.rm = TRUE) -->\n\n<!-- # Continuous variable -->\n<!-- min(dt_start$hour_exercise, na.rm = TRUE) -->\n<!-- min(dt_start$hour_exercise, na.rm = TRUE) -->\n<!-- max(dt_start$hour_exercise, na.rm = TRUE) -->\n<!-- mean(dt_start$hour_exercise, na.rm = TRUE) -->\n<!-- sd(dt_start$hour_exercise, na.rm = TRUE) # note wide standard deviation -->\n\n\n\n<!-- min(dt_start$hours_exercise_log, na.rm = TRUE) -->\n<!-- max(dt_start$hours_exercise_log, na.rm = TRUE) -->\n<!-- mean(dt_start$hours_exercise_log, na.rm = TRUE) -->\n<!-- sd(dt_start$hours_exercise_log, na.rm = TRUE) -->\n\n<!-- mean_log_ex <- mean(exp((dt_start$hours_exercise_log))-1) # recover original scale  -->\n<!-- sd_log_ex <- sd(exp(dt_start$hours_exercise_log)-1) # recover sd on original scale -->\n\n\n<!-- # for the continuous change, we would investigate an experiment in which everyone moved from  -->\n<!-- mean_log_ex # 5.88  -->\n\n<!-- # to  -->\n<!-- mean_log_ex + sd_log_ex # 13.12 -->\n\n\n<!-- # make negative value zero (can't have negative exercise) -->\n<!-- ``` -->\n\n<!-- ### Investigate assumption of positivity: -->\n\n<!-- Recall the positive assumption: -->\n\n<!-- **Positivity:** Can we intervene on the exposure at all levels of the covariates? -->\n\n\n<!-- ```{r} -->\n<!-- #| label: prepare_exposure_data -->\n<!-- #| eval: true -->\n\n<!-- #These are the data by wave, but they don't track who changed. -->\n\n<!-- #  select only the baseline year and the exposure year.  That will give us change in the exposure. () -->\n<!-- dt_exposure <- dt_start |> -->\n\n<!--   # select baseline year and exposure year -->\n<!--   filter(wave == \"2018\" | wave == \"2019\") |> -->\n\n<!--   # select variables of interest -->\n<!--   select(id, wave, hours_exercise_coarsen,  hours_exercise_log_round, eth_cat) |> -->\n\n<!--   # the categorical variable needs to be numeric for us to use msm package to investigate change -->\n<!--   mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |> -->\n<!--   droplevels() -->\n\n\n<!-- # check -->\n<!-- # dt_exposure |> -->\n<!-- #   tabyl(hours_exercise_coarsen_n, eth_cat,  wave ) -->\n<!-- ``` -->\n\n<!-- I've written a function called `transition_table` that will help us assess change in the exposure at the *individual level.* -->\n\n<!-- ```{r} -->\n<!-- #| label: transition_table_interp -->\n\n<!-- #   consider people going from active to vary active -->\n<!-- out <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure) -->\n\n\n<!-- # for a function I wrote to create state tables -->\n<!-- state_names <- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\") -->\n\n<!-- # transition table -->\n\n<!-- transition_table(out, state_names) -->\n<!-- ``` -->\n\n<!-- Next consider MƒÅori only -->\n\n<!-- ```{r} -->\n<!-- #| label: maori_only_transition -->\n<!-- # Maori only -->\n<!-- dt_exposure_maori <- dt_exposure |> -->\n<!--   filter(eth_cat == \"mƒÅori\") -->\n\n<!-- out_m <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori) -->\n\n<!-- # with this little support we might consider parametric models -->\n<!-- t_tab_m<- transition_table( out_m, state_names) -->\n\n<!-- #interpretation -->\n<!-- cat(t_tab_m$explanation) -->\n<!-- print(t_tab_m$table) -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: euro_only_transition -->\n\n<!-- # filter euro -->\n<!-- dt_exposure_euro <- dt_exposure |> -->\n<!--   filter(eth_cat == \"euro\") -->\n\n<!-- # model change -->\n<!-- out_e <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro) -->\n\n<!-- # creat transition table. -->\n<!-- t_tab_e <- transition_table( out_e, state_names) -->\n\n<!-- #interpretation -->\n<!-- cat(t_tab_e$explanation) -->\n\n<!-- # table -->\n<!-- print(t_tab_e$table) -->\n\n<!-- ``` -->\n\n<!-- Overall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation. -->\n\n<!-- ## Draw Dag -->\n\n<!-- ```{tikz} -->\n<!-- #| label: fig-dag-6 -->\n<!-- #| fig-cap: \"Causal graph: three-wave panel design\" -->\n<!-- #| out-width: 100% -->\n<!-- #| echo: true -->\n<!-- #| codefolding: true -->\n\n<!-- \\usetikzlibrary{positioning} -->\n<!-- \\usetikzlibrary{shapes.geometric} -->\n<!-- \\usetikzlibrary{arrows} -->\n<!-- \\usetikzlibrary{decorations} -->\n<!-- \\tikzstyle{Arrow} = [->, thin, preaction = {decorate}] -->\n<!-- \\tikzset{>=latex} -->\n\n<!-- \\begin{tikzpicture}[{every node/.append style}=draw] -->\n<!-- \\node [rectangle, draw=white] (U) at (0, 0) {U}; -->\n<!-- \\node [rectangle, draw=black, align=left] (L) at (2, 0) {t0/L \\\\t0/A \\\\t0/Y}; -->\n<!-- \\node [rectangle, draw=white] (A) at (4, 0) {t1/A}; -->\n<!-- \\node [ellipse, draw=white] (Y) at (6, 0) {t2/Y}; -->\n<!-- \\draw [-latex, draw=black] (U) to (L); -->\n<!-- \\draw [-latex, draw=black] (L) to (A); -->\n<!-- \\draw [-latex, draw=red, dotted] (A) to (Y); -->\n<!-- \\draw [-latex, bend left=50, draw =black] (L) to (Y); -->\n<!-- \\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y); -->\n<!-- \\draw [-latex, bend left=50, draw =black, dotted] (U) to (A); -->\n\n\n<!-- \\end{tikzpicture} -->\n<!-- ``` -->\n\n<!-- ### Create wide data frame for analysis -->\n\n<!-- I've written a function -->\n\n<!-- ```{r} -->\n<!-- #| table: wide_data -->\n<!-- ############## ############## ############## ############## ############## ############## ############## ######## -->\n<!-- ####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## ######### -->\n<!-- ############## ############## ############## ############## ############## ############## ############# ######### -->\n\n\n<!-- # I have created a function that will put the data into the correct shape. Here are the steps. -->\n\n<!-- # Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables. -->\n\n<!-- # Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these.  -->\n\n\n<!-- # here are some plausible baseline confounders -->\n<!-- baseline_vars = c( -->\n<!--   \"edu\", -->\n<!--   \"male\", -->\n<!--   \"eth_cat\", -->\n<!--   \"employed\", -->\n<!--   \"gen_cohort\", -->\n<!--   \"nz_dep2018\", -->\n<!--   \"nzsei13\", -->\n<!--   \"partner\", -->\n<!--   \"parent\", -->\n<!--   \"pol_orient\", -->\n<!--   \"urban\", -->\n<!--   \"agreeableness\", -->\n<!--   \"conscientiousness\", -->\n<!--   \"extraversion\", -->\n<!--   \"honesty_humility\", -->\n<!--   \"openness\", -->\n<!--   \"neuroticism\", -->\n<!--   \"modesty\", -->\n<!--   \"religion_identification_level\" -->\n<!-- ) -->\n\n\n<!-- ## Step 2, select the exposure variable.  This is the \"cause\" -->\n<!-- exposure_var = c(\"hours_exercise_coarsen\", \"hours_exercise_log\") -->\n\n\n<!-- ## step 3. select the outcome variable.  These are the outcomes. -->\n<!-- outcome_vars_reflective = c(\"kessler_latent_anxiety\", -->\n<!--                             \"kessler_latent_depression\") -->\n\n\n\n<!-- # the function \"create_wide_data\" should be in your environment. -->\n<!-- # If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below -->\n<!-- # source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\") -->\n\n<!-- dt_prepare <- -->\n<!--   create_wide_data( -->\n<!--     dat_long = dt_start, -->\n<!--     baseline_vars = baseline_vars, -->\n<!--     exposure_var = exposure_var, -->\n<!--     outcome_vars = outcome_vars_reflective -->\n<!--   ) -->\n\n\n<!-- # ignore warning -->\n\n<!-- ``` -->\n\n<!-- ## Descriptive table -->\n\n<!-- ```{r} -->\n<!-- #| label: simple_table -->\n<!-- #| eval: false -->\n<!-- #| code-fold: true -->\n<!-- # I have created a function that will allow you to take a data frame and -->\n<!-- # create a table -->\n<!-- baseline_table(dt_prepare, output_format = \"markdown\") -->\n\n<!-- # but it is not very nice. Next up, is a better table -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: table_better -->\n\n<!-- # get data into shape -->\n<!-- dt_new <- dt_prepare %>% -->\n<!--   select(starts_with(\"t0\")) %>% -->\n<!--   rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %>% -->\n<!--   mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |> -->\n<!--   janitor::clean_names(case = \"screaming_snake\") -->\n\n\n<!-- # create a formula string -->\n<!-- baseline_vars_names <- dt_new %>% -->\n<!--   select(-WAVE) %>% -->\n<!--   colnames() -->\n\n<!-- table_baseline_vars <- -->\n<!--   paste(baseline_vars_names, collapse = \"+\") -->\n\n<!-- formula_string_table_baseline <- -->\n<!--   paste(\"~\", table_baseline_vars, \"|WAVE\") -->\n\n<!-- table1::table1(as.formula(formula_string_table_baseline), -->\n<!--                data = dt_new, -->\n<!--                overall = FALSE) -->\n<!-- ``` -->\n\n<!-- We need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward. -->\n\n<!-- 1.  `mutate(id = factor(1:nrow(dt_prepare)))`: This creates a new column called `id` that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset. -->\n\n<!-- 2.  The next `mutate` operation is used to convert the `t0_eth_cat`, `t0_urban`, and `t0_gen_cohort` variables to factor type, if they are not already. -->\n\n<!-- 3.  The `filter` command is used to subset the dataset to only include rows where the `t0_eth_cat` is either \"euro\" or \"mƒÅori\". The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups. -->\n\n<!-- 4.  `ungroup()` ensures that there is no grouping in the dataframe. -->\n\n<!-- 5.  The `mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\"))` step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include \"\\_z\" at the end of their original names. -->\n\n<!-- 6.  The `select` function is used to keep only specific columns: the `id` column, any columns that are factors, and any columns that end in \"\\_z\". -->\n\n<!-- 7.  The `relocate` functions re-order columns. The first `relocate` places the `id` column at the beginning. The next three `relocate` functions order the rest of the columns based on their names: those starting with \"t0\\_\" are placed before \"t1\\_\" columns, and those starting with \"t2\\_\" are placed after \"t1\\_\" columns. -->\n\n<!-- 8.  `droplevels()` removes unused factor levels in the dataframe. -->\n\n<!-- 9.  Finally, `skimr::skim(dt)` will print out a summary of the data in the `dt` object using the skimr package. This provides a useful overview of the data, including data types and summary statistics. -->\n\n<!-- This function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0\\_, t1\\_, t2\\_, etc.). -->\n\n<!-- ```{r} -->\n<!-- ### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ### -->\n\n<!-- dt <- dt_prepare|> -->\n<!--   mutate(id = factor(1:nrow(dt_prepare))) |> -->\n<!--   mutate( -->\n<!--   t0_eth_cat = as.factor(t0_eth_cat), -->\n<!--   t0_urban = as.factor(t0_urban), -->\n<!--   t0_gen_cohort = as.factor(t0_gen_cohort) -->\n<!-- ) |> -->\n<!--   dplyr::filter(t0_eth_cat == \"euro\" | -->\n<!--                 t0_eth_cat == \"mƒÅori\") |> # Too few asian and pacific -->\n<!--   ungroup() |> -->\n<!--   # transform numeric variables into z scores (improves estimation) -->\n<!--   dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %>% -->\n<!--   # select only factors and numeric values that are z-scores -->\n<!--   select(id, # category is too sparse -->\n<!--          where(is.factor), -->\n<!--          ends_with(\"_z\"), ) |> -->\n<!--   # tidy data frame so that the columns are ordered by time (useful for more complex models) -->\n<!--   relocate(id, .before = starts_with(\"t1_\"))   |> -->\n<!--   relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |> -->\n<!--   relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |> -->\n<!--   droplevels() -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: more-data-checks -->\n<!-- #| eval: false -->\n\n<!-- # checks -->\n<!-- hist(dt$t2_kessler_latent_depression_z) -->\n<!-- hist(dt$t2_kessler_latent_anxiety_z) -->\n<!-- hist(dt$t1_hour_exercise_log_z) -->\n\n<!-- dt |> -->\n<!--   tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |> -->\n<!--   kbl(format = \"markdown\") -->\n\n<!-- # Visualise missingness -->\n<!-- naniar::vis_miss(dt) -->\n\n<!-- # save your dataframe for future use -->\n\n<!-- # make dataframe -->\n<!-- dt = as.data.frame(dt) -->\n\n<!-- # save data -->\n<!-- saveRDS(dt, here::here(\"data\", \"dt\")) -->\n\n<!-- ``` -->\n\n<!-- ### Calculate propensity scores -->\n\n<!-- Next we generate propensity scores. -->\n\n<!-- ```{r} -->\n<!-- #| label: propensity_score prepare -->\n\n<!-- # read  data -- you may start here if you need to repeat the analysis -->\n<!-- dt <- readRDS(here::here(\"data\", \"dt\")) -->\n\n<!-- # get column names -->\n<!-- baseline_vars_reflective_propensity <- dt|> -->\n<!--   dplyr::select(starts_with(\"t0\"), -t0_eth_cat, -t0_hours_exercise_log_z) |> colnames() -->\n\n<!-- # define our exposure -->\n<!-- X <- \"t1_hours_exercise_coarsen\" -->\n\n<!-- # define subclasses -->\n<!-- S <- \"t0_eth_cat\" -->\n\n<!-- # Make sure data is in a data frame format -->\n<!-- dt <- data.frame(dt) -->\n\n\n<!-- # next we use our trick for creating a formula string, which will reduce our work -->\n<!-- formula_str_prop <- -->\n<!--   paste(X, -->\n<!--         \"~\", -->\n<!--         paste(baseline_vars_reflective_propensity, collapse = \"+\")) -->\n\n<!-- # this shows the exposure variable as predicted by the baseline confounders. -->\n<!-- formula_str_prop -->\n<!-- ``` -->\n\n<!-- For propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance. -->\n\n<!-- I typically use `ps` (classical propensity scores), `ebal` and `energy`. The latter two in my experience yeild good balance. Also `energy` will work with *continuous* exposures. -->\n\n<!-- For more information, see <https://ngreifer.github.io/WeightIt/> -->\n\n<!-- ```{r} -->\n<!-- #| label: propensity_scores -->\n<!-- #| eval: false -->\n\n\n<!-- # traditional propensity scores-- note we select the ATT and we have a subgroup  -->\n<!-- dt_match_ps <- match_mi_general( -->\n<!--   data = dt, -->\n<!--   X = X, -->\n<!--   baseline_vars = baseline_vars_reflective_propensity, -->\n<!--   subgroup = \"t0_eth_cat\", -->\n<!--   estimand = \"ATE\", -->\n<!--   method = \"ps\" -->\n<!-- ) -->\n\n<!-- saveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\")) -->\n\n\n<!-- # ebalance -->\n<!-- dt_match_ebal <- match_mi_general( -->\n<!--   data = dt, -->\n<!--   X = X, -->\n<!--   baseline_vars = baseline_vars_reflective_propensity, -->\n<!--   subgroup = \"t0_eth_cat\", -->\n<!--   estimand = \"ATE\", -->\n<!--   method = \"ebal\" -->\n<!-- ) -->\n\n<!-- # save output -->\n<!-- saveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\")) -->\n\n\n\n<!-- ## energy balance method -->\n<!-- dt_match_energy <- match_mi_general( -->\n<!--   data = dt, -->\n<!--   X = X, -->\n<!--   baseline_vars = baseline_vars_reflective_propensity, -->\n<!--   subgroup = \"t0_eth_cat\", -->\n<!--   estimand = \"ATE\", -->\n<!--   #focal = \"high\", # for use with ATT -->\n<!--   method = \"energy\" -->\n<!-- ) -->\n<!-- saveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\")) -->\n<!-- ``` -->\n\n<!-- Results, first for Europeans -->\n\n<!-- ```{r} -->\n<!-- #| label: results_ps_analysis -->\n<!-- #|  -->\n<!-- #dt_match_energy <- readRDS(here::here(\"data\", \"dt_match_energy\")) -->\n<!-- dt_match_ebal <- readRDS(here::here(\"data\", \"dt_match_ebal\")) -->\n<!-- #dt_match_ps <- readRDS(here::here(\"data\", \"dt_match_ps\")) -->\n\n<!-- # next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2.  -->\n<!-- # note that if the variables are unlikely to influence the outcome we can be less strict.  -->\n\n<!-- #See: Hainmueller, J. 2012. ‚ÄúEntropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.‚Äù Political Analysis 20 (1): 25‚Äì46. https://doi.org/10.1093/pan/mpr025. -->\n\n<!-- # Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of -->\n<!-- # Epidemiology 2008; 168(6):656‚Äì664. -->\n\n<!-- # Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies -->\n<!-- # Peter C. Austin, Elizabeth A. Stuart -->\n<!-- # https://onlinelibrary.wiley.com/doi/10.1002/sim.6607 -->\n\n<!-- #bal.tab(dt_match_energy$euro)   #  good -->\n<!-- bal.tab(dt_match_ebal$euro)   #  best -->\n<!-- #bal.tab(dt_match_ps$euro)   #  not as good -->\n\n<!-- # here we show only the best tab, but you should put all information into an appendix -->\n<!-- ``` -->\n\n<!-- Results for Maori -->\n\n<!-- ```{r} -->\n<!-- #| label: balance_tab_maori -->\n\n<!-- # who only Ebal -->\n<!-- #bal.tab(dt_match_energy$mƒÅori)   #  good -->\n<!-- bal.tab(dt_match_ebal$mƒÅori)   #  best -->\n<!-- #bal.tab(dt_match_ps$mƒÅori)   #  not good -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plots -->\n\n<!-- # code for summar -->\n<!-- sum_e <- summary(dt_match_ebal$euro) -->\n<!-- sum_m <- summary(dt_match_ebal$mƒÅori) -->\n\n<!-- # summary euro -->\n<!-- sum_e -->\n\n<!-- # summary maori -->\n<!-- sum_m -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plot_euro -->\n<!-- #| column: page-right -->\n<!-- love_plot_e <- love.plot(dt_match_ebal$euro, -->\n<!--           binary = \"std\", -->\n<!--           thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\") -->\n\n<!-- # plot -->\n<!-- love_plot_e  -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plot_maori -->\n<!-- #| column: page-right -->\n\n<!-- love_plot_m <- love.plot(dt_match_ebal$mƒÅori, -->\n<!--           binary = \"std\", -->\n<!--           thresholds = c(m = .1)) + labs(title = \"MƒÅori Weighting: method e-balance\") -->\n<!-- # plot -->\n<!-- love_plot_m -->\n<!-- ``` -->\n\n<!-- ### Example Summary NZ Euro Propensity scores. -->\n\n<!-- > We estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs. -->\n\n<!-- > The effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively. -->\n\n<!-- > The weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights. -->\n\n<!-- > We also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights. -->\n\n<!-- > We plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders. -->\n\n<!-- > Overall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model. -->\n\n<!-- ### Example Summary Maori Propensity scores. -->\n\n<!-- Results: -->\n\n<!-- > The entropy balancing method was also the best performing method that was applied to a subgroup analysis of the MƒÅori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the MƒÅori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the MƒÅori subgroup. -->\n\n<!-- > The effective sample sizes for the MƒÅori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively -->\n\n<!-- > The weight ranges for the inactive, active, and very active groups in the MƒÅori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights. -->\n\n<!-- > The study also identified the units with the five most extreme weights by group for the MƒÅori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights. -->\n\n<!-- > In conclusion, the results of the MƒÅori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses. -->\n\n<!-- ### More data wrangling, again, this time for the weights on the continuous exposure -->\n\n<!-- Note that we need to attach the `weights` from the propensity score model back to the data. -->\n\n<!-- However, because our weighting analysis estimates a model for the *exposure*, we only need to do this analysis once, no matter how many outcomes we investigate. So there ia a little good news. -->\n\n<!-- ```{r} -->\n<!-- #| label: yet_more_wrangling -->\n<!-- #|  -->\n<!-- # prepare nz_euro data -->\n<!-- dt<- readRDS(here::here(\"data\", \"dt\")) # original data subset only nz europeans -->\n\n<!-- dt_ref_e <- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans -->\n\n\n\n<!-- # add weights -->\n<!-- dt_ref_e$weights <- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data -->\n\n<!-- # prepare maori data -->\n<!-- dt_ref_m <- subset(dt, t0_eth_cat == \"mƒÅori\")# original data subset only maori -->\n\n<!-- # add weights -->\n<!-- dt_ref_m$weights <- dt_match_ebal$mƒÅori$weights # get weights from the ps matching model, add to data -->\n\n<!-- # combine data into one data frame -->\n<!-- dt_ref_all <- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe.  -->\n\n<!-- # save data for later use, if needed -->\n<!-- saveRDS(dt_ref_all, here::here(\"data\",\"dt_ref_all\")) -->\n\n<!-- ``` -->\n\n<!-- ## Anxiety Analysis and Results -->\n\n<!-- Below shows the new code for our regression analysis. Notably, we have adjusted the regression model to include a spline, which allows for a non-linear relationship between the continuous exposure and the outcome. By accommodating potential non-linear effects, we improve the robustness of our model, reducing the risk of misspecification. This enhancement is due to our not assuming a strictly linear impact of the exposure on the outcome. -->\n\n<!-- ```{r} -->\n<!-- #| label: model_anxiety -->\n<!-- #| eval: false -->\n\n<!-- # we do not evaluate to save time -->\n<!-- ### SUBGROUP analysis -->\n<!-- df <-  dt_ref_all -->\n<!-- Y <-  \"t2_kessler_latent_anxiety_z\" -->\n<!-- X <- \"t1_hours_exercise_coarsen\" # already defined above -->\n<!-- baseline_vars = baseline_vars_reflective_propensity -->\n<!-- treat_0 = \"inactive\" -->\n<!-- treat_1 = \"very_active\" -->\n<!-- estimand = \"ATE\" -->\n<!-- scale = \"RD\" -->\n<!-- nsims = 1000 -->\n<!-- family = \"gaussian\" -->\n<!-- continuous_X = FALSE -->\n<!-- splines = FALSE -->\n<!-- cores = parallel::detectCores() -->\n<!-- S = \"t0_eth_cat\" -->\n\n<!-- # not we interact the subclass X treatment X covariates -->\n\n<!-- formula_str <- -->\n<!--   paste( -->\n<!--     Y, -->\n<!--     \"~\", -->\n<!--     S, -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     X , -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     paste(baseline_vars_reflective_propensity, collapse = \"+\"), -->\n<!--     \")\", -->\n<!--     \")\" -->\n<!--   ) -->\n\n<!--   # formula_str. # inspect on our own time  -->\n\n\n\n<!-- # fit model -->\n<!-- fit_all_all  <- glm( -->\n<!--   as.formula(formula_str), -->\n<!--   weights = weights, -->\n<!--   # weights = if (!is.null(weight_var)) weight_var else NULL, -->\n<!--   family = family, -->\n<!--   data = df -->\n<!-- ) -->\n\n<!-- # simulate coefficients -->\n<!-- conflicts_prefer(clarify::sim) -->\n<!-- sim_model_all <- sim(fit_all_all, n = nsims, vcov = \"HC0\") -->\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_estimand_all_e <- sim_ame( -->\n<!--   sim_model_all, -->\n<!--   var = X, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"euro\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- #rm(sim_estimand_all_e) -->\n<!-- # note contrast of interest -->\n<!-- sim_estimand_all_e <- -->\n<!--   transform(sim_estimand_all_e, RD = `E[Y(very_active)]` - `E[Y(inactive)]`) -->\n\n<!-- #rm(sim_estimand_all_m) -->\n\n<!-- # simulate effect as modified in mƒÅori -->\n<!-- sim_estimand_all_m <- sim_ame( -->\n<!--   sim_model_all, -->\n<!--   var = X, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"mƒÅori\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- # combine -->\n<!-- #m(sim_estimand_all_m) -->\n\n<!-- sim_estimand_all_m <- -->\n<!--   transform(sim_estimand_all_m, RD = `E[Y(very_active)]` - `E[Y(inactive)]`) -->\n\n<!-- # rearrange -->\n<!-- names(sim_estimand_all_e) <- -->\n<!--   paste(names(sim_estimand_all_e), \"e\", sep = \"_\") -->\n\n\n<!-- names(sim_estimand_all_m) <- -->\n<!--   paste(names(sim_estimand_all_m), \"m\", sep = \"_\") -->\n\n<!-- summary( sim_estimand_all_e ) -->\n\n<!-- est_all_anxiety <- cbind(sim_estimand_all_m, sim_estimand_all_e) -->\n<!-- est_all_anxiety <- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e) -->\n\n<!-- saveRDS(sim_estimand_all_e, here::here(\"data\",\"sim_estimand_all_e\")) -->\n<!-- saveRDS(sim_estimand_all_m, here::here(\"data\",\"sim_estimand_all_m\")) -->\n<!-- saveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\")) -->\n\n\n<!-- ``` -->\n\n<!-- ## Table of Subgroup Results plus Evalues -->\n\n\n\n<!-- <!-- > E[Y(1)]-E[Y(0)] presents the causal contrast between the expected value of the exposure were all within the target population {Group Var Here} to receive the exposure E[Y(1)] in comparison to the expected value of the exposure were to recieve the baseline treatment  E[Y(0)].  --> -->\n\n<!-- <!-- > The Confidence interval for these estimates is between {2.5% column here} at the lower bound and {97.5% value here at the uppoer bound}.  --> -->\n\n\n<!-- <!-- > E-values represent the minimum strength or magnitude of association that an unmeasured confounder would need to have with both treatment and outcome in order to explain the observed effect estimates conditional on measured covariates [@vanderweele2017]. --> -->\n\n<!-- <!-- > With an observed risk ratio of RR = {E_Value Here}, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of {E_VAL_bound}-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of {EVALUE HERE} - fold each could do so, but weaker joint confounder associations could not..[@vanderweele2017] --> -->\n\n\n<!-- I've created functions for reporting results so you can use this code, changing it to suit your analysis. -->\n\n<!-- ```{r} -->\n<!-- #| label: summary_anxiety -->\n\n\n<!-- # return stored estimates  -->\n<!-- sim_estimand_all_e <- readRDS(here::here(\"data\",\"sim_estimand_all_e\")) -->\n<!-- sim_estimand_all_m<- readRDS(here::here(\"data\",\"sim_estimand_all_m\")) -->\n\n<!-- # create individual summaries  -->\n<!-- sum_e <- summary(sim_estimand_all_e) -->\n<!-- sum_m <- summary(sim_estimand_all_m) -->\n\n\n<!-- # create individual tables -->\n<!-- tab_e <- sub_tab_ate(sum_e, new_name = \"NZ Euro Anxiety\") -->\n<!-- tab_m <- sub_tab_ate(sum_m, new_name = \"MƒÅori Anxiety\") -->\n\n\n<!-- # expand tables  -->\n<!-- plot_e <- sub_group_tab(tab_e, type= \"RD\") -->\n<!-- plot_m <- sub_group_tab(tab_m, type= \"RD\") -->\n\n<!-- big_tab <- rbind(plot_e,plot_m) -->\n\n\n<!-- # table for anxiety outcome --format as \"markdown\" if you are using quarto documents -->\n<!-- big_tab |>  -->\n<!--   kbl(format=\"markdown\") -->\n\n<!-- ``` -->\n\n\n\n<!-- ## Graph of the result -->\n\n<!-- I've create a function you can use to graph your results. Here is the code, adjust to suit. -->\n\n<!-- ```{r} -->\n<!-- # group tables -->\n<!-- sub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and MƒÅori\", xlab = \"Groups\", ylab = \"Effects\", -->\n<!--                  x_offset = -1, -->\n<!--                            x_lim_lo = -1, -->\n<!--                            x_lim_hi = 1.5) -->\n\n<!-- ``` -->\n\n<!-- ### Report the anxiety result. -->\n\n\n<!-- > For the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate. -->\n\n<!-- > E-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect. -->\n\n<!-- > Turning to the MƒÅori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect. -->\n\n<!-- > Thus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and MƒÅori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture. -->\n\n\n<!-- Here's a function that will do much of this work for you. However, you'll need to adjust it, and supply your own interpretation. -->\n\n<!-- ```{r} -->\n<!-- #|label: interpretation function -->\n<!-- #| eval: false -->\n<!-- interpret_results_subgroup <- function(df, outcome, exposure) { -->\n<!--   df <- df %>% -->\n<!--     mutate( -->\n<!--       report = case_when( -->\n<!--         E_Val_bound > 1.2 & E_Val_bound < 2 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\" -->\n<!--         ), -->\n<!--         E_Val_bound >= 2 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\" -->\n<!--         ), -->\n<!--         E_Val_bound < 1.2 & E_Val_bound > 1 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\" -->\n<!--         ), -->\n<!--         E_Val_bound == 1 ~ paste0( -->\n<!--           \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \", -->\n<!--           \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\" -->\n<!--         ) -->\n<!--       ) -->\n<!--     ) -->\n<!--   return(df$report) -->\n<!-- } -->\n\n<!-- ``` -->\n\n\n<!-- You run the function like this:  -->\n\n\n<!-- ```{r} -->\n<!-- interpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\") -->\n<!-- ``` -->\n\n\n<!-- Easy! -->\n\n\n\n<!-- ### Estimate the subgroup contrast -->\n\n<!-- ```{r} -->\n<!-- # calculated above -->\n<!-- est_all_anxiety <- readRDS( here::here(\"data\",\"est_all_anxiety\")) -->\n\n<!-- # make the sumamry into a dataframe so we can make a table -->\n<!-- df <- as.data.frame(summary(est_all_anxiety)) -->\n\n<!-- # get rownames for selecting the correct row -->\n<!-- df$RowName <- row.names(df) -->\n\n<!-- # select the correct row -- the group contrast -->\n<!-- filtered_df <- df |>  -->\n<!--   dplyr::filter(RowName == \"RD_m - RD_e\")  -->\n\n\n<!-- # pring the filtered data frame -->\n<!-- library(kableExtra) -->\n<!-- filtered_df  |>  -->\n<!--   select(-RowName) |>  -->\n<!--   kbl(digits = 3) |>  -->\n<!--   kable_material(c(\"striped\", \"hover\"))  -->\n\n<!-- ``` -->\n\n<!-- Another option for making the table using markdown. This would be useful if you were writing your article using qaurto.  -->\n\n<!-- ```{r} -->\n<!-- filtered_df  |>  -->\n<!--   select(-RowName) |>  -->\n<!--   kbl(digits = 3, format = \"markdown\") -->\n\n<!-- ``` -->\n\n<!-- Report result along the following lines: -->\n\n<!-- > The estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to MƒÅori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and MƒÅori is reliable. It's possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for MƒÅori. Thus, while there is an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution. -->\n\n\n\n\n\n<!-- ## Depression Analysis and Results -->\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- ### SUBGROUP analysis -->\n<!-- dt_ref_all <- readRDS(here::here(\"data\", \"dt_ref_all\")) -->\n<!-- # get column names -->\n<!-- baseline_vars_reflective_propensity <- dt|> -->\n<!--   dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |> colnames() -->\n<!-- df <-  dt_ref_all -->\n<!-- Y <-  \"t2_kessler_latent_depression_z\" -->\n<!-- X <- \"t1_hours_exercise_coarsen\" # already defined above -->\n<!-- baseline_vars = baseline_vars_reflective_propensity -->\n<!-- treat_0 = \"inactive\" -->\n<!-- treat_1 = \"very_active\" -->\n<!-- estimand = \"ATE\" -->\n<!-- scale = \"RD\" -->\n<!-- nsims = 1000 -->\n<!-- family = \"gaussian\" -->\n<!-- continuous_X = FALSE -->\n<!-- splines = FALSE -->\n<!-- cores = parallel::detectCores() -->\n<!-- S = \"t0_eth_cat\" -->\n\n<!-- # not we interact the subclass X treatment X covariates -->\n\n<!-- formula_str <- -->\n<!--   paste( -->\n<!--     Y, -->\n<!--     \"~\", -->\n<!--     S, -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     X , -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     paste(baseline_vars_reflective_propensity, collapse = \"+\"), -->\n<!--     \")\", -->\n<!--     \")\" -->\n<!--   ) -->\n\n<!-- # fit model -->\n<!-- fit_all_dep  <- glm( -->\n<!--   as.formula(formula_str), -->\n<!--   weights = weights, -->\n<!--   # weights = if (!is.null(weight_var)) weight_var else NULL, -->\n<!--   family = family, -->\n<!--   data = df -->\n<!-- ) -->\n\n\n<!-- # coefs <- coef(fit_all_dep) -->\n<!-- # table(is.na(coefs))#    -->\n<!-- # insight::get_varcov(fit_all_all) -->\n\n<!-- # simulate coefficients -->\n<!-- conflicts_prefer(clarify::sim) -->\n<!-- sim_model_all <- sim(fit_all_dep, n = nsims, vcov = \"HC1\") -->\n\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_estimand_all_e_d <- sim_ame( -->\n<!--   sim_model_all, -->\n<!--   var = X, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"euro\", -->\n<!--   verbose = TRUE) -->\n\n\n<!-- # note contrast of interest -->\n<!-- sim_estimand_all_e_d <- -->\n<!--   transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`) -->\n\n\n<!-- # simulate effect as modified in mƒÅori -->\n<!-- sim_estimand_all_m_d <- sim_ame( -->\n<!--   sim_model_all, -->\n<!--   var = X, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"mƒÅori\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- # combine -->\n<!-- sim_estimand_all_m_d <- -->\n<!--   transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`) -->\n\n\n<!-- # summary -->\n<!-- #summary(sim_estimand_all_e_d) -->\n<!-- #summary(sim_estimand_all_m_d) -->\n\n<!-- # rearrange -->\n<!-- names(sim_estimand_all_e_d) <- -->\n<!--   paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\") -->\n\n<!-- names(sim_estimand_all_m_d) <- -->\n<!--   paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\") -->\n\n\n<!-- est_all_d <- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d) -->\n<!-- est_all_d <- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e) -->\n<!-- saveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\")) -->\n<!-- saveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\")) -->\n\n<!-- ``` -->\n\n<!-- ### Report anxiety results -->\n<!-- ```{r} -->\n<!-- # return stored estimates  -->\n<!-- sim_estimand_all_e_d <- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\")) -->\n<!-- sim_estimand_all_m_d<- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\")) -->\n\n<!-- # create individual summaries  -->\n<!-- sum_e_d <- summary(sim_estimand_all_e_d) -->\n<!-- sum_m_d <- summary(sim_estimand_all_m_d) -->\n\n\n<!-- # create individual tables -->\n<!-- tab_ed <- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\") -->\n<!-- tab_md <- sub_tab_ate(sum_m_d, new_name = \"MƒÅori Depression\") -->\n\n\n<!-- # expand tables  -->\n<!-- plot_ed <- sub_group_tab(tab_ed, type= \"RD\") -->\n<!-- plot_md <- sub_group_tab(tab_md, type= \"RD\") -->\n\n<!-- big_tab_d <- rbind(plot_ed,plot_md) -->\n\n\n<!-- # table for anxiety outcome --format as \"markdown\" if you are using quarto documents -->\n<!-- big_tab_d |>  -->\n<!--   kbl(format=\"markdown\") -->\n<!-- ``` -->\n\n\n<!-- ### Graph depression result -->\n\n\n<!-- ```{r} -->\n<!-- # group tables -->\n<!-- sub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and MƒÅori\", xlab = \"Groups\", ylab = \"Effects\", -->\n<!--                  x_offset = -1, -->\n<!--                            x_lim_lo = -1, -->\n<!--                            x_lim_hi = 1.5) -->\n\n<!-- ``` -->\n\n<!-- ###  Interpretation -->\n\n<!-- Use the function, again, modify the outputs to fit with your study and results and provide your own interpretation.  -->\n\n\n<!-- ```{r} -->\n<!-- interpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\") -->\n<!-- ``` -->\n\n<!-- ### Estimate the subgroup contrast -->\n\n\n<!-- ```{r} -->\n<!-- # calculated above -->\n<!-- est_all_d <- readRDS( here::here(\"data\",\"est_all_d\")) -->\n\n<!-- # make the sumamry into a dataframe so we can make a table -->\n<!-- dfd <- as.data.frame(summary(est_all_d)) -->\n\n<!-- # get rownames for selecting the correct row -->\n<!-- dfd$RowName <- row.names(dfd) -->\n\n<!-- # select the correct row -- the group contrast -->\n<!-- filtered_dfd <- dfd |>  -->\n<!--   dplyr::filter(RowName == \"RD_m - RD_e\")  -->\n\n\n<!-- # Print the filtered data frame -->\n<!-- library(kableExtra) -->\n<!-- filtered_dfd  |>  -->\n<!--   select(-RowName) |>  -->\n<!--   kbl(digits = 3) |>  -->\n<!--   kable_material(c(\"striped\", \"hover\"))  -->\n\n<!-- ``` -->\n\n\n\n<!-- Reporting might be:  -->\n\n<!-- > The estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to MƒÅori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and MƒÅori is statistically significant. It's possible that the true difference could be zero or even negative, implying a greater depression reduction for MƒÅori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution. -->\n\n\n\n<!-- ### Discusion  -->\n\n<!-- You'll need to write the discussion for yourself. Here's a start:  -->\n\n\n<!-- > In our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups ‚Äì New Zealand Europeans and MƒÅori. This method has the advantage of providing reliable results even if our underlying assumptions aren't entirely accurate ‚Äì a likely scenario given the complexity of real-world data.  However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn't mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have. -->\n\n<!-- #### **Exercise and anxiety** -->\n\n<!-- > Our analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to MƒÅori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it's crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in MƒÅori. Hence, while there is an indication of a difference, we should interpret it cautiously given the wide range of uncertainty. -->\n\n<!-- > Thus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns. -->\n\n<!-- #### **Exercise and depression** -->\n\n\n<!-- > In addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise -- as self-reported -- on depression.  -->\n\n<!-- #### **Limitations** -->\n\n<!-- > It is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.$\\dots$ nevertheless, $\\dots$ -->\n\n\n<!-- **Again, you will need to come up with your own discussion, but you may follow the step-by-step instructions above as a guide.** -->\n\n\n<!-- ## Appendix: Continuous Exposure -->\n\n\n<!-- ### Check positivity in continuous variable  -->\n\n<!-- Note we are using the natural log of exercise because the variable is highly dispersed.  Any linear transformation of a continuous variable will be OK as far as regression is concerned.   -->\n\n<!-- Let's see how much change there is in the rounded log value of exercise.  -->\n\n<!-- ```{r} -->\n<!-- #  -->\n\n<!-- out_m_cont  <- msm::statetable.msm(hours_exercise_log_round, id, data = dt_exposure_maori) -->\n\n<!-- # with this little support we might consider parametric models -->\n<!-- t_tab_m_c<- transition_table( out_m_cont) -->\n\n<!-- #interpretation -->\n<!-- cat(t_tab_m_c$explanation) -->\n<!-- print(t_tab_m_c$table) -->\n\n<!-- # there is not much change in the extremes. We might decide that we would like to assign to all values above e.g. 20 the same value, or simply to exclude these values.  -->\n\n<!-- # I will leave you to calculate the change in the euro only subgroup -->\n\n\n<!-- ``` -->\n\n\n<!-- ### Estimating Propensity Scores for Continuous Exposures -->\n\n<!-- In most cases, propensity scores are unstable when applied to continuous exposures.  -->\n\n<!-- Should you choose to execute a model without applying propensity score weights, it would equate to performing ordinary g-computation. In this scenario, the regression model leaves `weights` or simply unspecified.  -->\n\n<!-- However, the 'energy' balancing method provided by the `weightit` package offers robust results when estimating propensity scores for continuous exposures. Given this favorable characteristic, we will utilize energy balancing to estimate propensity scores for our continuous exercise exposure variable. -->\n\n<!-- ### Continuous model propensity scores -->\n\n\n\n<!-- ```{r} -->\n<!-- # read  data -- you may start here if you need to repeat the analysis -->\n<!-- dt <- readRDS(here::here(\"data\", \"dt\")) -->\n\n<!-- # get column names -->\n<!-- baseline_vars_reflective_propensity_c <- dt|> -->\n<!--   dplyr::select(starts_with(\"t0\"), -t0_eth_cat, -t0_hours_exercise_coarsen) |> colnames() # note we remove the coarsened excercise variable at baseline. -->\n\n<!-- # define our exposure -->\n<!-- X_c <- \"t1_hours_exercise_log_z\" -->\n<!-- # define subclasses -->\n<!-- S <- \"t0_eth_cat\" -->\n\n<!-- # Make sure data is in a data frame format -->\n<!-- dt <- data.frame(dt) -->\n\n\n<!-- # next we use our trick for creating a formula string, which will reduce our work -->\n<!-- formula_str_prop_c <- -->\n<!--   paste(X_c, -->\n<!--         \"~\", -->\n<!--         paste(baseline_vars_reflective_propensity_c, collapse = \"+\")) -->\n\n<!-- # this shows the exposure variable as predicted by the baseline confounders. -->\n<!-- formula_str_prop_c -->\n<!-- ``` -->\n<!-- Again, we will only use the `energy` method, see <https://ngreifer.github.io/WeightIt/> -->\n\n<!-- ```{r} -->\n<!-- #| label: propensity_scores_cont -->\n<!-- #| eval: false -->\n\n<!-- ## energy balance method -->\n<!-- # Distance Covariance Optimal Weighting (\"energy\") -->\n\n<!-- dt = as.data.frame(dt) -->\n<!-- formula_str_prop_c -->\n\n\n<!-- # this is not working -->\n<!-- dt_match_energy_c <- match_mi_general( -->\n<!--   data = dt, -->\n<!--   X = X_c, -->\n<!--   baseline_vars = baseline_vars_reflective_propensity_c, -->\n<!--   subgroup = \"t0_eth_cat\", -->\n<!--   estimand = \"ATE\", -->\n<!--   #focal = \"high\", # for use with ATT -->\n<!--   method = \"energy\" -->\n<!-- ) -->\n\n<!-- saveRDS(dt_match_energy_c, here::here(\"data\", \"dt_match_energy_c\")) -->\n<!-- ``` -->\n\n<!-- Propensity score assessment on a continuous exposure; let's examine NZ Europeans -->\n\n<!-- ```{r} -->\n<!-- #| label: results_ps_analysis_c -->\n<!-- #|  -->\n<!-- #dt_match_energy <- readRDS(here::here(\"data\", \"dt_match_energy\")) -->\n<!-- dt_match_energy_c <- readRDS(here::here(\"data\", \"dt_match_energy_c\")) -->\n<!-- #dt_match_ps <- readRDS(here::here(\"data\", \"dt_match_ps\")) -->\n\n<!-- # next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2.  -->\n<!-- # note that if the variables are unlikely to influence the outcome we can be less strict.  -->\n\n<!-- #See: Hainmueller, J. 2012. ‚ÄúEntropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.‚Äù Political Analysis 20 (1): 25‚Äì46. https://doi.org/10.1093/pan/mpr025. -->\n\n<!-- # Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of -->\n<!-- # Epidemiology 2008; 168(6):656‚Äì664. -->\n\n<!-- # Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies -->\n<!-- # Peter C. Austin, Elizabeth A. Stuart -->\n<!-- # https://onlinelibrary.wiley.com/doi/10.1002/sim.6607 -->\n\n<!-- bal.tab(dt_match_energy_c$euro)   #  best -->\n<!-- ``` -->\n\n<!-- Results for Maori -->\n\n<!-- ```{r} -->\n<!-- #| label: balance_tab_maori_c -->\n\n<!-- # who only Ebal -->\n<!-- bal.tab(dt_match_energy_c$mƒÅori)   #  good -->\n\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plots_c -->\n\n<!-- # code for summar -->\n<!-- sum_e_c <- summary(dt_match_energy_c$euro) -->\n<!-- sum_m_c <- summary(dt_match_energy_c$mƒÅori) -->\n\n<!-- # summary euro -->\n<!-- sum_e_c -->\n\n<!-- # summary maori -->\n<!-- sum_m_c -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plot_euro_c -->\n<!-- #| column: page-right -->\n<!-- love_plot_e_c<- love.plot(dt_match_energy_c$euro, -->\n<!--           binary = \"std\", -->\n<!--           thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method energy\") -->\n\n<!-- # plot -->\n<!-- love_plot_e_c  -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| label: love_plot_maori_c -->\n<!-- #| column: page-right -->\n\n<!-- love_plot_m_c <- love.plot(dt_match_energy_c$mƒÅori, -->\n<!--           binary = \"std\", -->\n<!--           thresholds = c(m = .1)) + labs(title = \"MƒÅori Weighting: method energy\") -->\n<!-- # plot -->\n<!-- love_plot_m_c -->\n<!-- ``` -->\n\n<!-- ### Example Summary NZ Euro Propensity scores. -->\n\n<!-- > To derive a continuous exposure for weekly exercise, we log transformed reported hours of exercise and then translated these values to z-scores. We then estimated propensity scores within the NZ European subgroup for this continuous exposure `t1_hours_exercise_log_z` using energy balancing in the `WeightIt` package in R. -->\n\n<!-- >The balance measures table reports Corr.Adj values for a range of binary and continuous variables, serving as a measure of balance achieved post-propensity score adjustment. Corr.Adj values that are close to zero suggest that these variables are balanced across the exposure groups after weighting. For example, the Corr.Adj values for variables such as `t0_urban_urban` (a binary variable) and `t0_edu_z` (a continuous variable) were -0.0015 and -0.0001, respectively, indicating excellent balance. The variable representing the log-transformed hours of exercise, `t0_hours_exercise_log_z`, exhibited a Corr.Adj of 0.0104, which is still significantly below our target threshold of 0.05, suggesting an acceptable level of balance. -->\n\n<!-- > The initial total unadjusted sample size was 8641. After energy balancing adjustment, the effective sample size was reduced to 5200.62. The reduction in sample size comes from the application of weights to the observations, thereby reducing imbalance across the exposure groups.  -->\n\n<!-- > The range of weights, as reported in the summary of weights, varied from 0 to 2.8893. Despite this relatively wide range, the weight statistics show that the balance of weights is acceptable. The coefficient of variation was 0.518, the Mean Absolute Deviation (MAD) was 0.407, and entropy was 0.15, all indicating a reasonable balance of weights across the samples.  -->\n\n<!-- > In addition, we identified the five units with the most extreme weights. These units had higher weights compared to the rest of the units in their respective groups. However, the balance of weights was not significantly affected by these extreme cases. -->\n\n<!-- >The unweighted and weighted effective sample sizes were also reported. The total unweighted sample size was 821, while the weighted sample size, after the application of energy balancing, was 647.62. -->\n\n<!-- > We infer that the application of energy balancing to estimate propensity scores for the continuous exposure variable `t1_hours_exercise_log_z` resulted in a good level of balance across all covariates in our sample. These results support the use of energy balancing in propensity score analysis when handling continuous exposure variables, effectively reducing bias and creating a more balanced distribution of covariates across exposure groups. -->\n\n<!-- ### Example Summary Maori Propensity scores. -->\n\n<!-- > We also estimated propensity scores for the continuous exposure `t1_hours_exercise_log_z` within the Maori subgroup using energy balancing in the `WeightIt` package in R. The analysis output provides a detailed view of the balance achieved and is summarized as follows: -->\n\n<!-- > The balance measures table provides Corr.Adj values for a range of binary and continuous variables. These Corr.Adj values indicate the level of balance achieved post-propensity score adjustment. A Corr.Adj value closer to zero signifies an excellent balance across the exposure groups post weighting. For instance, the `t0_urban_urban` (a binary variable) showed a Corr.Adj value of -0.0066 and `t0_edu_z` (a continuous variable) exhibited a Corr.Adj value of 0.0116, both indicating a fair balance. However, the log-transformed hours of exercise, `t0_hours_exercise_log_z`, had a Corr.Adj of 0.0515, just surpassing our target threshold of 0.05. This suggests a slightly reduced level of balance for this particular variable compared to others. -->\n\n<!-- > The total unadjusted sample size was 821, and after energy balancing adjustment, the effective sample size was reduced to 647.62. The reduction in sample size is attributed to the application of weights to the observations, aiming to minimize imbalance across the exposure groups. -->\n\n<!-- > The range of weights, as reported in the summary of weights, varied from 0 to 2.8893. Despite the relatively wide range, the weight statistics demonstrate that the balance of weights is acceptable. The coefficient of variation was 0.518, the Mean Absolute Deviation (MAD) was 0.407, and entropy was 0.15, all indicating an acceptable balance of weights across the samples. -->\n\n<!-- > We also identified the five units with the most extreme weights. These units possessed higher weights compared to the rest of the units in their respective groups. However, these extreme weights did not significantly disrupt the overall balance. -->\n\n<!-- > The total unweighted sample size was 821, while the weighted sample size, after the application of energy balancing, was 647.62. -->\n\n<!-- > We infer that energy balancing procedure applied to estimate propensity scores for the continuous exposure variable `t1_hours_exercise_log_z` within the Maori subgroup resulted in an acceptable level of balance across most covariates. The energy balancing approach effectively reduces bias and ensured a balanced distribution of covariates across exposure groups, reaffirming its utility in propensity score analysis for continuous exposure variables. However, for the variable `t0_hours_exercise_log_z`, some attention may be needed due to a Corr.Adj value slightly exceeding the usual threshold. -->\n\n<!-- ### More data wrangling -->\n\n<!-- Note that we need to attach the `weights` from the propensity score model back to the data. -->\n\n<!-- However, because our weighting analysis estimates a model for the *exposure*, we only need to do this analysis once, no matter how many outcomes we investigate. So, again, there is a little good news.(I will leave the depression outcome analysis to you.) -->\n\n<!-- ```{r} -->\n<!-- #| label: yet_more_wrangling_c -->\n<!-- #|  -->\n<!-- # prepare nz_euro data -->\n<!-- dt<- readRDS(here::here(\"data\", \"dt\")) # original data subset only nz europeans -->\n<!-- dt_ref_e_c <- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans -->\n\n\n\n<!-- # add weights -->\n<!-- dt_ref_e_c$weights <- dt_match_energy_c$euro$weights # get weights from the ps matching model,add to data -->\n\n<!-- # prepare maori data -->\n<!-- dt_ref_m_c <- subset(dt, t0_eth_cat == \"mƒÅori\")# original data subset only maori -->\n\n<!-- # add weights -->\n<!-- dt_ref_m_c$weights <- dt_match_energy_c$mƒÅori$weights # get weights from the ps matching model, add to data -->\n\n<!-- # combine data into one data frame -->\n<!-- dt_ref_all_c <- rbind(dt_ref_e_c, dt_ref_m_c) # combine the data into one dataframe.  -->\n\n<!-- # save data for later use, if needed -->\n<!-- saveRDS(dt_ref_all_c, here::here(\"data\",\"dt_ref_all_c\")) -->\n\n<!-- ``` -->\n\n<!-- ## Anxiety Analysis and Results: Continuous Exposure -->\n\n<!-- This is the analysis code -->\n\n<!-- ```{r} -->\n<!-- #| label: model_anxiety_c -->\n<!-- #| eval: false -->\n\n<!-- # we do not evaluate to save time -->\n<!-- ### SUBGROUP analysis -->\n<!-- df <-  dt_ref_all_c -->\n<!-- Y <-  \"t2_kessler_latent_anxiety_z\" -->\n<!-- X_c <- t1_hours_exercise_log_z# already defined above -->\n<!-- baseline_vars = baseline_vars_reflective_propensity_c -->\n<!-- treat_0 = 0 -->\n<!-- treat_1 = 1 -->\n<!-- estimand = \"ATE\" -->\n<!-- scale = \"RD\" -->\n<!-- nsims = 1000 -->\n<!-- family = \"gaussian\" -->\n<!-- continuous_X = TRUE # note change -->\n<!-- splines = TRUE # note change -->\n<!-- cores = parallel::detectCores() -->\n<!-- S = \"t0_eth_cat\" -->\n\n<!-- # not we interact the subclass X treatment X covariates -->\n\n<!-- formula_str_c <- -->\n<!--   paste( -->\n<!--     Y, -->\n<!--     \"~\", -->\n<!--     S, -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     X_c , -->\n<!--     \"*\", -->\n<!--     \"(\", -->\n<!--     paste(baseline_vars_reflective_propensity_c, collapse = \"+\"), -->\n<!--     \")\", -->\n<!--     \")\" -->\n<!--   ) -->\n\n<!-- # formula_str. # inspect on our own time  -->\n\n\n\n<!-- # fit model -->\n<!-- fit_all_all_c  <- glm( -->\n<!--   as.formula(formula_str_c), -->\n<!--   weights = weights, -->\n<!--   # weights = if (!is.null(weight_var)) weight_var else NULL, -->\n<!--   family = family, -->\n<!--   data = df -->\n<!-- ) -->\n\n<!-- # simulate coefficients -->\n<!-- conflicts_prefer(clarify::sim) -->\n<!-- sim_model_all_c <- sim(fit_all_all_c, n = nsims, vcov = \"HC0\") -->\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_estimand_all_e_c <- sim_ame( -->\n<!--   sim_model_all_c, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"euro\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- tab_e_c<- summary(sim_estimand_all_e_c) -->\n\n<!-- tab_real_euro <- tab_ate(tab_e_c, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"NZ Euro Anxiety, Continuous X\") -->\n\n<!-- # tab_real_euro -->\n\n\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_estimand_all_m_c <- sim_ame( -->\n<!--   sim_model_all_c, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"mƒÅori\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n\n\n<!-- tab_m_c<- summary(sim_estimand_all_m_c) -->\n\n<!-- tab_real_maori<- tab_ate(tab_m_c, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"MƒÅori Anxiety Anxiety, Continuous X\") -->\n\n<!-- #tab_real_maori -->\n\n<!-- # Save tables  -->\n<!-- saveRDS(tab_real_euro, here::here(\"data\",\"tab_real_euro\")) -->\n\n<!-- saveRDS(tab_real_maori, here::here(\"data\",\"tab_real_maori\")) -->\n<!-- ``` -->\n\n<!-- ## Table of Subgroup Results plus Evalues -->\n\n\n<!-- I've created functions for reporting results so you can use this code, changing it to suit your analysis. -->\n\n<!-- ```{r} -->\n<!-- #| label: summary_anxiety_c -->\n\n\n<!-- # return stored estimates  -->\n<!-- tab_real_euro <- readRDS(here::here(\"data\",\"tab_real_euro\")) -->\n<!-- tab_real_maori <- readRDS(here::here(\"data\",\"tab_real_maori\")) -->\n\n<!-- big_tab_c <- rbind(tab_real_euro,tab_real_maori) -->\n\n<!-- # expand tables  -->\n<!-- plot_ed_c <- sub_group_tab(tab_real_euro, type= \"RD\") -->\n<!-- plot_md_c <- sub_group_tab(tab_real_maori, type= \"RD\") -->\n\n<!-- # use this for graph -->\n<!-- big_tab_d_c <- rbind(plot_ed_c,plot_md_c) -->\n\n<!-- # use this simple table for the report -->\n<!-- big_tab_c |>  -->\n<!--   kbl(format=\"markdown\") -->\n\n<!-- ``` -->\n\n\n\n<!-- ## Graph of the result -->\n\n<!-- I've create a function you can use to graph your results. Here is the code, adjust to suit. -->\n\n<!-- ```{r} -->\n<!-- # group tables -->\n<!-- sub_group_plot_ate(big_tab_d_c, title = \"Effect of Continuous Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and MƒÅori\", xlab = \"Groups\", ylab = \"Effects\", -->\n<!--                  x_offset = -1, -->\n<!--                            x_lim_lo = -1, -->\n<!--                            x_lim_hi = 1.5) -->\n\n<!-- ``` -->\n\n<!-- ### Report the anxiety result. -->\n\n\n<!-- Here's a function that will do much of this work for you. However, you'll need to adjust it, and supply your own interpretation. -->\n\n<!-- ```{r} -->\n<!-- #|label: interpretation function -->\n<!-- #| eval: false -->\n<!-- interpret_results_subgroup <- function(df, outcome, exposure) { -->\n<!--   df <- df %>% -->\n<!--     mutate( -->\n<!--       report = case_when( -->\n<!--         E_Val_bound > 1.2 & E_Val_bound < 2 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\" -->\n<!--         ), -->\n<!--         E_Val_bound >= 2 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\" -->\n<!--         ), -->\n<!--         E_Val_bound < 1.2 & E_Val_bound > 1 ~ paste0( -->\n<!--           \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \", -->\n<!--           \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\" -->\n<!--         ), -->\n<!--         E_Val_bound == 1 ~ paste0( -->\n<!--           \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\", -->\n<!--           \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \", -->\n<!--           \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\" -->\n<!--         ) -->\n<!--       ) -->\n<!--     ) -->\n<!--   return(df$report) -->\n<!-- } -->\n\n<!-- ``` -->\n\n\n<!-- You run the function like this:  -->\n\n\n<!-- ```{r} -->\n<!-- interpret_results_subgroup(big_tab_d_c, outcome = \"Anxiety\", exposure = \"Excercise\") -->\n<!-- ``` -->\n\n<!-- > The second set of results extends our understanding of the impact of continuous exercise exposure on anxiety for both the New Zealand European and MƒÅori groups. The causal contrast, in this case, compares the effect of exercising 13.12 hours per week (the treatment group) to exercising 5.88 hours per week (the control group).  -->\n\n<!-- > For the New Zealand European group, the causal contrast value decreased from -0.077 in the first output to -0.029 in the second output. This suggests a smaller estimated effect of exercise on reducing anxiety when considering continuous exposure compared to the initial analysis. The associated confidence interval (-0.053 to -0.006) in the second output does not cross zero, suggesting more certainty in the negative effect of exercise on anxiety compared to the initial results. However, the E-values, reflecting the potential impact of unmeasured confounding variables, suggest that the interpretation of these findings should be made with caution.  -->\n\n<!-- > For the MƒÅori group, the second analysis indicates a reduced effect of exercise on anxiety compared to the first analysis, with the causal contrast decreasing from 0.027 to -0.007. However, the associated confidence interval for this estimate (-0.072 to 0.062) crosses zero, echoing the initial results and indicating considerable uncertainties in the estimate. The E-values further reinforce that these findings should be interpreted with caution due to potential unmeasured confounding factors.  -->\n\n<!-- > Thus, while both the categorical and continuous exposure results suggest a potential influence of exercise in reducing anxiety for NZ Europeans, although perhaps not MƒÅori, the effects seem to be less pronounced in NZ Europeans when modelling a continuous exercise exposure.  -->\n<!-- > However, it is important to note that each model addresses a different causal question. The categorical exposure model investigates the contrast between not exercising at all and exercising over eight hours per week. On the other hand, the continuous exposure model examines the contrast between exercising almost six hours a week and doubling that amount of exercise. -->\n\n<!-- > These two modelling approaches describe different hypothetical experiments. Arguably, neither scenario reflects realistic interventions. Nonetheless, the indication of exercise potentially aiding in anxiety reduction in NZ Europeans tentatively suggests it might be worthwhile to further investigate the role of exercise in promoting mental health, and also for identifying pathways to better expressing exercise benefits on mental health among MƒÅori -->\n\n\n<!-- I'll pass the baton back to you now to refine the interpretation of these results. Additionally, you may want to explore conducting a depression analysis using a continuous exercise exposure model. The steps provided in this Appendix should aid you in performing such an analysis with a continuous exposure variables. -->\n\n<!-- Ask me if you have any questions! -->\n\n\n<!-- ## Appendix 2 G-computation alone: a step-by-step guide. -->\n\n<!-- Just leave out the weights.  -->\n\n<!-- Note the effect estimate is similar, but slightly stronger using only G-computation. -->\n\n<!-- ```{r} -->\n<!-- #| label: continuous_exposure_g-compuation  -->\n<!-- #| eval: false -->\n\n<!-- formula_str_c -->\n\n<!-- fit_all_all_c_gcomp  <- glm( -->\n<!--   as.formula(formula_str_c), -->\n<!--   data = df -->\n<!-- ) -->\n\n<!-- # simulate coefficients -->\n<!-- conflicts_prefer(clarify::sim) -->\n<!-- sim_model_all_c_gcomp <- sim(fit_all_all_c_gcomp, n = nsims, vcov = \"HC0\") -->\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_model_all_c_gcomp_e <- sim_ame( -->\n<!--   sim_model_all_c_gcomp, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"euro\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- tab_e_comp <- summary(sim_model_all_c_gcomp_e) -->\n\n<!-- tab_real_euro_comp <- tab_ate(tab_e_comp, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"NZ Euro Anxiety, Continuous X\") -->\n\n<!-- # tab_real_euro -->\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_estimand_all_m_c_gcomp <- sim_ame( -->\n<!--   sim_model_all_c_gcomp, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"mƒÅori\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- tab_m_comp<- summary(sim_estimand_all_m_c_gcomp) -->\n\n<!-- tab_real_maori_comp<- tab_ate(tab_m_comp, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"MƒÅori Anxiety Anxiety, Continuous X\") -->\n\n<!-- gcomp_tab_euro_maori <- rbind(tab_e_comp, tab_m_comp) -->\n\n<!-- plot_comp <- rbind( tab_real_euro_comp, tab_real_maori_comp ) -->\n\n\n<!-- # expand tables  -->\n<!-- plot_ed_comp <- sub_group_tab(tab_real_euro_comp, type= \"RD\") -->\n<!-- plot_md_comp <- sub_group_tab(tab_real_maori_comp, type= \"RD\") -->\n\n<!-- plot_ed_comp -->\n\n<!-- # use this for graph -->\n<!-- big_tab_d_comp <- rbind(plot_ed_comp,plot_md_comp) -->\n\n<!-- # use this simple table for the report -->\n<!-- big_tab_d_comp |>  -->\n<!--   kbl(format=\"markdown\") -->\n\n<!-- saveRDS(big_tab_d_comp, here::here(\"data\", \"big_tab_d_comp\")) -->\n<!-- ``` -->\n\n\n<!-- ```{r} -->\n<!-- big_tab_d_comp<- readRDS( here::here(\"data\", \"big_tab_d_comp\")) -->\n\n\n<!-- sub_group_plot_ate(big_tab_d_comp, title = \"Effect of Continuous Exercise on Anxiety: G-computation\", subtitle = \"Subgroup Analysis: NZ Euro and MƒÅori\", xlab = \"Groups\", ylab = \"Effects\", -->\n<!--                  x_offset = -1, -->\n<!--                            x_lim_lo = -1, -->\n<!--                            x_lim_hi = 1.5) -->\n<!-- ``` -->\n\n\n<!-- ### Appendix 3 Propensity Scores Alone: a step-by-step guide. -->\n\n<!-- Again the effect estimate is similar, but slightly less efficient (more uncertainty).  -->\n\n<!-- It is encouraging that we do not see large differences from model choice.  -->\n\n\n<!-- ```{r} -->\n<!-- #| label: continuous_exposure_propensity_score -->\n<!-- #| eval: false -->\n<!-- Y -->\n<!-- X_c -->\n\n<!-- table(df$t0_eth_cat) -->\n<!-- formula_str_c -->\n<!-- fit_prop <- lm(t2_kessler_latent_anxiety_z ~ t0_eth_cat * ( t1_hours_exercise_log_z), weights = weights, data = df) -->\n\n<!-- # simulate coefficients -->\n<!-- conflicts_prefer(clarify::sim) -->\n<!-- sim_prop <- sim(fit_prop, n = nsims, vcov = \"HC0\") -->\n\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_model_prop<- sim_ame( -->\n<!--   sim_prop, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"euro\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- tab_e_prop <- summary(sim_model_prop) -->\n\n<!-- tab_real_euro_prop<- tab_ate(tab_e_prop, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"NZ Euro Anxiety, Continuous X\") -->\n\n\n<!-- # tab_real_euro -->\n<!-- # simulate effect as modified in europeans -->\n<!-- sim_maori_prop <- sim_ame( -->\n<!--   sim_prop, -->\n<!--   var = X_c, -->\n<!--   cl = cores, -->\n<!--   subset = t0_eth_cat == \"mƒÅori\", -->\n<!--   verbose = TRUE -->\n<!-- ) -->\n\n<!-- tab_m_prop<- summary(sim_maori_prop) -->\n\n<!-- tab_real_maori_prop<- tab_ate(tab_m_prop, delta = 1, sd = 1, type =\"RD\", continuous_X = TRUE, new_name = \"MƒÅori Anxiety Anxiety, Continuous X\") -->\n\n\n<!-- # expand tables  -->\n<!-- plot_ed_prop<- sub_group_tab(tab_real_euro_prop, type= \"RD\") -->\n<!-- plot_md_prop <- sub_group_tab(tab_real_maori_prop, type= \"RD\") -->\n\n\n<!-- # use this for graph -->\n<!-- big_tab_d_prop <- rbind(plot_ed_prop,plot_md_prop) -->\n\n<!-- # use this simple table for the report -->\n<!-- big_tab_d_prop |>  -->\n<!--   kbl(format=\"markdown\") -->\n\n<!-- saveRDS(big_tab_d_prop, here::here(\"data\", \"big_tab_d_prop\")) -->\n<!-- ``` -->\n\n\n<!-- ```{r} -->\n\n<!-- # graph -->\n<!-- big_tab_d_prop<- readRDS( here::here(\"data\", \"big_tab_d_prop\")) -->\n\n\n<!-- sub_group_plot_ate(big_tab_d_prop, title = \"Effect of Continuous Exercise on Anxiety: Propensity Scores\", subtitle = \"Subgroup Analysis: NZ Euro and MƒÅori\", xlab = \"Groups\", ylab = \"Effects\", -->\n<!--                  x_offset = -1, -->\n<!--                            x_lim_lo = -1, -->\n<!--                            x_lim_hi = 1.5) -->\n<!-- ``` -->\n\n",
    "supporting": [
      "step-by-step-reporting-guide_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}