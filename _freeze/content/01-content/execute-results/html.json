{
  "hash": "08dc2ad61aa4ae117fa24671f6616f02",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Asking questions in cross-cultural psychology\"\ndate: \"2023-FEB-28\"\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell caption='dag'}\n::: {.cell-output-display}\n![A caual graph ](01-content_files/figure-html/fig-line-plot-1.png){#fig-line-plot width=60%}\n:::\n:::\n\n\n\n## Lecture: Introduction to the Course\n\n\n\n## Slides\n\n[PREVIEW](/slides/01-slides.html)\n\n<div>\n\n\n```{=html}\n<iframe class=\"slide-deck\" src=\"/slides/01-slides.html\"></iframe>\n```\n\n\n</div>\n\nOpen in browser [here](/slides/01-slides.html){target=\"_blank\"}\n\n## Background Readings\n\n[@he2012]\n\n[@vandevijver2021]\n\n[@berry1989]\n\n\n## Lab: Introduction to R. \n\n\n# Session 1: Installing R and RStudio\n\n## Introduction\n\nThis session is designed to introduce you to R and RStudio, the essential tools for statistical analysis in cross-cultural psychology. We aim to familiarise you with the software and enable you to simulate and manipulate data from the beginning.\n\n## Installing R\n\n1. Visit the Comprehensive R Archive Network (CRAN) at [https://cran.r-project.org/](https://cran.r-project.org/).\n2. Select the version of R suitable for your operating system (Windows, Mac, or Linux).\n3. Download and install it by following the on-screen instructions.\n\n## Installing RStudio\n\n1. Go to the RStudio download page at [https://www.rstudio.com/products/rstudio/download/](https://www.rstudio.com/products/rstudio/download/).\n2. Choose the free version of RStudio Desktop, and download it for your operating system.\n3. Install RStudio by following the provided instructions.\n\n## Familiarizing Yourself with RStudio\n\n- **Console**: Executes R code line by line.\n- **Source Editor**: Allows you to write and execute scripts (series of commands).\n- **Environment**: Displays variables and data you've loaded.\n- **Files/Plots/Packages/Help**: Allows you to navigate your files, view plots, manage packages, and access R documentation.\n\n## Basic R Commands\n\nLet us start by using R as a calculator. This will help you understand how to execute simple commands in the console.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Addition\n3 + 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Subtraction\n5 - 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\n# Multiplication\n3 * 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6\n```\n\n\n:::\n\n```{.r .cell-code}\n# Division\n10 / 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# Modulus\n7 %% 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Exponentiation\n2 ^ 3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8\n```\n\n\n:::\n\n```{.r .cell-code}\n# Integer Division\n10 %/% 3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n### Exercise 1: Installing the `tidyverse` Package\n\nIn this exercise, you will install the `tidyverse` package, a collection of R packages designed for data science. Follow the steps below to complete the installation:\n\n1. **Open RStudio:** Start by launching RStudio on your computer.\n\n2. **Access Help Tab:** Locate and click on the \"Help\" tab in the lower right pane of RStudio.\n\n3. **Search for Installation Instructions:** In the search bar within the \"Help\" tab, type in \"install packages\" and press Enter. Browse through the help documents if available. (Note: The specific steps to search within the Help tab might vary based on RStudio version and setup. If you cannot find the option to search for \"install packages\" directly in the Help tab, proceed to the next step.)\n\n4. **Open Package Installation:**\n    - Alternatively, you can directly access the package installation option by going to the \"Tools\" menu at the top of RStudio.\n    - Select \"Install Packages...\" from the dropdown menu.\n\n5. **Install `tidyverse`:**\n    - In the \"Install Packages\" dialogue box, you'll find a field to type in the name of the package you wish to install. Type `tidyverse` into this field.\n    - Ensure the \"Install dependencies\" checkbox is ticked. This option ensures that any additional packages needed by `tidyverse` are also installed.\n\n6. **Begin Installation:**\n    - Click on the \"Install\" button to start the installation process.\n\n7. **Wait for Completion:** The installation might take a few minutes. Monitor the progress in the \"Console\" pane. Once the installation is complete, you will see a message in the console indicating that the process has finished.\n\n8. **Loading `tidyverse`:** After successful installation, you can load the `tidyverse` package into your R session by typing `library(tidyverse)` in the console and pressing Enter.\n\n\n\n## Simulating Data in R\n\nSimulating data is a powerful method to understand statistical concepts and data manipulation. Let's simulate a simple dataset representing scores from two cultural groups.\n\n\n#### Step 1: Setting Up Your R Environment\n\nBefore simulating data, ensure R or RStudio is installed and open. RStudio provides a user-friendly interface for R, which can simplify the process of writing and executing R scripts.\n\n#### Step 2: Setting a Seed for Reproducibility\n\nTo ensure that your simulated data can be reproduced exactly, it's good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\n\n```r\nset.seed(123) # use any number to set the seed\n```\n\n#### Step 3: Simulating Continuous Data\n\nTo simulate continuous data, you can use functions like `rnorm()` for normal distributions, `runif()` for uniform distributions, etc. Hereâ€™s how to simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100 # number of observations\nmean <- 50\nsd <- 10\ndata_continuous <- rnorm(n, mean, sd)\n```\n:::\n\n\n#### Step 4: Simulating Categorical Data\n\nCategorical data can be simulated using the `sample()` function. For example, to simulate a binary variable (e.g., gender) with two levels for 100 observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels <- c(\"Male\", \"Female\")\ndata_categorical <- sample(levels, n, replace = TRUE)\n```\n:::\n\n\n#### Step 5: Simulating Data Frames\n\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame <- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\n```\n:::\n\n\n\nNote that you can sample probabilistically for your groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))\n```\n:::\n\n\n\n#### Step 6: Add Complexity\n\nTo simulate more complex datasets, you can introduce relationships between variables. For instance, simulating age and income with a correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set the number of observations\nn <- 100\n\n# simulate the 'Age' variable\nmean_age <- 30\nsd_age <- 5\nAge <- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept <- 20000   # Intercept for the income equation\nbeta_age <- 1500     # Coefficient for the effect of age on income\nerror_sd <- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome <- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex <- data.frame(Age, Income)\n```\n:::\n\n\n#### Step 7: Visualising Simulated Data\n\nVisualising your simulated data can help understand its distribution and relationships. Use the `ggplot2` package for this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")\n```\n\n::: {.cell-output-display}\n![](01-content_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n## Practice\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) #  reproducibility\ngroupA_scores <- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores <- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\nscores_df <- data.frame(Group = rep(c(\"A\", \"B\"), each = 100), Scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\nstr(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t200 obs. of  2 variables:\n $ Group : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ Scores: num  91.6 96.5 123.4 101.1 101.9 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# summary of columns\nsummary(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Group               Scores      \n Length:200         Min.   : 65.36  \n Class :character   1st Qu.: 92.59  \n Mode  :character   Median :101.38  \n                    Mean   :102.37  \n                    3rd Qu.:111.59  \n                    Max.   :153.62  \n```\n\n\n:::\n\n```{.r .cell-code}\n# top rows\nhead(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Group    Scores\n1     A  91.59287\n2     A  96.54734\n3     A 123.38062\n4     A 101.05763\n5     A 101.93932\n6     A 125.72597\n```\n\n\n:::\n\n```{.r .cell-code}\n# bottom rows\ntail(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Group    Scores\n195     B  85.33798\n196     B 134.95820\n197     B 114.01063\n198     B  86.23093\n199     B  95.83251\n200     B  87.21780\n```\n\n\n:::\n:::\n\n\n\n## Visualising simulated data\n\nUnderstanding your data visually is as important as the statistical analysis itself. Let's create a simple plot to compare the score distributions between the two groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# plot your data\nggplot(scores_df, aes(x = Group, y = Scores, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n```\n\n::: {.cell-output-display}\n![Score Distribution by Group](01-content_files/figure-html/visualize-data-1.png){width=672}\n:::\n:::\n\n\n\n## Histogram {#sec-histogram}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# H=histograms for both groups\nggplot(scores_df, aes(x = Scores, fill = Group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~Group, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](01-content_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Excercise 1\n\n1. Modify the simulation parameters to change each group's mean and standard deviation. Observe how these changes affect the distribution.\n\n2. Go to the [histogram](#sec-histogram). Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another. \n\n\n## Simulating data for familiar statistical tests \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate some data\ndata <- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nt.test(data, mu = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  data\nt = 11.796, df = 99, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.931989 5.308942\nsample estimates:\nmean of x \n 5.120465 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate data for two groups\ngroup1 <- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 <- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nt.test(group1, group2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  group1 and group2\nt = -2.0293, df = 97.95, p-value = 0.04514\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.837548023 -0.009343886\nsample estimates:\nmean of x mean of y \n 5.002054  5.425500 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate pre-test and post-test scores\npre_test <- rnorm(30, mean = 80, sd = 10)\npost_test <- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nt.test(pre_test, post_test, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  pre_test and post_test\nt = -4.7761, df = 29, p-value = 4.725e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -6.785042 -2.716352\nsample estimates:\nmean difference \n      -4.750697 \n```\n\n\n:::\n:::\n\n\n\n## Equivalence of ANOVA and Regression\n\n\nWe will simulate data in R to show that a one-way ANOVA is a special case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings).\n\n## Method\n\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a \"dependant\") variable. This setup allows us to apply both ANOVA and linear regression for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# nice tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n\nset.seed(321) # reproducibility\nn <- 90 # total number of observations\nk <- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup <- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulate outcome variable\nmeans <- c(100, 100, 220) # Mean for each group\nsd <- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny <- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 <- cbind.data.frame(y, group)\n\nanova_model <- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova <- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe ANOVA (formula: y ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 87) =\n689.11, p < .001; Eta2 = 0.94, 95% CI [0.92, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n```\n\n\n:::\n:::\n\n\n\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit <- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit <- parameters::model_parameters(fit)\n\n# print table\ntable_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter   | Coefficient |   SE |           95% CI | t(87) |      p\n--------------------------------------------------------------------\n(Intercept) |      101.22 | 2.60 | [ 96.06, 106.39] | 38.98 | < .001\ngroup [2]   |       -0.80 | 3.67 | [ -8.10,   6.50] | -0.22 | 0.827 \ngroup [3]   |      117.67 | 3.67 | [110.37, 124.97] | 32.04 | < .001\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit <- report_parameters(fit)\n\n#print\nreport_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - The intercept is statistically significant and positive (beta = 101.22, 95% CI [96.06, 106.39], t(87) = 38.98, p < .001; Std. beta = -0.68, 95% CI [-0.76, -0.59])\n  - The effect of group [2] is statistically non-significant and negative (beta = -0.80, 95% CI [-8.10, 6.50], t(87) = -0.22, p = 0.827; Std. beta = -0.01, 95% CI [-0.14, 0.11])\n  - The effect of group [3] is statistically significant and positive (beta = 117.67, 95% CI [110.37, 124.97], t(87) = 32.04, p < .001; Std. beta = 2.04, 95% CI [1.91, 2.17])\n```\n\n\n:::\n:::\n\n\n\n## Upshot\n\nANOVA partitions variance into between-group and within-group components, while regression models the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when we are comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain understanding. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, Along the way, you'll learn more about data visualisation, modelling, and reporting.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# graph the output of the parameters table\n# visualisation\nplot(table_fit)\n```\n\n::: {.cell-output-display}\n![](01-content_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### Exercise 3\n\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\n1. **Simulate Data:**\n   - Generate two continuous variables, `Y` and `A`, with `n = 100` observations each.\n   - The variable `A` should have a mean of `50` and a standard deviation (`sd`) of `10`.\n\n2. **Define the Relationship:**\n   - Simulate the variable `Y` such that it is linearly related to `A` with a specified effect size. The effect size of `A` on `Y` must be explicitly defined as `2`.\n\n3. **Incorporate an Error Term:**\n   - When simulating `Y`, include an error term with a standard deviation (`sd`) of `20` to introduce variability.\n\n4. **Regression Analysis:**\n   - Use the `lm()` function in R to regress `Y` on `A`.\n   - Ensure the regression model captures the specified effect of `A` on `Y`.\n\n5. **Report the Results:**\n   - Output the regression model summary to examine the coefficients, including the effect of `A` on `Y`, and assess the model's overall fit and significance.\n\n\nHere's a template to get you started\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn <-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA <- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A <-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 <- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 <- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)\n```\n:::\n\n\n## For more information about the packages used here:\n\n  - [ggplot2](https://ggplot2.tidyverse.org/): A system for declaratively creating graphics, based on The Grammar of Graphics.\n\n  - [Parameters package](https://easystats.github.io/parameters/): Provides utilities for processing model parameters and their metrics.\n\n  - [Report package](https://easystats.github.io/report/index.html): Facilitates the automated generation of reports from statistical models.\n\n\n## What You Have Learned\n\n- **How to install and setup R:** \n\nYou've successfully installed R and RStudio, setting up your workstation for statistical analysis.\n  \n- **How to install and use RStudio:** \n\nYou've familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n  \n- **Basic R operations:** \n\nYou've practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n  \n- **Data simulation:** \n\nYou've learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations! \n  \n- **Data visualisation:** \n\nYou've begun data visualising data through boxplots and histograms and coefficient plots, which is crucial for analysing and communicating statistical findings.\n  \n- **Statistical tests:** You've conducted basic statistical tests, including t-tests and ANOVA, gaining insights into comparing means across groups.\n  \n- **Understanding ANOVA and regression:** \n\nYou've explored the equivalence of ANOVA and regression analysis, learning how these methods can be applied to analyse and interpret data effectively.\n\n## Getting Help \n\nAs sure as night follows day, you will need help coding.  Key resources\n\n1. **Large Language Models (LLMs):** OpenAI's premium LLM (GPT-4) outperforms the free version (GPT-3.5) for complex queries. \n\n2. **Stack Exchange:** a valuable resource for coding advice and solutions.\n\n3. **Developer Websites and GitHub Pages:** Directly engage with package developers and the community for insights and support.[Parameters package discussion page](https://github.com/easystats/parameters/discussions) offers insights and support directly from its developers and user community.\n\n4.  Your tutors and lecturer. We care. Weâ€™re here to help you. \n\n\n## Recommended Reading\n\n- Wickham, H., & Grolemund, G. (2016). *R for Data Science*. O'Reilly Media. [Available online](https://r4ds.had.co.nz\n\n\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(report)\nreport::cite_packages()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, <https://CRAN.R-project.org/package=extrafont>.\n  - LÃ¼decke D, Ben-Shachar M, Patil I, Makowski D (2020). \"Extracting, Computing and Exploring the Parameters of Statistical Models using R.\" _Journal of Open Source Software_, *5*(53), 2445. doi:10.21105/joss.02445 <https://doi.org/10.21105/joss.02445>.\n  - Makowski D, LÃ¼decke D, Patil I, ThÃ©riault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. <https://easystats.github.io/report/>.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, <https://ggplot2.tidyverse.org>.\n  - Xie Y (2023). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.49, <https://github.com/rstudio/tinytex>. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. <https://tug.org/TUGboat/Contents/contents40-1.html>.\n```\n\n\n:::\n:::\n\n\n\n\n## Appendix A: Solutions to Problem Sets {#appendix-a}\n\n\n\n### Solution Problem Set 3: simulate data and regression reporting \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\n#  seed for reproducibility\nset.seed(12345)\n\n# number of observations\nn <- 100\n\n# simulate data for variable A with specified mean and sd\nA <- rnorm(n, mean = 50, sd = 10)\n\n# define the specified effect size of A on Y\nbeta_A <- 2\n\n\n# simulate data and make data frame in one step\n\ndf_3 <- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A =  rnorm(n, mean = 50, sd = 10),\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20)\n)\n\n# view\nhead(df_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         A         Y\n1 52.23925  87.98766\n2 38.43777 106.60413\n3 54.22419 107.68437\n4 36.75245 117.09730\n5 51.41084 133.74473\n6 44.63952  70.74512\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(df_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t100 obs. of  2 variables:\n $ A: num  52.2 38.4 54.2 36.8 51.4 ...\n $ Y: num  88 107 108 117 134 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform linear regression of Y on A\n\nfit_3 <- lm(Y ~ A, data = df_3)\n\n# Report the results of the regression\n# summary(model)\n\n# report\nparameters::model_parameters(fit_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter   | Coefficient |    SE |          95% CI | t(98) |      p\n--------------------------------------------------------------------\n(Intercept) |      109.17 | 14.62 | [80.17, 138.18] |  7.47 | < .001\nA           |   -3.80e-03 |  0.28 | [-0.57,   0.56] | -0.01 | 0.989 \n```\n\n\n:::\n\n```{.r .cell-code}\nreport(fit_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWe fitted a linear model (estimated using OLS) to predict Y with A (formula: Y\n~ A). The model explains a statistically not significant and very weak\nproportion of variance (R2 = 1.83e-06, F(1, 98) = 1.79e-04, p = 0.989, adj. R2\n= -0.01). The model's intercept, corresponding to A = 0, is at 109.17 (95% CI\n[80.17, 138.18], t(98) = 7.47, p < .001). Within this model:\n\n  - The effect of A is statistically non-significant and negative (beta =\n-3.80e-03, 95% CI [-0.57, 0.56], t(98) = -0.01, p = 0.989; Std. beta =\n-1.35e-03, 95% CI [-0.20, 0.20])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n\n\n:::\n:::\n\n\n\n## Appendix B: I lied\n\nWe can get group comparisons with ANOVA, for example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc <- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(tukey_post_hoc)\n```\n\n::: {.cell-output-display}\n![](01-content_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nRegression and ANOVA are equivalent\n\n\n\n",
    "supporting": [
      "01-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}