{
  "hash": "92dac098dc60620994a42802577705d3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Causal diagrams: Five elementary structures\"\ndate: \"2023-MAR-04\"\nbibliography: /Users/joseph/GIT/templates/bib/references.bib\neditor_options: \n  chunk_output_type: console\nformat:\n  pdf:\n    sanitize: true\n    keep-tex: true\n    link-citations: true\n    colorlinks: true\n    documentclass: article\n    classoption: [singlecolumn]\n    lof: false\n    lot: false\n    number-sections: false\n    number-depth: 4\n    highlight-style: github\n    geometry:\n      - top=30mm\n      - left=20mm\n      - heightrounded\n    header-includes:\n      - \\input{/Users/joseph/GIT/latex/latex-for-quarto.tex}\n  html:\n    html-math-method: katex\n    warnings: FALSE\n    error: FALSE\n    messages: FALSE\n    code-overflow: scroll\n    highlight-style: kate\n    code-tools:\n      source: true\n      toggle: FALSE\n---\n\n::: {.cell}\n\n:::\n\n  \n  \n\n\n::: {.callout-note}\n## Readings\n\n- Barrett M (2023). _ggdag: Analyze and Create Elegant Directed Acyclic Graphs_. R package version 0.2.7.9000, <https://github.com/malcolmbarrett/ggdag>\n- \"An Introduction to Directed Acyclic Graphs\", <https://r-causal.github.io/ggdag/articles/intro-to-dags.html>\n- \"Common Structures of Bias\", <https://r-causal.github.io/ggdag/articles/bias-structures.html>\n\n:::\n\n::: {.callout-important}\n## Key concepts for the test(s):\n- **Confounding**  (introduced in week 2)\n- **Internal Validity** (introduced in week 2)\n- **External Validity** (introduced in week 3)\n:::\n\n::: {.callout-important}\n## Download Your Laboratory R Script Here\n\nAssuming you have installed R and RStudio:\n\n1. Download the R script for Lab 01 by clicking the link below. This script contains the code you will work with during your laboratory session.\n\n2. After downloading, open RStudio. \n\n3. In RStudio, create a new R script file by going to `File > New File > R Script`.\n\n4. Copy the contents of the downloaded `.R` file and paste them into the new R script file you created in RStudio.\n\n5. Save the new R script file in your project directory for easy access during the lab.\n\n[Download the R script for Lab 01](https://raw.githubusercontent.com/go-bayes/psyc-434-2024/main/laboratory/01-lab.R)\n:::\n\n\n## Overview\n- Understanding causal diagrams: definitions and applications\n- Introduction to five elementary structures and four rules in causal inference\n- Introduction to R interface and data simulation\n\n  \n\n\n## Review\n\n1. All research begins with a question. Our question should be stated first. What do we want to know? For whom does this knowledge apply? \n\n2. In psychological research, we typically want to understand the causes and consequences of thought and behaviour - \"What if?\" questions.\n\n3.  The following concepts help us to describe two types of failure mode:\n\n  - **External Validity**:  the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods. That is, we want to know if our findings carry beyond the *sample population* to the *target population*.   We fail when our study does not generalise as we think. \n  \n  - **Internal Validity**: the degree to which a study can demonstrate that a causal relationship exists between the 'independent' and 'dependent' variables, free of confounding. In this course, we will use the terms \"outcome\" and \"treatment\" in place of \"independent\" and \"dependent\".[^note]\n\n  [^note]: We use the terms \"treatment\" and \"outcome\" because \"What if?\" questions implicitly invoke the idea of intervening on the world. \"If we did this, *then* what would happen to that...?\n\n5.  Our focus in the first four weeks of the course:  challenges to internal validity from **confounding bias.**  \n\n\n::: {#def-confounding}\nWe say there is \n:::\n\n\n::: {.callout-defintion}\n## Download Your Laboratory R Script Here\n\nAssuming you have installed R and RStudio:\n\n1. Download the R script for Lab 01 by clicking the link below. This script contains the code you will work with during your laboratory session.\n\n2. After downloading, open RStudio. \n\n3. In RStudio, create a new R script file by going to `File > New File > R Script`.\n\n4. Copy the contents of the downloaded `.R` file and paste them into the new R script file you created in RStudio.\n\n5. Save the new R script file in your project directory for easy access during the lab.\n\n[Download the R script for Lab 01](https://raw.githubusercontent.com/go-bayes/psyc-434-2024/main/laboratory/01-lab.R)\n:::\n\n\n\n## Causal Diagrams -- or \"Causal Directed Acyclic Graphs\"\n\n\n\n\n\n## Lab -- Regression in R\n\n\n\n## Simulating Data in R:  `Outcome ~ Treatment`\n\n\n#### Step 1: Set Up Your R Environment\n\nEnsure R or RStudio is installed and open. \n\n\n#### Step 2: Set a Seed for Reproducibility\n\nTo ensure that your simulated data can be reproduced exactly, it's good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # use any number to set the seed\n```\n:::\n\n\n#### Step 3: Simulating Continuous Data\n\nTo simulate continuous data, you can use functions like `rnorm()` for normal distributions, `runif()` for uniform distributions, etc. Here we simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100 # number of observations\nmean <- 50\nsd <- 10\ndata_continuous <- rnorm(n, mean, sd)\n\n# view\nhead(data_continuous)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 44.39524 47.69823 65.58708 50.70508 51.29288 67.15065\n```\n\n\n:::\n:::\n\n\n\n#### Step 4: Simulating Categorical Data\n\nCategorical data can be simulated using the `sample()` function. Here, we simulate a binary variable (gender) with two levels for 100 observations. There is equal probability of assignment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevels <- c(\"Male\", \"Female\")\ndata_categorical <- sample(levels, n, replace = TRUE)\n\n# view\nhead(data_categorical)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Female\" \"Female\" \"Female\" \"Male\"   \"Female\" \"Female\"\n```\n\n\n:::\n:::\n\n\nTo generate categories with unequal probabilities, you can use the `sample()` function by specifying the `prob` parameter, which defines the probability of selecting each level. This allows for simulating categorical data where the distribution between categories is not uniform. \n\nBelow is an example that modifies your initial code to create a categorical variable with unequal probabilities for \"Male\" and \"Female\". Here is an example with unequal probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define levels and number of observations\nlevels <- c(\"Male\", \"Female\")\nn <- 100 # total number of observations\n\n# Generate categorical data with unequal probabilities\ndata_categorical_unequal <- sample(levels, n, replace = TRUE, prob = c(0.3, 0.7))\n\n# View the first few elements\nhead(data_categorical_unequal)\n```\n:::\n\n\nIn this example, the `prob` parameter is set to `c(0.3, 0.7)`, indicating a 30% probability for \"Male\" and a 70% probability for \"Female\". This results in a simulated dataset where approximately 30% of the observations are \"Male\" and 70% are \"Female\", reflecting the specified unequal probabilities. Adjust the probabilities as needed to fit the scenario you wish to simulate.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) #  reproducibility\ngroupA_scores <- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores <- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\nscores_df <- data.frame(Group = rep(c(\"A\", \"B\"), each = 100), Scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\nstr(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t200 obs. of  2 variables:\n $ Group : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ Scores: num  91.6 96.5 123.4 101.1 101.9 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# summary of columns\nsummary(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Group               Scores      \n Length:200         Min.   : 65.36  \n Class :character   1st Qu.: 92.59  \n Mode  :character   Median :101.38  \n                    Mean   :102.37  \n                    3rd Qu.:111.59  \n                    Max.   :153.62  \n```\n\n\n:::\n\n```{.r .cell-code}\n# top rows\nhead(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Group    Scores\n1     A  91.59287\n2     A  96.54734\n3     A 123.38062\n4     A 101.05763\n5     A 101.93932\n6     A 125.72597\n```\n\n\n:::\n\n```{.r .cell-code}\n# bottom rows\ntail(scores_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Group    Scores\n195     B  85.33798\n196     B 134.95820\n197     B 114.01063\n198     B  86.23093\n199     B  95.83251\n200     B  87.21780\n```\n\n\n:::\n:::\n\n\n\n## Visualising simulated data\n\nUnderstanding your data visually is as important as the statistical analysis itself. Let's create a simple plot to compare the score distributions between the two groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# plot your data\nggplot(scores_df, aes(x = Group, y = Scores, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n```\n\n::: {.cell-output-display}\n![Score Distribution by Group](02-content_files/figure-pdf/visualize-data-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Histogram {#sec-histogram}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# H=histograms for both groups\nggplot(scores_df, aes(x = Scores, fill = Group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~Group, ncol = 1)\n```\n\n::: {.cell-output-display}\n![](02-content_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Excercise 3\n\n1. Modify the simulation parameters to change each group's mean and standard deviation. Observe how these changes affect the distribution.\n\n2. Go to the [histogram](#sec-histogram). Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another. \n\n\n## Simulating data for familiar statistical tests \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate some data\ndata <- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nt.test(data, mu = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  data\nt = 11.796, df = 99, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.931989 5.308942\nsample estimates:\nmean of x \n 5.120465 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate data for two groups\ngroup1 <- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 <- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nt.test(group1, group2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  group1 and group2\nt = -2.0293, df = 97.95, p-value = 0.04514\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.837548023 -0.009343886\nsample estimates:\nmean of x mean of y \n 5.002054  5.425500 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate pre-test and post-test scores\npre_test <- rnorm(30, mean = 80, sd = 10)\npost_test <- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nt.test(pre_test, post_test, paired = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPaired t-test\n\ndata:  pre_test and post_test\nt = -4.7761, df = 29, p-value = 4.725e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -6.785042 -2.716352\nsample estimates:\nmean difference \n      -4.750697 \n```\n\n\n:::\n:::\n\n\n### **Exercise 3: Linear Regression Analysis with Simulated Data**\n\n#### **Task 1: Simulating Continuous Treatment Variable**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library for enhanced model reporting\nlibrary(parameters)\n\n# set seed for reproducibility\nset.seed(123) # choose a seed number for consistency\n\n# define the number of observations\nn <- 100 # total observations\n\n# simulate continuous treatment variable A\na <- rnorm(n, mean = 50, sd = 10) # mean = 50, sd = 10 for A\n\n# specify the effect size of A on Y\nbeta_a <- 2 # explicit effect size\n\n# simulate outcome variable Y including an error term\ny <- 5 + beta_a * a + rnorm(n, mean = 0, sd = 20) # Y = intercept + beta_a*A + error\n\n# create a dataframe\ndf <- data.frame(a = a, y = y)\n\n# view the structure and first few rows of the dataframe\nstr(df)\nhead(df)\n```\n:::\n\n\n#### **Task 2: Exploring the Simulated Data**\n\nBefore moving on to regression analysis, ensure students understand the structure and distribution of the simulated data. Encourage them to use `summary()`, `plot()`, and other exploratory data analysis functions.\n\n#### **Task 3: Regression Analysis of Continuous Treatment Effect**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform linear regression of Y on A\nfit <- lm(y ~ a, data = df)\n\n# display the regression model summary\nsummary(fit)\n\n# report the model in a reader-friendly format\nreport_fit <- report::report(fit)\nprint(report_fit)\n\n# optionally, visualize the relationship\nplot(df$a, df$y, main = \"Scatterplot of Y vs. A\", xlab = \"Treatment (A)\", ylab = \"Outcome (Y)\")\nabline(fit, col = \"red\")\n```\n:::\n\n\n\n\n## Equivalence of ANOVA and Regression\n\n\nWe will simulate data in R to show that a one-way ANOVA is a special case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings).\n\n## Method\n\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a \"dependant\") variable. This setup allows us to apply both ANOVA and linear regression for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# nice tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n\nset.seed(321) # reproducibility\nn <- 90 # total number of observations\nk <- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup <- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulate outcome variable\nmeans <- c(100, 100, 220) # Mean for each group\nsd <- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny <- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 <- cbind.data.frame(y, group)\n\nanova_model <- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova <- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThe ANOVA (formula: y ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 87) =\n689.11, p < .001; Eta2 = 0.94, 95% CI [0.92, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n```\n\n\n:::\n:::\n\n\n\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit <- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit <- parameters::model_parameters(fit)\n\n# print table\ntable_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter   | Coefficient |   SE |           95% CI | t(87) |      p\n--------------------------------------------------------------------\n(Intercept) |      101.22 | 2.60 | [ 96.06, 106.39] | 38.98 | < .001\ngroup [2]   |       -0.80 | 3.67 | [ -8.10,   6.50] | -0.22 | 0.827 \ngroup [3]   |      117.67 | 3.67 | [110.37, 124.97] | 32.04 | < .001\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit <- report_parameters(fit)\n\n#print\nreport_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  - The intercept is statistically significant and positive (beta = 101.22, 95% CI [96.06, 106.39], t(87) = 38.98, p < .001; Std. beta = -0.68, 95% CI [-0.76, -0.59])\n  - The effect of group [2] is statistically non-significant and negative (beta = -0.80, 95% CI [-8.10, 6.50], t(87) = -0.22, p = 0.827; Std. beta = -0.01, 95% CI [-0.14, 0.11])\n  - The effect of group [3] is statistically significant and positive (beta = 117.67, 95% CI [110.37, 124.97], t(87) = 32.04, p < .001; Std. beta = 2.04, 95% CI [1.91, 2.17])\n```\n\n\n:::\n:::\n\n\n\n## Upshot\n\nANOVA partitions variance into between-group and within-group components, while regression models the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when we are comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain understanding. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, Along the way, you'll learn more about data visualisation, modelling, and reporting.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# graph the output of the parameters table\n# visualisation\nplot(table_fit)\n```\n\n::: {.cell-output-display}\n![](02-content_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Exercise 3\n\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\n1. **Simulate Data:**\n   - Generate two continuous variables, `Y` and `A`, with `n = 100` observations each.\n   - The variable `A` should have a mean of `50` and a standard deviation (`sd`) of `10`.\n\n2. **Define the Relationship:**\n   - Simulate the variable `Y` such that it is linearly related to `A` with a specified effect size. The effect size of `A` on `Y` must be explicitly defined as `2`.\n\n3. **Incorporate an Error Term:**\n   - When simulating `Y`, include an error term with a standard deviation (`sd`) of `20` to introduce variability.\n\n4. **Regression Analysis:**\n   - Use the `lm()` function in R to regress `Y` on `A`.\n   - Ensure the regression model captures the specified effect of `A` on `Y`.\n\n5. **Report the Results:**\n   - Output the regression model summary to examine the coefficients, including the effect of `A` on `Y`, and assess the model's overall fit and significance.\n\n\nHere is a template to get you started. Copy the code and paste it into your R script. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn <-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA <- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A <-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 <- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 <- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)\n```\n:::\n\n\n\n\n#### Step 5: Simulating Data Frames\n\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame <- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\n```\n:::\n\n\n\nNote that you can sample probabilistically for your groups\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))\n```\n:::\n\n\n## More complexity \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set the number of observations\nn <- 100\n\n# simulate the 'Age' variable\nmean_age <- 30\nsd_age <- 5\nAge <- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept <- 20000   # Intercept for the income equation\nbeta_age <- 1500     # Coefficient for the effect of age on income\nerror_sd <- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome <- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex <- data.frame(Age, Income)\n```\n:::\n\n\n#### Step 7: Visualising Simulated Data\n\nVisualising your simulated data can help understand its distribution and relationships. Use the `ggplot2` package for this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")\n```\n\n::: {.cell-output-display}\n![](02-content_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Practice\nSimulating data is a powerful method to understand statistical concepts and data manipulation. Let's simulate a simple dataset representing scores from two cultural groups.\n\n\n- **Data simulation:** \n\nYou've learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations! \n\n\n## Appendix A: Solutions {#appendix-a}\n\n\n### Solution Problem Set 3: simulate data and regression reporting \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\n#  seed for reproducibility\nset.seed(12345)\n\n# number of observations\nn <- 100\n\n# simulate data for variable A with specified mean and sd\nA <- rnorm(n, mean = 50, sd = 10)\n\n# define the specified effect size of A on Y\nbeta_A <- 2\n\n\n# simulate data and make data frame in one step\n\ndf_3 <- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A =  rnorm(n, mean = 50, sd = 10),\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20)\n)\n\n# view\nhead(df_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         A         Y\n1 52.23925  87.98766\n2 38.43777 106.60413\n3 54.22419 107.68437\n4 36.75245 117.09730\n5 51.41084 133.74473\n6 44.63952  70.74512\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(df_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t100 obs. of  2 variables:\n $ A: num  52.2 38.4 54.2 36.8 51.4 ...\n $ Y: num  88 107 108 117 134 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform linear regression of Y on A\n\nfit_3 <- lm(Y ~ A, data = df_3)\n\n# Report the results of the regression\n# summary(model)\n\n# report\nparameters::model_parameters(fit_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nParameter   | Coefficient |    SE |          95% CI | t(98) |      p\n--------------------------------------------------------------------\n(Intercept) |      109.17 | 14.62 | [80.17, 138.18] |  7.47 | < .001\nA           |   -3.80e-03 |  0.28 | [-0.57,   0.56] | -0.01 | 0.989 \n```\n\n\n:::\n\n```{.r .cell-code}\nreport(fit_3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWe fitted a linear model (estimated using OLS) to predict Y with A (formula: Y\n~ A). The model explains a statistically not significant and very weak\nproportion of variance (R2 = 1.83e-06, F(1, 98) = 1.79e-04, p = 0.989, adj. R2\n= -0.01). The model's intercept, corresponding to A = 0, is at 109.17 (95% CI\n[80.17, 138.18], t(98) = 7.47, p < .001). Within this model:\n\n  - The effect of A is statistically non-significant and negative (beta =\n-3.80e-03, 95% CI [-0.57, 0.56], t(98) = -0.01, p = 0.989; Std. beta =\n-1.35e-03, 95% CI [-0.20, 0.20])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n\n\n:::\n:::\n\n\n\n\n## What You Have Learned\n\n- **Data simulation:** \n\nYou've learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations! \n  \n- **Data visualisation:** \n\nYou've begun data visualising data through boxplots and histograms and coefficient plots, which is crucial for analysing and communicating statistical findings.\n  \n- **Statistical tests:** You've conducted basic statistical tests, including t-tests and ANOVA, gaining insights into comparing means across groups.\n  \n- **Understanding ANOVA and regression:** \n\nYou've explored the equivalence of ANOVA and regression analysis, learning how these methods can be applied to analyse and interpret data effectively.\n\n\n## For more information about the packages used here:\n\n  - [ggplot2](https://ggplot2.tidyverse.org/): A system for declaratively creating graphics, based on The Grammar of Graphics.\n\n  - [Parameters package](https://easystats.github.io/parameters/): Provides utilities for processing model parameters and their metrics.\n\n  - [Report package](https://easystats.github.io/report/index.html): Facilitates the automated generation of reports from statistical models.\n\n## Appendix A: I lied\n\nWe can get group comparisons with ANOVA, for example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc <- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(tukey_post_hoc)\n```\n\n::: {.cell-output-display}\n![](02-content_files/figure-pdf/unnamed-chunk-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nRegression and ANOVA are equivalent\n\n\n\n",
    "supporting": [
      "02-content_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}