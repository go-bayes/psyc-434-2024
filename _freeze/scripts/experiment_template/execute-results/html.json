{
  "hash": "3ee7f3f207d050491346ca05f7c7690f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"experiment_template.qmd\"\nformat: \n  html:\n    code-fold: true\n    code-overflow: scroll\n    html-math-method: katex\n    warnings: false\n    error: false\n    messages: false\n    highlight: kate\n---\n\n\nToday we will do an analysis, proceeding in a step-by-step way. \n\nRecall our checklist:\n\n1. Stating the Question: Is my question clearly stated? If not, state it.\n2. Relevance of the Question: Have I explained its importance? If not, explain.\n3. Causality of the Question: Is my question causal? If not, refine your question.\n4. Subgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\n\n## Questions \n\nQuestion 1. Does exercise affect anxiety/depression? \n  \nQuestion 2: Do these effects vary among NZ Europeans and Māori? \n\n\nAre these questions clearly stated? No, they are vague: \n\n- How much exercise? \n- By which measures of depression? \n- When should the effects be observed?  \n\nRecall we can help to clarify these questions by attempting to emulate an experiment. \n\nWe shall use the measure of exercise in the NZAVS: hours of activity per week (we shall see, the question remains vague).\n\nWe will assess the 1-year effect on Kessler-6 depression after initiating a change in exercise (intention-to-treat).\n\nWe will investigate effect-modification by NZ European and Māori ethnic identification.\n\n\n\n## Preliminaries: source functions, import data. \n\nWe source our functions, load libraries, and important our (synthetic) data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# PSYCH 434: Example script for assessment 3 and 5.\n\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/libs2.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/funs.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/experimental_funs.R\")\n############## ############## ############## ############## ############## ############## ############## ########\n#########  ############## ############## IMPORT DATA ##############  ############## ############## ##############\n############## ############## ############## ############## ############## ############## ############## ########\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables. \ntab_ate_subgroup_rd <- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages <- c(\"EValue\", \"dplyr\")\n  new_packages <-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols <- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols <- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) > 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x <- x %>%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %>%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error <- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list <- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue <- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] <- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] <- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2,], 3)) # exclude the NA column\n  })\n  \n  evalues_df <- do.call(rbind, evalues_list)\n  colnames(evalues_df) <- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p <- cbind(x, evalues_df)\n  \n  tab <-\n    tab_p |> select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth <-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n```\n:::\n\n\n\n\nNext, we will inspect column names. \n\nMake sure to familiarise your self with the variable names [here](https://github.com/go-bayes/psych-434-2023/blob/main/data/readme.qmd)\n\nIt's a good idea to plot the data\n\n\n::: {.cell}\n\n:::\n\n\n\n### Data Wrangling.\n\nNext, we'll get the data into shape.  \n\nConsider the following causal questions: \"Does exercise affect well-being?\" \"Do such effects, if they exist, differ by ethnicity?\"\n\nThese questions are not precise.  What type of excercise?  How regularly must one exercise? For how long must one exercise? Which ethnicities shall we compare?  Why? \n\nIt helpst to think like an experimentalist... (say more)\n\nIn this exampe, we'll do the following: \n\n1. Create a Kessler 6 average score\n2. Create a Kessler 6 sum score \n3. Create a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\n4. Create a log Exercise score\n5. Create a coarsened Exercise score. \n\n\nConsider: the NZAVS asks participants the following question. During the past week, list \"Hours spent exercising/physical activity.\"  The question is inherently unclear about what sort of physical activity someone is doing.  When participants respond to this question, what do they mean?  John considers anything that is not sleep to be physical activity. He returns a high number. Jane only counts aerobic exercise. For Jane, walking an hour to work and back doesn't count. \n\nRecall that an assumption of causal inference is consistency. (Say more ... then leave to the side.)\n\n\nFor now, let's create the indicators. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create sum score of kessler 6\ndt_start <- nzavs_synth %>%\n  arrange(id, wave) %>%\n  rowwise() %>%\n  mutate(kessler_6  = mean(\n    sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )))) |>\n  mutate(kessler_6_sum = round(\n    sum(c (kessler_depressed,\n                   kessler_hopeless,\n                   kessler_nervous,\n                   kessler_effort,\n                   kessler_restless,\n                   kessler_worthless)),\n    digits = 0\n  )) |>  ungroup() |>\n# Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n       labels = c(\n        \"not_depressed\",\n        \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |>\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) |> # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\n        \"inactive\",\n        \"active\",\n        \"very_active\"      ),\n      # Define thresholds for categories\n      levels = c(\"(-1,2]\", \"(2,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |>\n\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n```\n:::\n\n\n\nWe next do some data checks.  I will leave you to do these in your own time. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax( dt_start$hours_exercise)\nmin( dt_start$hours_exercise)\n# checks\ntable(is.na(dt_start$kessler_6_coarsen))\ntable(is.na(dt_start$hours_exercise_coarsen))\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below < or = to 20\nhist(subset(dt_start, hours_exercise <= 20)$hours_exercise)\n\n\n# inspect kessler 6\ntable(dt_start$kessler_6_coarsen)\ntable(dt_start$hours_exercise_coarsen)\n\nhist( as.numeric(dt_start$kessler_6_coarsen) )\nhist( as.numeric(dt_start$hours_exercise_coarsen))\n```\n:::\n\n\n### CFA for Kessler 6\n\n\nWe have learned how to do confirmatory factor analysis. Let's put this knowledge to use, and consider whether Kessler 6 is one variable.\n\n\nThe code below will:\n\n* Load required packages. \n* Select the Kessler 6 items\n* Check whether there is sufficient correlation among the variables to support factor analysis. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Suppose we have reason to think Kessler 6 isn't one thing.\n# Let's put our factor analysis skills to work\n# Here we will use the paramters and see packages for R (part of the Easystats suite)\n\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: psych\n```\n\n\n:::\n\n```{.r .cell-code}\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: see\n```\n\n\n:::\n\n```{.r .cell-code}\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lavaan\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThis is lavaan 0.6-16\nlavaan is FREE software! Please report any bugs.\n```\n\n\n:::\n\n```{.r .cell-code}\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: datawizard\n```\n\n\n:::\n\n```{.r .cell-code}\n# select the columns we need. \ndt_only_k6 <- dt_start |> select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 70564.23, p < .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.86). The individual KMO scores are: kessler_depressed (0.83), kessler_effort (0.89), kessler_hopeless (0.85), kessler_worthless (0.85), kessler_nervous (0.88), kessler_restless (0.85).\n```\n\n\n:::\n:::\n\n\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa <- psych::fa(dt_only_k6, nfactors = 3) %>%\n  model_parameters(sort = TRUE, threshold = \"max\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required namespace: GPArotation\n```\n\n\n:::\n\n```{.r .cell-code}\nefa\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          | MR1  | MR2  | MR3  | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_depressed | 0.85 |      |      |    1.01    |    0.33   \nkessler_worthless | 0.79 |      |      |    1.00    |    0.35   \nkessler_hopeless  | 0.75 |      |      |    1.02    |    0.33   \nkessler_nervous   |      | 1.00 |      |    1.00    |  5.00e-03 \nkessler_restless  |      |      | 0.69 |    1.02    |    0.52   \nkessler_effort    |      |      | 0.48 |    1.66    |    0.50   \n\nThe 3 latent factors (oblimin rotation) accounted for 66.05% of the total variance of the original data (MR1 = 35.14%, MR2 = 17.17%, MR3 = 13.73%).\n```\n\n\n:::\n\n```{.r .cell-code}\n# This output presents the results of an exploratory factor analysis (EFA), a statistical method used to discover the underlying structure of a relatively large set of variables. It's often used when you don't have a specific hypothesis about what latent factors (unobservable variables) might be influencing the observed variables in your dataset.\n#\n# In this analysis, we've requested three factors (latent variables), and the table presents the loadings of each observed variable on each of these factors. The loadings can be interpreted as the correlations between the observed variables and the latent factors.\n#\n# Here's how to interpret the output:\n#\n#   The variables kessler_depressed, kessler_worthless, and kessler_hopeless load strongly on the first latent factor (MR1), and do not significantly load on the other two. This suggests that these three variables share some common underlying factor.\n#\n# The variable kessler_nervous loads exclusively on the second latent factor (MR2), suggesting it might represent a different latent construct.\n#\n# The variables kessler_restless and kessler_effort load on the third latent factor (MR3), which could represent yet another underlying construct.\n#\n# The \"Complexity\" column indicates the complexity of each item. Complexity 1 indicates that the item is influenced mostly by a single factor.\n#\n# The \"Uniqueness\" column represents the proportion of variance in each variable that is not explained by the factors. For example, the uniqueness of kessler_depressed is 0.33, which means that 33% of the variance in this variable is not accounted for by the three factors.\n#\n# Lastly, the total variance explained by the three latent factors is 66.05%, with MR1 explaining 35.14%, MR2 explaining 17.17%, and MR3 explaining 13.73%. This indicates that about two-thirds of the variance in the six observed variables can be explained by the three latent factors extracted in the analysis.\n# fa -- there is no agreed method!\n# method of agreement:\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- n_factors(dt_only_k6)\n\n# # summary\n# as.data.frame(n)\n\n# plot of smmary\nplot(n) + theme_modern()\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/plot_factors-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## CFA\n```\n:::\n\n\nNext try a CFA\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first partition the data \npart_data <- datawizard::data_partition(dt_only_k6, traing_proportion = .07, seed = 123)\n\ntraining <- part_data$p_0.7\ntest <- part_data$test\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#|label: cfa_all\n# one factor\nstructure_k6_one <- psych::fa(training, nfactors = 1) |>\n  efa_to_cfa()\n\n# two factor model\nstructure_k6_two <- psych::fa(training, nfactors = 2) |>\n  efa_to_cfa()\n\n# three structure model\nstructure_k6_three <- psych::fa(training, nfactors = 3) %>%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n```\n\n\n:::\n\n```{.r .cell-code}\nstructure_k6_two\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n```\n\n\n:::\n\n```{.r .cell-code}\nstructure_k6_three\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_restless\nMR3 =~ kessler_nervous + .row_id\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit and compare models\none_latent <- suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\ntwo_latents <- suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\nthree_latents <- suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\ncompare <- performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare)|>\n  kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|Name          |Model  |      Chi2| Chi2_df| p_Chi2|  Baseline| Baseline_df| p_Baseline|       GFI|      AGFI|       NFI|      NNFI|       CFI|     RMSEA| RMSEA_CI_low| RMSEA_CI_high|   p_RMSEA|      RMR|      SRMR|       RFI|      PNFI|       IFI|       RNI| Loglikelihood|      AIC| AIC_wt|      BIC| BIC_wt| BIC_adjusted|\n|:-------------|:------|---------:|-------:|------:|---------:|-----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|------------:|-------------:|---------:|--------:|---------:|---------:|---------:|---------:|---------:|-------------:|--------:|------:|--------:|------:|------------:|\n|one_latent    |lavaan | 1359.7168|      14|      0| 159746.19|          21|          0| 0.9533955| 0.9067909| 0.9914883| 0.9873622| 0.9915748| 0.1033455|    0.0987385|     0.1080285| 0.0000000| 36.00334| 0.0493327| 0.9872324| 0.6609922| 0.9915752| 0.9915748|     -151483.7| 302995.3|      0| 303094.8|      0|     303050.3|\n|two_latents   |lavaan |  317.9709|      13|      0|  30915.75|          21|          0| 0.9900793| 0.9786322| 0.9897149| 0.9840541| 0.9901287| 0.0510548|    0.0462789|     0.0559908| 0.3499758| 36.31236| 0.0226983| 0.9833856| 0.6126807| 0.9901313| 0.9901287|     -150962.8| 301955.6|      1| 302062.2|      1|     302014.5|\n|three_latents |lavaan |  747.8723|      12|      0|  20903.30|          21|          0| 0.9763317| 0.9447739| 0.9642223| 0.9383317| 0.9647609| 0.0825447|    0.0775761|     0.0876237| 0.0000000| 37.13824| 0.0377955| 0.9373890| 0.5509842| 0.9647761| 0.9647609|     -151177.7| 302387.5|      0| 302501.2|      0|     302450.3|\n\n\n:::\n:::\n\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\n#### One_latent Model: This model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p<0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can't solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n#### Two_latents Model: This model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n#### Three_latents Model: This model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\n\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It's also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically. \n\n\n\n\n### Try with multiple groups\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select the columns we need + ethnicity\ndt_eth_k6_eth <- dt_start |> \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |> \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless, eth_cat)\n\n\n\n# remove ethnicity for traning data \n\n\n\n\n# first partition the data \npart_data_eth <- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .07, seed = 123, group = \"eth_cat\")\n\ntraining_eth <- part_data_eth$p_0.7\ntest_eth <- part_data_eth$test\n\n\n# fit and compare models for configural equivalence\none_latent_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural <- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\ncompare_eth_configural <- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n\n# fit and compare models for metric equivalence\none_latent_eth_metric <- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  <- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  <- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\ncompare_eth_metric  <- performance::compare_performance(one_latent_eth_metric, \n                                                        two_latents_eth_metric, \n                                                        three_latents_eth_metric, \n                                                        verbose = FALSE)\n\n\n# fit and compare models for scalar equivalence\none_latent_eth_scalar <- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  <- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  <- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\ncompare_eth_scalar  <- performance::compare_performance(one_latent_eth_scalar, \n                                                        two_latents_eth_scalar, \n                                                        three_latents_eth_scalar, \n                                                        verbose = FALSE)\n```\n:::\n\n\nRecall, in the context of measurement and factor analysis, the concepts of *configural*, *metric*, and *scalar invariance* relate to the comparability of a measurement instrument, such as a survey or test, across different groups. \n\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\n\nLet's first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\n1. **Configural invariance** refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying constructs (factors) are defined the same way for different groups. This doesn't mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups. \n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\n2. **Metric invariance** (also known as \"weak invariance\") refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups. \n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans. \n\n3. **Scalar invariance** (also known as \"strong invariance\") involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\n\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\n### Configural invariance:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(compare_eth_configural)|>\n  kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|Name                         |Model  |      Chi2| Chi2_df| p_Chi2|  Baseline| Baseline_df| p_Baseline|       GFI|      AGFI|       NFI|      NNFI|       CFI|     RMSEA| RMSEA_CI_low| RMSEA_CI_high|   p_RMSEA|      RMR|      SRMR|       RFI|      PNFI|       IFI|       RNI| Loglikelihood|      AIC| AIC_wt|      BIC| BIC_wt| BIC_adjusted|\n|:----------------------------|:------|---------:|-------:|------:|---------:|-----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|------------:|-------------:|---------:|--------:|---------:|---------:|---------:|---------:|---------:|-------------:|--------:|------:|--------:|------:|------------:|\n|one_latent_eth_configural    |lavaan | 1162.4746|      14|      0| 341229.17|          21|          0| 0.9831752| 0.9579381| 0.9965933| 0.9949511| 0.9966341| 0.1027048|    0.0977499|     0.1077479| 0.0000000| 38.29915| 0.0439380| 0.9948899| 0.6643955| 0.9966342| 0.9966341|     -129452.1| 258946.1|      0| 259092.3|      0|     259025.5|\n|two_latents_eth_configural   |lavaan |  276.7703|      13|      0|  42034.04|          21|          0| 0.9961916| 0.9897467| 0.9934156| 0.9898581| 0.9937217| 0.0510782|    0.0459383|     0.0564019| 0.3560216| 34.17489| 0.0201464| 0.9893636| 0.6149715| 0.9937229| 0.9937217|     -129009.2| 258062.4|      1| 258215.5|      1|     258145.6|\n|three_latents_eth_configural |lavaan |  701.6287|      12|      0|  27397.50|          21|          0| 0.9906044| 0.9725962| 0.9743908| 0.9559166| 0.9748095| 0.0859629|    0.0806184|     0.0914299| 0.0000000| 76.26852| 0.0358868| 0.9551839| 0.5567947| 0.9748177| 0.9748095|     -129221.6| 258489.3|      0| 258649.3|      0|     258576.2|\n\n\n:::\n:::\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It's the most basic form of measurement invariance.\n\nLooking at the results, we can draw the following conclusions:\n\n1. **Chi2 (Chi-square)**: A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\n\n2. **GFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index)**: These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\n\n3. **NFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index)**: These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\n\n4. **RMSEA (Root Mean Square Error of Approximation)**: Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\n\n5. **RMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual)**: Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\n\n6. **RFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index)**: These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\n\n7. **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\n\n8. **p_Chi2 and p_RMSEA**: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p > 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model.\n\n### Metric Equivalence\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(compare_eth_metric)|>\n  kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|Name                     |Model  |      Chi2| Chi2_df| p_Chi2|  Baseline| Baseline_df| p_Baseline|       GFI|      AGFI|       NFI|      NNFI|       CFI|     RMSEA| RMSEA_CI_low| RMSEA_CI_high|   p_RMSEA|      RMR|      SRMR|       RFI|      PNFI|       IFI|       RNI| Loglikelihood|      AIC| AIC_wt|      BIC| BIC_wt| BIC_adjusted|\n|:------------------------|:------|---------:|-------:|------:|---------:|-----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|------------:|-------------:|---------:|--------:|---------:|---------:|---------:|---------:|---------:|-------------:|--------:|------:|--------:|------:|------------:|\n|one_latent_eth_metric    |lavaan | 1162.4746|      14|      0| 341229.17|          21|          0| 0.9831752| 0.9579381| 0.9965933| 0.9949511| 0.9966341| 0.1027048|    0.0977499|     0.1077479| 0.0000000| 38.29915| 0.0439380| 0.9948899| 0.6643955| 0.9966342| 0.9966341|     -129452.1| 258946.1|      0| 259092.3|      0|     259025.5|\n|two_latents_eth_metric   |lavaan |  276.7703|      13|      0|  42034.04|          21|          0| 0.9961916| 0.9897467| 0.9934156| 0.9898581| 0.9937217| 0.0510782|    0.0459383|     0.0564019| 0.3560216| 34.17489| 0.0201464| 0.9893636| 0.6149715| 0.9937229| 0.9937217|     -129009.2| 258062.4|      1| 258215.5|      1|     258145.6|\n|three_latents_eth_metric |lavaan |  701.6287|      12|      0|  27397.50|          21|          0| 0.9906044| 0.9725962| 0.9743908| 0.9559166| 0.9748095| 0.0859629|    0.0806184|     0.0914299| 0.0000000| 76.26852| 0.0358868| 0.9551839| 0.5567947| 0.9748177| 0.9748095|     -129221.6| 258489.3|      0| 258649.3|      0|     258576.2|\n\n\n:::\n:::\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\n\nHere's the interpretation of the fit indices:\n\n1. **Chi2 (Chi-square)**: Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\n\n2. **GFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index)**: These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\n\n3. **NFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index)**: These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\n\n4. **RMSEA (Root Mean Square Error of Approximation)**: Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\n\n5. **RMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual)**: Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\n\n6. **RFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index)**: These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\n\n7. **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\n\n8. **p_Chi2 and p_RMSEA**: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p > 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\n### Scalar Equivalence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# view as html table\nas.data.frame(compare_eth_scalar)|>\n  kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|Name                     |Model  |      Chi2| Chi2_df| p_Chi2|  Baseline| Baseline_df| p_Baseline|       GFI|      AGFI|       NFI|      NNFI|       CFI|     RMSEA| RMSEA_CI_low| RMSEA_CI_high|   p_RMSEA|      RMR|      SRMR|       RFI|      PNFI|       IFI|       RNI| Loglikelihood|      AIC| AIC_wt|      BIC| BIC_wt| BIC_adjusted|\n|:------------------------|:------|---------:|-------:|------:|---------:|-----------:|----------:|---------:|---------:|---------:|---------:|---------:|---------:|------------:|-------------:|---------:|--------:|---------:|---------:|---------:|---------:|---------:|-------------:|--------:|------:|--------:|------:|------------:|\n|one_latent_eth_scalar    |lavaan | 1162.4746|      14|      0| 341229.17|          21|          0| 0.9831752| 0.9579381| 0.9965933| 0.9949511| 0.9966341| 0.1027048|    0.0977499|     0.1077479| 0.0000000| 38.29915| 0.0439380| 0.9948899| 0.6643955| 0.9966342| 0.9966341|     -129452.1| 258946.1|      0| 259092.3|      0|     259025.5|\n|two_latents_eth_scalar   |lavaan |  276.7703|      13|      0|  42034.04|          21|          0| 0.9961916| 0.9897467| 0.9934156| 0.9898581| 0.9937217| 0.0510782|    0.0459383|     0.0564019| 0.3560216| 34.17489| 0.0201464| 0.9893636| 0.6149715| 0.9937229| 0.9937217|     -129009.2| 258062.4|      1| 258215.5|      1|     258145.6|\n|three_latents_eth_scalar |lavaan |  701.6287|      12|      0|  27397.50|          21|          0| 0.9906044| 0.9725962| 0.9743908| 0.9559166| 0.9748095| 0.0859629|    0.0806184|     0.0914299| 0.0000000| 76.26852| 0.0358868| 0.9551839| 0.5567947| 0.9748177| 0.9748095|     -129221.6| 258489.3|      0| 258649.3|      0|     258576.2|\n\n\n:::\n:::\n\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance. \n\nHere's the interpretation of the fit indices:\n\n1. **Chi2 (Chi-square)**: Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\n\n2. **GFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index)**: These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\n\n3. **NFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index)**: These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\n\n4. **RMSEA (Root Mean Square Error of Approximation)**: Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\n\n5. **RMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual)**: Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\n\n6. **RFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index)**: These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\n\n7. **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\n\n8. **p_Chi2 and p_RMSEA**: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p > 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, one must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nOverall it seems that we have good evidence for the two-factor model of Kessler-6. \n\nLet's next get the data into shape for analysis. Here we create a variable for the two factors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get two factors from data\ndt_start2 <- dt_start |>\n  arrange(id, wave) |>\n  rowwise() |>\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |> ungroup()\n```\n:::\n\n\n\nIt is useful toinspect histograms\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dt_start2$kessler_latent_anxiety)\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/histogram_kessler_anxiety-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(dt_start2$kessler_latent_depression)\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/histogram_kessler_depression-1.png){width=672}\n:::\n:::\n\n\n\n### Assess change in the exposure\n\n\nNot this is just a description of the the summary scores. We do not assess change within indivuals \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure <- dt_start2 |>\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |>\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |>\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |>\n  droplevels()\n\n\n# check\ndt_exposure |>\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n```\n\n\n:::\n:::\n\n\n\nI've written a function called `transition_table` that will help us assess change in the exposure at the individual level.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#   consider people going from active to vary active\nout <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names <- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n```\n\n\n:::\n:::\n\n\nNext consider Māori only \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Maori only\n\ndt_exposure_maori <- dt_exposure |>\n  filter(eth_cat == \"māori\")\n\nout_m <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m<- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t_tab_m$table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# filter euro\ndt_exposure_euro <- dt_exposure |>\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e <- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\nt_tab_e <- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n```\n\n\n:::\n\n```{.r .cell-code}\n# table\nprint(t_tab_e$table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n```\n\n\n:::\n:::\n\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\n\n### Create wide data frame for analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n# To find out more about our dataset go here:\n# https://github.com/go-bayes/psych-434-2023/blob/main/data/readme.qmd\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# note that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare <-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(exclude_vars)\n\n  # Now:\n  data %>% select(all_of(exclude_vars))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %>% select(t0_column_order)\n\n  # Now:\n  data %>% select(all_of(t0_column_order))\n\nSee <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\n```\n\n\n:::\n:::\n\n\n\n\n## Descriptive table \n\nI created a simple function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# I have created a function that will allow you to take a data frame and\n# create a table\n# REDO\n```\n:::\n\n\n\nHowever, if would like like a nicer table, try this: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get data into shape\ndt_new <- dt_prepare %>%\n  select(starts_with(\"t0\")) %>%\n  rename_all(~ stringr::str_replace(., \"^t0_\", \"\")) %>%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |>\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\n\nbaseline_vars_names <- dt_new %>%\n  select(-WAVE) %>%\n  colnames()\n\ntable_baseline_vars <-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline <-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"Rtable1\"><table class=\"Rtable1\">\n<thead>\n<tr>\n<th class='rowlabel firstrow lastrow'></th>\n<th class='firstrow lastrow'><span class='stratlabel'>baseline<br><span class='stratn'>(N=10000)</span></span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td class='rowlabel firstrow'>EDU</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.85 (2.59)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>6.96 [-0.128, 10.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>MALE</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Male</td>\n<td>3905 (39.1%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Not_male</td>\n<td class='lastrow'>6095 (61.0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>ETH_CAT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>euro</td>\n<td>8641 (86.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>māori</td>\n<td>821 (8.2%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>pacific</td>\n<td>190 (1.9%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>asian</td>\n<td class='lastrow'>348 (3.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>EMPLOYED</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.836 (0.370)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>GEN_COHORT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Gen_Silent: born< 1946</td>\n<td>166 (1.7%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>Gen Boomers: born >= 1946 & b.< 1965</td>\n<td>4257 (42.6%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>GenX: born >=1961 & b.< 1981</td>\n<td>3493 (34.9%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>GenY: born >=1981 & b.< 1996</td>\n<td>1883 (18.8%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>GenZ: born >= 1996</td>\n<td class='lastrow'>201 (2.0%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NZ_DEP2018</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>4.46 (2.65)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>4.01 [0.835, 10.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NZSEI13</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>57.0 (16.1)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>61.0 [9.91, 90.1]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>PARTNER</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.795 (0.404)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>PARENT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.706 (0.456)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [0, 1.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>POL_ORIENT</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.47 (1.40)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.09 [0.862, 7.14]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>URBAN</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>rural</td>\n<td>1738 (17.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>urban</td>\n<td class='lastrow'>8262 (82.6%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>AGREEABLENESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.36 (0.986)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.48 [0.977, 7.13]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>CONSCIENTIOUSNESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.19 (1.03)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.28 [0.938, 7.16]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>EXTRAVERSION</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.85 (1.21)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.80 [0.861, 7.07]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>HONESTY_HUMILITY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.52 (1.12)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.71 [1.14, 7.15]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>OPENNESS</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>5.06 (1.10)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>5.12 [0.899, 7.15]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>NEUROTICISM</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>3.41 (1.17)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>3.31 [0.860, 7.08]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>MODESTY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>6.07 (0.860)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>6.24 [2.17, 7.17]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>RELIGION_IDENTIFICATION_LEVEL</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>2.19 (2.07)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.00 [1.00, 7.00]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>HOURS_EXERCISE_COARSEN</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>inactive</td>\n<td>3805 (38.1%)</td>\n</tr>\n<tr>\n<td class='rowlabel'>active</td>\n<td>4342 (43.4%)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>very_active</td>\n<td class='lastrow'>1853 (18.5%)</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>KESSLER_LATENT_ANXIETY</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>1.16 (0.719)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>1.03 [-0.0800, 4.03]</td>\n</tr>\n<tr>\n<td class='rowlabel firstrow'>KESSLER_LATENT_DEPRESSION</td>\n<td class='firstrow'></td>\n</tr>\n<tr>\n<td class='rowlabel'>Mean (SD)</td>\n<td>0.744 (0.686)</td>\n</tr>\n<tr>\n<td class='rowlabel lastrow'>Median [Min, Max]</td>\n<td class='lastrow'>0.646 [-0.0871, 4.02]</td>\n</tr>\n</tbody>\n</table>\n</div>\n```\n\n:::\n\n```{.r .cell-code}\n# another method for making a table\n# x <- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |>\n#   kable_material(c(\"striped\", \"hover\"))\n```\n:::\n\n\n\nSome more data wrangling. \n\n\n1. `mutate(id = factor(1:nrow(dt_prepare)))`: This creates a new column called `id` that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset. \n\n2. The next `mutate` operation is used to convert the `t0_eth_cat`, `t0_urban`, and `t0_gen_cohort` variables to factor type, if they are not already. \n\n3. The `filter` command is used to subset the dataset to only include rows where the `t0_eth_cat` is either \"euro\" or \"māori\". The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\n\n4. `ungroup()` ensures that there's no grouping in the dataframe.\n\n5. The `mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\"))` step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include \"_z\" at the end of their original names.\n\n6. The `select` function is used to keep only specific columns: the `id` column, any columns that are factors, and any columns that end in \"_z\". \n\n7. The `relocate` functions re-order columns. The first `relocate` places the `id` column at the beginning. The next three `relocate` functions order the rest of the columns based on their names: those starting with \"t0_\" are placed before \"t1_\" columns, and those starting with \"t2_\" are placed after \"t1_\" columns.\n\n8. `droplevels()` removes unused factor levels in the dataframe.\n\n9. Finally, `skimr::skim(dt)` will print out a summary of the data in the `dt` object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt <- dt_prepare|>\n  mutate(id = factor(1:nrow(dt_prepare))) |>\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |>\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |> # Too few asian and pacific\n  ungroup() |>\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %>%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |>\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |>\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |>\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |>\n  droplevels()\n\n# view object\nskimr::skim(dt)\n```\n\n::: {.cell-output-display}\n\n<table style='width: auto;'\n      class='table table-condensed'>\n<caption>Data summary</caption>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Name </td>\n   <td style=\"text-align:left;\"> dt </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of rows </td>\n   <td style=\"text-align:left;\"> 9462 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of columns </td>\n   <td style=\"text-align:left;\"> 26 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> _______________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Column type frequency: </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> factor </td>\n   <td style=\"text-align:left;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> 19 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ________________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Group variables </td>\n   <td style=\"text-align:left;\"> None </td>\n  </tr>\n</tbody>\n</table>\n\n\n**Variable type: factor**\n\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:left;\"> ordered </th>\n   <th style=\"text-align:right;\"> n_unique </th>\n   <th style=\"text-align:left;\"> top_counts </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> id </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 9462 </td>\n   <td style=\"text-align:left;\"> 1: 1, 2: 1, 3: 1, 4: 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_male </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;\"> Not: 5767, Mal: 3695 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_eth_cat </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;\"> eur: 8641, māo: 821 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_gen_cohort </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:left;\"> Gen: 4107, Gen: 3311, Gen: 1716, Gen: 164 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_urban </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:left;\"> urb: 7762, rur: 1700 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_hours_exercise_coarsen </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\"> act: 4131, ina: 3557, ver: 1774 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t1_hours_exercise_coarsen </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:left;\"> act: 4281, ina: 3187, ver: 1994 </td>\n  </tr>\n</tbody>\n</table>\n\n\n**Variable type: numeric**\n\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> p0 </th>\n   <th style=\"text-align:right;\"> p25 </th>\n   <th style=\"text-align:right;\"> p50 </th>\n   <th style=\"text-align:right;\"> p75 </th>\n   <th style=\"text-align:right;\"> p100 </th>\n   <th style=\"text-align:left;\"> hist </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> t0_edu_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -2.29 </td>\n   <td style=\"text-align:right;\"> -1.05 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n   <td style=\"text-align:right;\"> 1.66 </td>\n   <td style=\"text-align:left;\"> ▂▃▃▇▂ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_employed_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -2.26 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:left;\"> ▂▁▁▁▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_nz_dep2018_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.36 </td>\n   <td style=\"text-align:right;\"> -0.92 </td>\n   <td style=\"text-align:right;\"> -0.16 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 2.17 </td>\n   <td style=\"text-align:left;\"> ▇▆▆▅▂ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_nzsei13_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -2.94 </td>\n   <td style=\"text-align:right;\"> -0.75 </td>\n   <td style=\"text-align:right;\"> 0.25 </td>\n   <td style=\"text-align:right;\"> 0.81 </td>\n   <td style=\"text-align:right;\"> 2.07 </td>\n   <td style=\"text-align:left;\"> ▁▃▅▇▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_partner_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.99 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:left;\"> ▂▁▁▁▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_parent_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.58 </td>\n   <td style=\"text-align:right;\"> -1.58 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:left;\"> ▃▁▁▁▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_pol_orient_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.87 </td>\n   <td style=\"text-align:right;\"> -1.02 </td>\n   <td style=\"text-align:right;\"> -0.28 </td>\n   <td style=\"text-align:right;\"> 0.44 </td>\n   <td style=\"text-align:right;\"> 2.62 </td>\n   <td style=\"text-align:left;\"> ▇▆▇▅▂ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_agreeableness_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -4.46 </td>\n   <td style=\"text-align:right;\"> -0.62 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 1.79 </td>\n   <td style=\"text-align:left;\"> ▁▁▃▇▆ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_conscientiousness_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -4.13 </td>\n   <td style=\"text-align:right;\"> -0.65 </td>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> 0.76 </td>\n   <td style=\"text-align:right;\"> 1.91 </td>\n   <td style=\"text-align:left;\"> ▁▁▅▇▅ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_extraversion_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -2.48 </td>\n   <td style=\"text-align:right;\"> -0.71 </td>\n   <td style=\"text-align:right;\"> -0.04 </td>\n   <td style=\"text-align:right;\"> 0.72 </td>\n   <td style=\"text-align:right;\"> 2.67 </td>\n   <td style=\"text-align:left;\"> ▂▆▇▅▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_honesty_humility_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -3.95 </td>\n   <td style=\"text-align:right;\"> -0.69 </td>\n   <td style=\"text-align:right;\"> 0.17 </td>\n   <td style=\"text-align:right;\"> 0.84 </td>\n   <td style=\"text-align:right;\"> 1.45 </td>\n   <td style=\"text-align:left;\"> ▁▁▃▆▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_openness_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -3.76 </td>\n   <td style=\"text-align:right;\"> -0.71 </td>\n   <td style=\"text-align:right;\"> 0.05 </td>\n   <td style=\"text-align:right;\"> 0.81 </td>\n   <td style=\"text-align:right;\"> 1.90 </td>\n   <td style=\"text-align:left;\"> ▁▂▆▇▅ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_neuroticism_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -2.18 </td>\n   <td style=\"text-align:right;\"> -0.76 </td>\n   <td style=\"text-align:right;\"> -0.09 </td>\n   <td style=\"text-align:right;\"> 0.71 </td>\n   <td style=\"text-align:right;\"> 3.14 </td>\n   <td style=\"text-align:left;\"> ▃▇▇▃▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_modesty_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -4.67 </td>\n   <td style=\"text-align:right;\"> -0.66 </td>\n   <td style=\"text-align:right;\"> 0.19 </td>\n   <td style=\"text-align:right;\"> 0.83 </td>\n   <td style=\"text-align:right;\"> 1.26 </td>\n   <td style=\"text-align:left;\"> ▁▁▂▅▇ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_religion_identification_level_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.56 </td>\n   <td style=\"text-align:right;\"> -0.56 </td>\n   <td style=\"text-align:right;\"> -0.56 </td>\n   <td style=\"text-align:right;\"> -0.08 </td>\n   <td style=\"text-align:right;\"> 2.37 </td>\n   <td style=\"text-align:left;\"> ▇▁▁▁▂ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_kessler_latent_anxiety_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.72 </td>\n   <td style=\"text-align:right;\"> -0.69 </td>\n   <td style=\"text-align:right;\"> -0.19 </td>\n   <td style=\"text-align:right;\"> 0.70 </td>\n   <td style=\"text-align:right;\"> 4.01 </td>\n   <td style=\"text-align:left;\"> ▇▇▆▁▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t0_kessler_latent_depression_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.21 </td>\n   <td style=\"text-align:right;\"> -0.63 </td>\n   <td style=\"text-align:right;\"> -0.13 </td>\n   <td style=\"text-align:right;\"> 0.42 </td>\n   <td style=\"text-align:right;\"> 4.83 </td>\n   <td style=\"text-align:left;\"> ▇▂▂▁▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t2_kessler_latent_depression_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.23 </td>\n   <td style=\"text-align:right;\"> -0.65 </td>\n   <td style=\"text-align:right;\"> -0.16 </td>\n   <td style=\"text-align:right;\"> 0.39 </td>\n   <td style=\"text-align:right;\"> 4.75 </td>\n   <td style=\"text-align:left;\"> ▇▃▂▁▁ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> t2_kessler_latent_anxiety_z </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.74 </td>\n   <td style=\"text-align:right;\"> -0.70 </td>\n   <td style=\"text-align:right;\"> -0.20 </td>\n   <td style=\"text-align:right;\"> 0.68 </td>\n   <td style=\"text-align:right;\"> 3.97 </td>\n   <td style=\"text-align:left;\"> ▇▇▆▁▁ </td>\n  </tr>\n</tbody>\n</table>\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |>\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |>\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))\n```\n:::\n\n\n\n## Propensity scores\n\n\nNext we generate propensity scores.  Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\n\nThe first  step is to obtain the baseline variables. note that we must remove \"t0_eth_cat\" because we are performing separate weighting for each stratum within this variable. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read -- you may start here if you need to repeat the analysis\n\ndt <- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity <- dt|>\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |> colnames()\n\n# define our exposure\nX <- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS <- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt <- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop <-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n```\n\n\n:::\n:::\n\n\n\n\nFor propensity score analysis, we will try several different approaches.  We will want to select the method that produces the best balance. \n\nI typically use \"ps\" (classical propensity scores), `ebal` and `energy`.  The latter two in my experience yeild good balance. Also `energy` will work with *continuous* exposures. \n\nFor more information, see [https://ngreifer.github.io/WeightIt/](https://ngreifer.github.io/WeightIt/)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps <- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal <- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\n# dt_match_energy <- match_mi_general(\n#   data = dt,\n#   X = X,\n#   baseline_vars = baseline_vars_reflective_propensity,\n#   subgroup = \"t0_eth_cat\",\n#   estimand = \"ATE\",\n#   #focal = \"high\", # for use with ATT\n#   method = \"energy\"\n# )\n# saveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n```\n:::\n\n\n\n\nResults, first for Europeans \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndt_match_energy <- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal <- readRDS(here::here(\"data\", \"dt_match_ebal\"))\ndt_match_ps <- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\nbal.tab(dt_match_energy$euro)   #  good\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0008\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0007\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0006\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0012\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0001\nt0_urban_urban                                      Binary       0.0002\nt0_hours_exercise_coarsen_inactive                  Binary       0.0049\nt0_hours_exercise_coarsen_active                    Binary       0.0005\nt0_hours_exercise_coarsen_very_active               Binary       0.0045\nt0_edu_z                                           Contin.       0.0021\nt0_employed_z                                      Contin.       0.0016\nt0_nz_dep2018_z                                    Contin.       0.0039\nt0_nzsei13_z                                       Contin.       0.0024\nt0_partner_z                                       Contin.       0.0003\nt0_parent_z                                        Contin.       0.0010\nt0_pol_orient_z                                    Contin.       0.0033\nt0_agreeableness_z                                 Contin.       0.0019\nt0_conscientiousness_z                             Contin.       0.0015\nt0_extraversion_z                                  Contin.       0.0026\nt0_honesty_humility_z                              Contin.       0.0015\nt0_openness_z                                      Contin.       0.0003\nt0_neuroticism_z                                   Contin.       0.0031\nt0_modesty_z                                       Contin.       0.0011\nt0_religion_identification_level_z                 Contin.       0.0029\nt0_kessler_latent_anxiety_z                        Contin.       0.0009\nt0_kessler_latent_depression_z                     Contin.       0.0010\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1646.63 3164.13      946.78\n```\n\n\n:::\n\n```{.r .cell-code}\nbal.tab(dt_match_ebal$euro)   #  best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0001\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0001\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n```\n\n\n:::\n\n```{.r .cell-code}\nbal.tab(dt_match_ps$euro)   #  not as good\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0664\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0038\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0305\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0236\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0107\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0025\nt0_urban_urban                                      Binary       0.0447\nt0_hours_exercise_coarsen_inactive                  Binary       0.0832\nt0_hours_exercise_coarsen_active                    Binary       0.1000\nt0_hours_exercise_coarsen_very_active               Binary       0.0728\nt0_edu_z                                           Contin.       0.1782\nt0_employed_z                                      Contin.       0.0636\nt0_nz_dep2018_z                                    Contin.       0.1321\nt0_nzsei13_z                                       Contin.       0.1387\nt0_partner_z                                       Contin.       0.0698\nt0_parent_z                                        Contin.       0.0186\nt0_pol_orient_z                                    Contin.       0.0644\nt0_agreeableness_z                                 Contin.       0.1456\nt0_conscientiousness_z                             Contin.       0.0769\nt0_extraversion_z                                  Contin.       0.0607\nt0_honesty_humility_z                              Contin.       0.0181\nt0_openness_z                                      Contin.       0.0488\nt0_neuroticism_z                                   Contin.       0.0338\nt0_modesty_z                                       Contin.       0.0137\nt0_religion_identification_level_z                 Contin.       0.0757\nt0_kessler_latent_anxiety_z                        Contin.       0.0327\nt0_kessler_latent_depression_z                     Contin.       0.0768\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1585.48 3760.58      949.56\n```\n\n\n:::\n:::\n\n\n\nResults for Maori\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbal.tab(dt_match_energy$māori)   #  good\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0090\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0020\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0161\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0038\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0077\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0067\nt0_urban_urban                                      Binary       0.0058\nt0_hours_exercise_coarsen_inactive                  Binary       0.0408\nt0_hours_exercise_coarsen_active                    Binary       0.0152\nt0_hours_exercise_coarsen_very_active               Binary       0.0256\nt0_edu_z                                           Contin.       0.0205\nt0_employed_z                                      Contin.       0.0432\nt0_nz_dep2018_z                                    Contin.       0.0387\nt0_nzsei13_z                                       Contin.       0.0147\nt0_partner_z                                       Contin.       0.0179\nt0_parent_z                                        Contin.       0.0182\nt0_pol_orient_z                                    Contin.       0.0262\nt0_agreeableness_z                                 Contin.       0.0057\nt0_conscientiousness_z                             Contin.       0.0447\nt0_extraversion_z                                  Contin.       0.0117\nt0_honesty_humility_z                              Contin.       0.0073\nt0_openness_z                                      Contin.       0.0159\nt0_neuroticism_z                                   Contin.       0.0134\nt0_modesty_z                                       Contin.       0.0053\nt0_religion_identification_level_z                 Contin.       0.0167\nt0_kessler_latent_anxiety_z                        Contin.       0.0202\nt0_kessler_latent_depression_z                     Contin.       0.0125\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted    307.  354.        160.  \nAdjusted      212.2 289.68       97.34\n```\n\n\n:::\n\n```{.r .cell-code}\nbal.tab(dt_match_ebal$māori)   #  best\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0000\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0000\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n```\n\n\n:::\n\n```{.r .cell-code}\nbal.tab(dt_match_ps$māori)   #  not good\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0839\nt0_gen_cohort_Gen_Silent: born< 1946                Binary       0.0048\nt0_gen_cohort_Gen Boomers: born >= 1946 & b.< 1965  Binary       0.0692\nt0_gen_cohort_GenX: born >=1961 & b.< 1981          Binary       0.0203\nt0_gen_cohort_GenY: born >=1981 & b.< 1996          Binary       0.0548\nt0_gen_cohort_GenZ: born >= 1996                    Binary       0.0131\nt0_urban_urban                                      Binary       0.0790\nt0_hours_exercise_coarsen_inactive                  Binary       0.0810\nt0_hours_exercise_coarsen_active                    Binary       0.1137\nt0_hours_exercise_coarsen_very_active               Binary       0.1082\nt0_edu_z                                           Contin.       0.1130\nt0_employed_z                                      Contin.       0.0998\nt0_nz_dep2018_z                                    Contin.       0.1613\nt0_nzsei13_z                                       Contin.       0.1672\nt0_partner_z                                       Contin.       0.1720\nt0_parent_z                                        Contin.       0.1720\nt0_pol_orient_z                                    Contin.       0.1194\nt0_agreeableness_z                                 Contin.       0.1196\nt0_conscientiousness_z                             Contin.       0.0692\nt0_extraversion_z                                  Contin.       0.1158\nt0_honesty_humility_z                              Contin.       0.0994\nt0_openness_z                                      Contin.       0.2098\nt0_neuroticism_z                                   Contin.       0.0706\nt0_modesty_z                                       Contin.       0.1336\nt0_religion_identification_level_z                 Contin.       0.1557\nt0_kessler_latent_anxiety_z                        Contin.       0.1641\nt0_kessler_latent_depression_z                     Contin.       0.1164\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     181.71 334.58       79.73\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# code for summar\nsum_e <- summary(dt_match_ebal$euro)\nsum_m <- summary(dt_match_ebal$māori)\n\n\nlove_plot_e <- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro PS: ebalance\")\n\nlove_plot_m <- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori PS: ebalance\")\n\n\nlibrary(patchwork)\n\n\nlove_plot_e / love_plot_m\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/love_plots-1.png){width=672}\n:::\n:::\n\n\n\n More data wrangling\n \n \n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare data post-weighting ---------------------------------------------\n\n\n# prepare data\ndt_ref_e <- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\ndt_ref_e$weights <- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare data\ndt_ref_m <- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\ndt_ref_m$weights <- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all <- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n```\n:::\n\n\nLet's consider a pseudo experiment where someome moves from inactive to active. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf <-  dt_ref_all\nY <-  \"t2_kessler_latent_anxiety_z\"\nX <- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str <-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  <- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all <- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e <-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m <-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) <-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) <-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety <- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety <- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n```\n:::\n\n\nCalculate E-values\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read data\nest_all_anxiety <- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all <- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety <- df_anxiety_all |> \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |> \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |> \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |> \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n\n\n:::\n\n```{.r .cell-code}\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab <- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nConfidence interval crosses the true value, so its E-value is 1.\n```\n\n\n:::\n\n```{.r .cell-code}\ntest_tab |> kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|     | E[Y(1)]-E[Y(0)]| lower_ci| upper_ci| E_Value| E_Val_bound|\n|:----|---------------:|--------:|--------:|-------:|-----------:|\n|RD_m |           0.027|   -0.114|    0.188|   1.185|           1|\n|RD_e |          -0.077|   -0.131|   -0.022|   1.352|           1|\n\n\n:::\n:::\n\n\n##  Differences by subgroups\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_anxiety_all_plot <- df_anxiety_all |> \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |> \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |> \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |> \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.010    0.201\nE[Y(active)]_m         0.138    0.056    0.214\nE[Y(very_active)]_m    0.134    0.014    0.264\nRD_m                   0.027   -0.114    0.188\nE[Y(inactive)]_e       0.034    0.002    0.066\nE[Y(active)]_e        -0.021   -0.046    0.002\nE[Y(very_active)]_e   -0.043   -0.085    0.004\nRD_e                  -0.077   -0.131   -0.022\nRD_m - RD_e            0.104   -0.042    0.279\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_sub_forest(df_anxiety_all_plot)\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n\n```{.r .cell-code}\ndf_anxiety_all_plot|> \n  kbl(format = \"html\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> lower_ci </th>\n   <th style=\"text-align:right;\"> upper_ci </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(inactive)]_m </td>\n   <td style=\"text-align:right;\"> 0.108 </td>\n   <td style=\"text-align:right;\"> 0.010 </td>\n   <td style=\"text-align:right;\"> 0.201 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(active)]_m </td>\n   <td style=\"text-align:right;\"> 0.138 </td>\n   <td style=\"text-align:right;\"> 0.056 </td>\n   <td style=\"text-align:right;\"> 0.214 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(very_active)]_m </td>\n   <td style=\"text-align:right;\"> 0.134 </td>\n   <td style=\"text-align:right;\"> 0.014 </td>\n   <td style=\"text-align:right;\"> 0.264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m </td>\n   <td style=\"text-align:right;\"> 0.027 </td>\n   <td style=\"text-align:right;\"> -0.114 </td>\n   <td style=\"text-align:right;\"> 0.188 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(inactive)]_e </td>\n   <td style=\"text-align:right;\"> 0.034 </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n   <td style=\"text-align:right;\"> 0.066 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(active)]_e </td>\n   <td style=\"text-align:right;\"> -0.021 </td>\n   <td style=\"text-align:right;\"> -0.046 </td>\n   <td style=\"text-align:right;\"> 0.002 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(very_active)]_e </td>\n   <td style=\"text-align:right;\"> -0.043 </td>\n   <td style=\"text-align:right;\"> -0.085 </td>\n   <td style=\"text-align:right;\"> 0.004 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_e </td>\n   <td style=\"text-align:right;\"> -0.077 </td>\n   <td style=\"text-align:right;\"> -0.131 </td>\n   <td style=\"text-align:right;\"> -0.022 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m - RD_e </td>\n   <td style=\"text-align:right;\"> 0.104 </td>\n   <td style=\"text-align:right;\"> -0.042 </td>\n   <td style=\"text-align:right;\"> 0.279 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Depression \n\n\n::: {.cell}\n\n```{.r .cell-code}\n### SUBGROUP analysis\ndf <-  dt_ref_all\nY <-  \"t2_kessler_latent_depression_z\"\nX <- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str <-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  <- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs <- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all <- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n\n# note contrast of interest\nsim_estimand_all_e_d <-\n  transform(sim_estimand_all_e_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d <- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\nsim_estimand_all_m_d <-\n  transform(sim_estimand_all_m_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) <-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) <-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d <- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d <- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(est_all_d, here::here(\"data\", \"est_all_d\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nest_all_d <- readRDS( here::here(\"data\", \"est_all_d\"))\n\n        \n# make dataframe\ndf_dep <- data.frame( summary(est_all_d) )\n\ntable_estimates_depression <- df_dep |> \n    filter(row.names(df_dep) %in% c(\"RD_m\", \"RD_e\")) |> \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |> \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |> \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntable_depression <- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nConfidence interval crosses the true value, so its E-value is 1.\nConfidence interval crosses the true value, so its E-value is 1.\n```\n\n\n:::\n\n```{.r .cell-code}\ntable_depression |> kbl(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n\n\n|     | E[Y(1)]-E[Y(0)]| lower_ci| upper_ci| E_Value| E_Val_bound|\n|:----|---------------:|--------:|--------:|-------:|-----------:|\n|RD_m |           0.028|   -0.120|    0.185|   1.189|           1|\n|RD_e |          -0.039|   -0.103|    0.021|   1.230|           1|\n\n\n:::\n:::\n\n\n\nSummary\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# view summary\ndf_dep |> \n  kbl(format = \"html\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Estimate </th>\n   <th style=\"text-align:right;\"> X2.5.. </th>\n   <th style=\"text-align:right;\"> X97.5.. </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(inactive)]_m </td>\n   <td style=\"text-align:right;\"> 0.1510358 </td>\n   <td style=\"text-align:right;\"> 0.0537454 </td>\n   <td style=\"text-align:right;\"> 0.2442848 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(active)]_m </td>\n   <td style=\"text-align:right;\"> 0.1877594 </td>\n   <td style=\"text-align:right;\"> 0.1031401 </td>\n   <td style=\"text-align:right;\"> 0.2866124 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(very_active)]_m </td>\n   <td style=\"text-align:right;\"> 0.1793980 </td>\n   <td style=\"text-align:right;\"> 0.0718405 </td>\n   <td style=\"text-align:right;\"> 0.2990333 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m </td>\n   <td style=\"text-align:right;\"> 0.0283622 </td>\n   <td style=\"text-align:right;\"> -0.1197312 </td>\n   <td style=\"text-align:right;\"> 0.1854479 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(inactive)]_e </td>\n   <td style=\"text-align:right;\"> 0.0035095 </td>\n   <td style=\"text-align:right;\"> -0.0324475 </td>\n   <td style=\"text-align:right;\"> 0.0411906 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(active)]_e </td>\n   <td style=\"text-align:right;\"> -0.0168208 </td>\n   <td style=\"text-align:right;\"> -0.0404293 </td>\n   <td style=\"text-align:right;\"> 0.0071173 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> E[Y(very_active)]_e </td>\n   <td style=\"text-align:right;\"> -0.0358775 </td>\n   <td style=\"text-align:right;\"> -0.0825212 </td>\n   <td style=\"text-align:right;\"> 0.0120629 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_e </td>\n   <td style=\"text-align:right;\"> -0.0393869 </td>\n   <td style=\"text-align:right;\"> -0.1027416 </td>\n   <td style=\"text-align:right;\"> 0.0212851 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RD_m - RD_e </td>\n   <td style=\"text-align:right;\"> 0.0677491 </td>\n   <td style=\"text-align:right;\"> -0.0902557 </td>\n   <td style=\"text-align:right;\"> 0.2292381 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# This table provides estimated levels of depression, in standard deviation units, for different levels of activity for two groups: Māori (indicated by \"_m\") and NZ Europeans (indicated by \"_e\").\n#\n# The expectations are named as `E[Y(<level of activity>)]_group`, where the level of activity can be `inactive`, `active`, or `very_active`.\n#\n# Here is a breakdown of the results.\n#\n#   1. For the Māori group (`_m`):\n#\n#   - `E[Y(inactive)]_m`: When inactive, the expected level of depression is 0.23 standard deviations, with a 95% confidence interval from 0.116 to 0.356.\n# - `E[Y(active)]_m`: When active, the expected level of depression decreases to 0.193 standard deviations, with a 95% confidence interval from 0.108 to 0.282.\n# - `E[Y(very_active)]_m`: When very active, the expected level of depression further decreases to 0.133 standard deviations, with a 95% confidence interval from 0.009 to 0.262.\n# - `RD_m`: The risk difference (RD) between inactive and very active Māori individuals is 0.097 standard deviations, with a 95% confidence interval from -0.068 to 0.274. This indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# 2. For the NZ European group (`_e`):\n#\n#   - `E[Y(inactive)]_e`: When inactive, the expected level of depression is 0.034 standard deviations, with a 95% confidence interval from -0.012 to 0.078.\n# - `E[Y(active)]_e`: When active, the expected level of depression slightly decreases to -0.006 standard deviations, with a 95% confidence interval from -0.03 to 0.016.\n# - `E[Y(very_active)]_e`: When very active, the expected level of depression further decreases to -0.046 standard deviations, with a 95% confidence interval from -0.086 to -0.007.\n# - `RD_e`: The risk difference (RD) between inactive and very active NZ European individuals is 0.081 standard deviations, with a 95% confidence interval from 0.02 to 0.138. Similar to the Māori group, this indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# The last row, `RD_m - RD_e`, represents the difference in risk differences between Māori and NZ Europeans. It's 0.017 standard deviations with a 95% confidence interval from -0.152 to 0.204. This is not statistically significant (the confidence interval contains 0), suggesting that the difference in depression reduction from being inactive to very active is not significantly different between the two groups.\n#\n# These are estimates and subject to statistical uncertainty. While they suggest a trend, the wide confidence intervals indicate that these estimates come with a degree of uncertainty.\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_dep_plot_data <- df_dep |> \n rename( lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) \n\n\ndf_dep\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        Estimate      X2.5..     X97.5..\nE[Y(inactive)]_m     0.151035788  0.05374538 0.244284755\nE[Y(active)]_m       0.187759424  0.10314013 0.286612442\nE[Y(very_active)]_m  0.179397991  0.07184045 0.299033259\nRD_m                 0.028362203 -0.11973124 0.185447917\nE[Y(inactive)]_e     0.003509459 -0.03244753 0.041190620\nE[Y(active)]_e      -0.016820762 -0.04042933 0.007117345\nE[Y(very_active)]_e -0.035877479 -0.08252119 0.012062852\nRD_e                -0.039386939 -0.10274163 0.021285115\nRD_m - RD_e          0.067749141 -0.09025568 0.229238107\n```\n\n\n:::\n\n```{.r .cell-code}\nplot_sub_forest <- function(df) {\n  require(ggplot2)\n\n  # Check if required packages are installed\n  required_packages <- c(\"ggplot2\")\n  new_packages <- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  # Check if required columns are in the dataframe\n  required_cols <- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols <- required_cols[!(required_cols %in% colnames(df))]\n  if (length(missing_cols) > 0)\n    stop(\"Missing columns in dataframe: \", paste(missing_cols, collapse = \", \"))\n  \n  # Order the factor levels by the estimate column in decreasing order\n  \n  ggplot(df, aes(x=estimate, y=factor(row.names(df)))) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height=0.3) +\n    geom_vline(xintercept = 0, linetype=\"dashed\", color = \"red\") +\n    theme_bw() +\n    xlab(\"Estimate\") +\n    ylab(\"\")\n}\nplot_sub_forest(df_dep_plot_data)\n```\n\n::: {.cell-output-display}\n![](experiment_template_files/figure-html/my_graph-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "experiment_template_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/table1-1.0/table1_defaults.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}