[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Accessing Lectures and Readings\n\n\n\n\nSeminar Time/Location: Mondays, 11:00-12:50pm Easterfield Building Room 406\nCourse Outline: find a detailed schedule of topics, readings, and assignments in the Course Outline tab.\nReadings: links to readings are directly within the Course Outline tab, essential for lecture preparation.\nLecture Materials: access slides, video recordings, and more under the Content tab, organised by week for ease of use.\nTests: in the same room as the seminar\n\n\n\n\n\nSeminar Day/Time: Mondays, 11:00-12:50pm\nSeminar Location Easterfield Building EA406\nTest/Quiz Location Easterfield Building EA406\n\n\n\nThe Contents tab offers direct access to weekly seminar and lab materials, including lecture outlines and lab resources.\n\nAccess it from the top right of the course platform by selecting the appropriate week.\nLab materials are available one week before the lecture; seminar review materials post-seminar.\n\n\n\n\n\n\n\n\n\n\nCourse Coordinator Prof Joseph Bulbulia joseph.bulbulia@vuw.ac.nz\nCourse Coordinator’s Office EA324\nR Help from  Dr.Inkuk Kim inkuk.kim@vuw.ac.nz\n\n\n\n\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nTest 1\n2\n25\n25 March (w5)\n\n\nTest 2\n2\n25\n6 May (w9)\n\n\nResearch Report\n1,2,3\n40\n3 June (w12)\n\n\n\n\n\n\n\n\n\n\n\nThe official description:\nThis course will focus on theoretical and practical challenges for conducting research involving individuals from more than one cultural background or ethnicity. Topics are likely to include defining and measuring culture; developing culture-sensitive studies, choice of language and translation; communication styles and bias; questionnaire and interview design; qualitative and quantitative data analysis for cultural and cross-cultural research; minorities, power and ethics in cross-cultural research; and ethno-methodologies and indigenous research methodologies. Appropriate background for this course: PSYC 338.\n\n\n\nPreamble: in this advanced course, students will develop foundational skills in cross-cultural psychological research with a strong emphasis on causal inference, a new and critical methodological approach.\n\nProgramming in R students will learn the basics of programming in the statistical language R, gaining essential computational tools for psychological research. The skills you acquire will lay the foundation for applying data analysis techniques in a causal inference framework and beyond.\nUnderstanding Causal Inference. students will develop a robust understanding of causal inference concepts and approaches, with particular emphasis on how they mitigate common pitfalls in cross-cultural research. We will focus on designing studies, analysing data, and drawing strong conclusions about cause-and-effect relationships across cultures.\nUnderstanding Measurement in Comparative Settings. students will learn techniques for constructing and validating psychometrically sound measures across diverse cultures. We will examine how to ensure measurements are reliable, cross-culturally valid, and aligned with theoretical constructs while focusing strongly on causal reasoning.\n\n\n\n\n\n\n\n\n\n\nAssessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nTest 1\n2\n25\n25 March (w5)\n\n\nTest 2\n2\n25\n6 May (w9)\n\n\nResearch Report\n1,2,3\n40\n3 June (w12)\n\n\n\n\n\n\n\n\n\n\n\nAttendance and Participation:\n\n\nClass attendance and active participation.\n\n\nLab attendance and active participation.\n\n\n\n\n\n\n\n\n\n\n\n\nTest Guidelines\n\n\n\n\nTest duration is under an hour. The allocated time is nearly two hours.\nEach lecture starts and ends with key concept definitions and reviews for the test.\nR or RStudio knowledge isn’t part of the test. R support aims to enhance research report skills.\nTests are conducted in the lecture room.\nRequired: pen/pencil. Computers are not allowed.\n\n\n\n\nFirst Test (50 minutes, total time allowed: 1 hour 50 minutes):\n\nFocuses on revising core statistical and methodological concepts.\nAims to refresh basic statistical knowledge foundational for later course material.\n\nSecond Test (50 minutes, total time allowed: 1 hour 50 minutes):\n\nBuilds upon the first test’s concepts, emphasising the application of basic conceptual, statistical, and theoretical knowledge.\n\n\n\n\n\n\n\n\n\n\n\nResearch Report Instructions\n\n\n\n\nWe will supply the data.\nLab sessions are designed to support you in this assignment.\nWe assume no statistical background.\n\n\n\n\nTitle: “Causal Inference in Cultural Psychology: Examining Exposure Effects on Dimensions of Well-being Modified by Cultural or Sociodemographic Categories”.\nObjective:\n\nTo quantify the causal effect of a specific exposure on well-being dimensions, modified by sociodemographic categories (born_nz, eth_cat, big_doms, gen_cohort) using the NZAVS longitudinal synthetic dataset.\n\nInstructions:\n\nTheoretical Interest and Research Question:\n\nDescribe the significance of your chosen exposure and its potential impact on the selected outcomes, modified by the cultural or sociodemographic category.\nState the research question clearly.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG illustrating the relationships between exposure, outcomes, sociodemographic category, and potential bias sources. Ensure clarity in labelling.\n\nConfounding Control Strategy:\n\nOutline your strategy for confounding control, justifying the chosen confounders.\n\nMeasurement Biases:\n\nAddress and analyse measurement biases as relevant.\n\nAssumptions and Statistical Models:\n\nDiscuss the assumptions of your causal inference approach and your statistical model, including their limitations.\n\n\nRequirements:\n\nIntroduction: 1,500 words limit.\nConclusion: 1,500 words limit.\nMethod and Results sections should be concise; no specific word limit.\nUse any useful sources, citing appropriately to avoid academic misconduct.\nFollow APA style for citations and references.\nInclude tables/figures as needed.\nSubmit as a single PDF, including R code in an appendix.\n\nEvaluation Criteria:\n\nClarity of theoretical framework, research question, and design.\nValidity of confounding control strategy.\nDiscussion on assumptions and statistical models.\nOrganisation and presentation quality.\n\n\n\n\n\n\nExtensions:\n\nNegotiate a new due date by writing (email) before March 10th, 2024, if necessary.\n\nPenalties:\n\nLate submissions incur a one-grade-per-day penalty.\nOver-length assignments will be penalised.\n\nUnforeseeable Events:\n\nExtensions post-March 10 require evidence (e.g., medical certificate).\n\n\n\n\n\n\nBring a laptop with R and RStudio installed for data analysis sessions. Contact the instructor if you lack computer access.\nFor in-class tests, bring a writing utensil. Electronic devices are not permitted."
  },
  {
    "objectID": "index.html#class-times-and-locations",
    "href": "index.html#class-times-and-locations",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Seminar Day/Time: Mondays, 11:00-12:50pm\nSeminar Location Easterfield Building EA406\nTest/Quiz Location Easterfield Building EA406"
  },
  {
    "objectID": "index.html#names-and-contact-details",
    "href": "index.html#names-and-contact-details",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Course Coordinator Prof Joseph Bulbulia joseph.bulbulia@vuw.ac.nz\nCourse Coordinator’s Office EA324\nR Help from  Dr.Inkuk Kim inkuk.kim@vuw.ac.nz"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The official description:\nThis course will focus on theoretical and practical challenges for conducting research involving individuals from more than one cultural background or ethnicity. Topics are likely to include defining and measuring culture; developing culture-sensitive studies, choice of language and translation; communication styles and bias; questionnaire and interview design; qualitative and quantitative data analysis for cultural and cross-cultural research; minorities, power and ethics in cross-cultural research; and ethno-methodologies and indigenous research methodologies. Appropriate background for this course: PSYC 338."
  },
  {
    "objectID": "index.html#course-learning-objectives",
    "href": "index.html#course-learning-objectives",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Preamble: in this advanced course, students will develop foundational skills in cross-cultural psychological research with a strong emphasis on causal inference, a new and critical methodological approach.\n\nProgramming in R students will learn the basics of programming in the statistical language R, gaining essential computational tools for psychological research. The skills you acquire will lay the foundation for applying data analysis techniques in a causal inference framework and beyond.\nUnderstanding Causal Inference. students will develop a robust understanding of causal inference concepts and approaches, with particular emphasis on how they mitigate common pitfalls in cross-cultural research. We will focus on designing studies, analysing data, and drawing strong conclusions about cause-and-effect relationships across cultures.\nUnderstanding Measurement in Comparative Settings. students will learn techniques for constructing and validating psychometrically sound measures across diverse cultures. We will examine how to ensure measurements are reliable, cross-culturally valid, and aligned with theoretical constructs while focusing strongly on causal reasoning."
  },
  {
    "objectID": "index.html#assignments-and-due-dates",
    "href": "index.html#assignments-and-due-dates",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nTest 1\n2\n25\n25 March (w5)\n\n\nTest 2\n2\n25\n6 May (w9)\n\n\nResearch Report\n1,2,3\n40\n3 June (w12)"
  },
  {
    "objectID": "content/common_graphs.html",
    "href": "content/common_graphs.html",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "",
    "text": "A, ~ Y\n\n\n\n\n\n   A \\to Y\n\n\n\n\nOutcome: typically denoted by Y. The effect or outcome of interest. Do not attempt to draw a causal DAG unless this outcome is clearly defined. Exposure or Treatment: typically denoted by A or X. The intervention. Do not attempt to draw a causal DAG unless the exposure is a clearly defined and does not violate deterministic non-positivity. Confounders: typically denoted by C or L. Informally the variables influencing both the exposure/treatment and the outcome. Or more formally: Unmeasured Confounders: typically denoted by U: Selection Variables: typically denoted by U: Variables affecting a unit’s inclusion in the study (including retention in the study). Box: denotes conditioning on a variable. For example, to denote selection into the study we write\n\\framebox{S}\nTo denote conditioning on a confounder set L we write\n\\framebox{L}\n\n\n\n\nMarkov Factorisation: Pertains to a causal DAG in which the joint distribution of all nodes can be expressed as a product of conditional distributions. Each variable is conditionally independent of its non-descendants, given its parents. This is crucial for identifying conditional independencies within the graph.\nD-separation (direction separation): Pertains to a condition in which there is no path between some sets of variables in the graph, given the conditioned variables. Establishing d-separation allows us to infer conditional independencies, which in turn help identify the set of measured variables we need to adjust for to obtain an unbiased estimate of the causal effect, or in the presence of unmeasured or partially measured confounders, to reduce bias.\n\n\n\n\n\n\n\nThe Causal Markov Condition is an assumption that each variable is independent of its non-descendants, given its parents in the graph. In other words, it assumes that all dependencies between variables are mediated by direct causal relationships. If two variables are correlated, it must be because one causes the other, or they have a shared cause, not because of any unmeasured confounding variables.\nFormally, for each variable X in the graph, X is independent of its non-descendants NonDesc(X), given its parents Pa(X).\nThis is strong assumption. Typically we must assume that there are hidden, unmeasured confounders that introduce dependencies between variables, which are not depicted in the graph. **It is important to (1) identify known unmeasured confounders and (2) label them on the the causal diagramme.\n\n\n\nThe Faithfulness assumption is the inverse to the Causal Markov Condition. It states that if two variables are uncorrelated, it is because there is no direct or indirect causal path between them, not because of any cancelling out of effects. Essentially, it assumes that the relationships in your data are stable and consistent, and will not change if you intervene to change some of the variables.\nFormally, if A and Y are independent given a set of variables L, then there does not exist a set of edges between A and Y that remains after conditioning on L.\nAs with the Causal Markov Condition, Faithfulness is a strong assumption, and it might not typically hold in the real world. There could be complex causal structures or interactions that lead to apparent independence between variables, even though they are causally related.\n\n\n\n\nDefine all variables clearly.\nDefine any novel conventions you employ. This could include dotted or coloured arrows to indicate confounding that is induced, or unaddressed (as below)\nAdopt minimalism. Include only those nodes and edges that are needed to clarify the problem. Use diagrams only when they bring more clarity than textual descriptions alone.\nChronological order. Where possible maintain temporal order of the nodes in the spatial order of the graph. Typically from left to right or top to bottom. When depicting repeated measures, index them using time subscripts:\nAdd time-stamps to your nodes. To bring additoinal clarity, it is almost always useful to time-stamp the nodes of your graph, for example, in schematic form:\n\n\nL_{t0} \\rightarrow A_{t1} \\rightarrow Y_{t2}\n\n\nWhere exposures are not assigned randomly, we should nearly always assume unmeasured confounding. For this reason, your causal DAG should include a description of the sensitivity analyses you will perform to clarify the sensitivity of your findings to unmeasured confounding. Where there are known unmeasured confounders these should be described.\n\nRecall that DAGs are qualitative representations. The stamps need not defined clearly defined units of time. Rather time stamps should preserve chronological order."
  },
  {
    "objectID": "content/common_graphs.html#common-causal-graphs",
    "href": "content/common_graphs.html#common-causal-graphs",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "",
    "text": "A, ~ Y\n\n\n\n\n\n   A \\to Y\n\n\n\n\nOutcome: typically denoted by Y. The effect or outcome of interest. Do not attempt to draw a causal DAG unless this outcome is clearly defined. Exposure or Treatment: typically denoted by A or X. The intervention. Do not attempt to draw a causal DAG unless the exposure is a clearly defined and does not violate deterministic non-positivity. Confounders: typically denoted by C or L. Informally the variables influencing both the exposure/treatment and the outcome. Or more formally: Unmeasured Confounders: typically denoted by U: Selection Variables: typically denoted by U: Variables affecting a unit’s inclusion in the study (including retention in the study). Box: denotes conditioning on a variable. For example, to denote selection into the study we write\n\\framebox{S}\nTo denote conditioning on a confounder set L we write\n\\framebox{L}\n\n\n\n\nMarkov Factorisation: Pertains to a causal DAG in which the joint distribution of all nodes can be expressed as a product of conditional distributions. Each variable is conditionally independent of its non-descendants, given its parents. This is crucial for identifying conditional independencies within the graph.\nD-separation (direction separation): Pertains to a condition in which there is no path between some sets of variables in the graph, given the conditioned variables. Establishing d-separation allows us to infer conditional independencies, which in turn help identify the set of measured variables we need to adjust for to obtain an unbiased estimate of the causal effect, or in the presence of unmeasured or partially measured confounders, to reduce bias.\n\n\n\n\n\n\n\nThe Causal Markov Condition is an assumption that each variable is independent of its non-descendants, given its parents in the graph. In other words, it assumes that all dependencies between variables are mediated by direct causal relationships. If two variables are correlated, it must be because one causes the other, or they have a shared cause, not because of any unmeasured confounding variables.\nFormally, for each variable X in the graph, X is independent of its non-descendants NonDesc(X), given its parents Pa(X).\nThis is strong assumption. Typically we must assume that there are hidden, unmeasured confounders that introduce dependencies between variables, which are not depicted in the graph. **It is important to (1) identify known unmeasured confounders and (2) label them on the the causal diagramme.\n\n\n\nThe Faithfulness assumption is the inverse to the Causal Markov Condition. It states that if two variables are uncorrelated, it is because there is no direct or indirect causal path between them, not because of any cancelling out of effects. Essentially, it assumes that the relationships in your data are stable and consistent, and will not change if you intervene to change some of the variables.\nFormally, if A and Y are independent given a set of variables L, then there does not exist a set of edges between A and Y that remains after conditioning on L.\nAs with the Causal Markov Condition, Faithfulness is a strong assumption, and it might not typically hold in the real world. There could be complex causal structures or interactions that lead to apparent independence between variables, even though they are causally related.\n\n\n\n\nDefine all variables clearly.\nDefine any novel conventions you employ. This could include dotted or coloured arrows to indicate confounding that is induced, or unaddressed (as below)\nAdopt minimalism. Include only those nodes and edges that are needed to clarify the problem. Use diagrams only when they bring more clarity than textual descriptions alone.\nChronological order. Where possible maintain temporal order of the nodes in the spatial order of the graph. Typically from left to right or top to bottom. When depicting repeated measures, index them using time subscripts:\nAdd time-stamps to your nodes. To bring additoinal clarity, it is almost always useful to time-stamp the nodes of your graph, for example, in schematic form:\n\n\nL_{t0} \\rightarrow A_{t1} \\rightarrow Y_{t2}\n\n\nWhere exposures are not assigned randomly, we should nearly always assume unmeasured confounding. For this reason, your causal DAG should include a description of the sensitivity analyses you will perform to clarify the sensitivity of your findings to unmeasured confounding. Where there are known unmeasured confounders these should be described.\n\nRecall that DAGs are qualitative representations. The stamps need not defined clearly defined units of time. Rather time stamps should preserve chronological order."
  },
  {
    "objectID": "content/common_graphs.html#elemental-counfounds",
    "href": "content/common_graphs.html#elemental-counfounds",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Elemental counfounds",
    "text": "Elemental counfounds\nThere are four elemental confounds [@mcelreath2020 p.185]. Consider how chronological consciensciousness assists with understanding both constraints on data.\n\n1. The problem of confounding by common cause\nThe problem of confounding by common cause arises when there is a variable denoted by L that influences both the exposure, denoted by A and the outcome variable, denoted by Y. Because L is a common cause of A and L is may create a statistical association between A and Y that does not reflect a causal association between A and Y. Put differently, although intervening on A might not affect Y, A and Y may be associated. For example, people who smoke may have yellow fingers. Smoking causes cancer. Because smoking (L) is a common cause of yellow fingers (A) and cancer (Y), A and Y will be associated. However, intervening to change the colour of people’s fingers would not affect cancer. The dashed red arrow in the graph indicate bias arising from the open backdoor path from A to Y that results from the common cause L.”\n\n\n\n\n\n\n\n\nFigure 1: Counfounding by common cause. The dashed red arrow indicates bias arising from the open backdoor path from A to Y.\n\n\n\n\n\n\n\nAdvice: attend to the temporal order of cauasality\nConfounding by a common cause can be addressed by adjusting for it. Typically we adjust through through statistical models such as regression, matching, or inverse probability of treatment weighting. Again, it is beyond the scope of this tutorial to describe causal estimation techniques. Figure Figure 2 clarifies that any confounding that is a cause of A and Y will precede A (and so Y), because causes precede effects. By indexing the the nodes on the graph, we can see that confounding control typically requires time-series data.\n\n\n\n\n\n\n\n\nFigure 2: Solution: adjust for pre-exposure confounder.\n\n\n\n\n\n\n\n2. Confounding by collider stratification (conditioning on a common effect)\nConditioning on a common effect occurs when a variable L is affected by both the treatment A and an outcome Y.\nSuppose A and Y are initially independent, such that A \\coprod Y(a). Conditioning on the common effect L opens a backdoor path between A and Y, possibly inducing an association. This occurs because L gives information about the relationship of A and Y. Here’s an example:\nLet A denote “exercise”. Let Y denote “heart disease”. Let L denote “weight”. Suppose, “exercise” and “heart disease” are not causally linked. However, they both affect “weight”, and if we condition on “weight” in a cross-sectional study, we might find a statistical association between “exercise” and “heart disease” even in the absence of causation.\nWe denote the observed associations as follows:\n\nP(A = 1): Probability of exercising\nP(Y = 1): Probability of having heart disease\nP(L = 1): Probability of being overweight\n\nWithout conditioning on L, we have:\nP(A = 1, Y = 1) = P(A = 1)P(Y = 1)\nHowever, if we condition on L (thecommon effect of both A and Y), we find:\nP(A = 1, Y = 1 | L = 1) \\neq P(A = 1 | L = 1)P(Y = 1 | L = 1)\nThe common effect L, once conditioned on, creates a non-causal association between A and Y. This can mislead us into believing there’s a direct link between exercise and heart disease, which is not the case. In the cross-sectional data, if we only observe A, Y, and L without understanding their causal relationship, we might erroneously conclude that there is a causal relationship between A and Y. This is the collider stratification bias.\n\n\n\n\n\n\n\n\nFigure 3: Confounding by conditioning on a collider.\n\n\n\n\n\n\n\nAdvice: attend to the temporal order of cauasality\nTo address the problem of conditioning on a common effect, we should generally ensure that all confounders L that are common causes of the exposure A and the outcome Y are measured before the occurance of the exposure A, and furthermore that the exposure A is measured before the occurance of the outcome Y. If such temporal order is preserved, L cannot be an effect of A, and thus neither of Y. By measuring all relevant confounders before the exposure, researchers can minimise the scope for collider confounding by conditioning on a common effect. This rule is not absolute. As indicated in Figure 9, it may be useful in certain circumstances to condition on a confounder that occurs after the outcome has occurred.\n\n\n\n\n\n\n\n\nFigure 4: Solution: avoid colliders\n\n\n\n\n\n\n\nM-bias: conditioning on a collider that occurs before the exposure may introduce bias\nTypically, confounders should be measured before their exposures. However, researchers should be cautious about conditioning on pre-exposure variable, as doing so can induce confounding. As shown in Figure 5, collider stratification may arise even if L occurs before A. This happens when L does not affect A or Y, but may be the descendent of a unmeasured variable that affects A and another unmeasured variable that also affects Y. Conditioning on L in this scenario elicits what is called “M-bias.” Note, however, that if L is not a common cause of A and Y, L should not be included in our model because it is not a source of confounding. Here, A \\coprod Y(a) and A \\cancel{\\coprod} Y(a)| L. The solution: do not condition on the pre-exposure variable L.\n\n\n\n\n\n\n\n\nFigure 5: M-bias: confounding control by including previous measures of the outcome\n\n\n\n\n\n\n\n3 The problem of conditioning on a mediator\nConditioning on a mediator occurs when L lies on the causal pathway between the treatment A and the outcome Y. Conditioning on L can lead to biased estimates by blocking or distorting the total effect of A and Y. Where L is a mediator, including L will typically attenuate the effect of A on Y. This scenario is presented in Figure 6. Where L is a collider between A and an unmeasured confouder U, then including L may increase the strength of association between A and Y. This scenario is presented in Figure 8.\nIn either case, unless one is interested in mediation analysis, conditioning on a post-treatment variable is nearly always a bad idea.\n\n\n\n\n\n\n\n\nFigure 6: Confounding by a mediator.\n\n\n\n\n\n\n\nAdvice: attend to the temporal order of cauasality\nTo address the problem of mediator bias, when interested in total effects do not condition on a mediator. This can be done by ensuring that L occurs before A (and Y). Again we discover the importance of an explicit temporal ordering for our variables. Although note, if L is associated with Y but is not associated with A conditioning on L will improve the efficiency of the causal effect estimate of A on Y. However, if A might affect L, then L might be a mediator, and including L risks bias. As with some much in causal estimation, we must understand the context.\n\n\n\n\n\n\n\n\nFigure 7: Ensure confounders occur before exposures.\n\n\n\n\n\n\n\n4. Conditioning on a descendant\nSay X is a cause of X\\prime. If we condition on X we partially condition on X\\prime.\nThere are both negative and positive implications for causal estimation in real-world scenarios.\nFirst the negative. Suppose there is a confounder L that is caused by an unobserved variable U, and is affected by the treatment A. Suppose further that U causes the outcome Y. In this scenario, as described in Figure 8, conditioning on L, which is a descendant of A and U, can lead to a spurious association between A and Y through the path A \\to L \\to U \\to Y.\n\n\n\n\n\n\n\n\nFigure 8: Confounding by descent\n\n\n\n\n\n\n\nAdvice: attend to the temporal order of causality, and use expert knowledge of all relevant nodes.\nEnsuring the confounder (L) is measured before the exposure (A) has two benefits.\nFirst, if L is a confounder, that is, if L is a variable which if we fail to condition on it will bias the association between treatment and outcome, the strategy of including only pre-treatment indicators of L will reduce bias. Figure 9 presents this strategy\n\n\n\n\n\n\n\n\nFigure 9: Solution: again, ensure temporal ordering in all measured variables.\n\n\n\n\n\nSecondly, note that we may use descendent to reduce bias. For example, if an unmeasured confounder U affects A, Y, and L\\prime, then adjusting for L\\prime may help to reduce confounding caused by U. This scenario is presented in Figure 10. Note that in this graph, L\\prime may occur after the exposure, and indeed after the outcome. This shows that it would be wrong to infer that merely because causes preceed effects, we should only condition on confounders that preceed the exposure.\n\n\n\n\n\n\n\n\nFigure 10: Solution: note that conditioning on a confounder that occurs after the exposure and outcome addresses the problem of unmeasured confounding. The dotted paths denote that the effect of U on A and Y is partially adjusted by conditioning on L, even though L occurs after the outcome. The dotted blue path suggest suppressing of the biased relationship between A and Y under the null. A genetic factor that affects the exposure and the outcome early in life, and that also expresses a measured indicator late in life, might constitute an example for which post-outcome confounding control might be possible.\n\n\n\n\n\n\n\nCausal Interaction?\nApplied researchers will often be interested in testing interactions. What is causal interaction and how may we represent it on a causal diagramme?\nWe must distinguish the concept of causal interaction from the concept of effect modification.\n\nCausal interaction as two independent exposures\nCausal interaction is the effect of two exosures that may occur jointly or separately (or not occur). We say there is interaction on the scale of interest when the effect of one exposure on an outcome depends on the level of another exposure. For example, the effect of a drug (exposure A) on recovery time from a disease (outcome Y) might depend on whether or not the patient is also receiving physical therapy (exposure B). In terms of causal quantities, if we denote the potential outcomes under different exposure combinations as Y(a,b), a causal interaction on the difference scale would be present if Y(1,1) - Y(1,0) \\neq Y(0,1) - Y(0,0).\nWhen drawing a causal diagram, we represent the two exposures as separate nodes and draw edges from them to the outcome, as showin in Figure 11. This is because causal diagrams are non-parametric; they represent the qualitative aspects of causal relationships without making specific assumptions about the functional form of these relationships.\n\n\n\n\n\n\n\n\nFigure 11: Causal interaction: the are two exposures are causally independent of each other\n\n\n\n\n\n\n\nEffect measures for causal interaction\nOn the difference scale, the total causal effect of an exposure A on an outcome Y is typically quantified as Y(1) - Y(0), where Y(a) represents the potential outcome under exposure level a. If there is another exposure B, the causal interaction effect on the difference scale would be quantified as [Y(1,1) - Y(1,0)] - [Y(0,1) - Y(0,0)].\nNote that causal effect of interactions might differ on the ratio scale. For instance, the total causal effect on the ratio scale would be Y(1) / Y(0), and the interaction effect would be [Y(1,1) / Y(1,0)] / [Y(0,1) / Y(0,0)].\n\n\nCausal interaction as effect modification\nEffect modification models the effect the magnitude of of a single exposure on an outcome across different levels of another variable.\nHere we assume independence of the counterfactual outcome conditional on measured confounders, within strata of co-variate G:\nY(a) \\coprod A | L, G\nNote that there here, there is only one counterfactual outcome.\n\n\n\n\n\n\n\n\nFigure 12: A simple graph for effect-modification.\n\n\n\n\n\n\n\n\nAdvice for causal mediation\n\nNo unmeasured exposure-outcome confounders given L\nThis assumption is denoted by Y(a,m) \\coprod A | L. It implies that when we control for the covariates L, there are no unmeasured confounders that influence both the exposure A and the outcome Y. For example, if we are studying the effect of a drug (exposure) on recovery time from a disease (outcome), and age and gender are our covariates L, this assumption would mean that there are no other factors, not accounted for in L, that influence both the decision to take the drug and the recovery time.\nNo unmeasured mediator-outcome confounders given L\nThis assumption is denoted by Y(a,m) \\coprod M | L. It implies that when we control for the covariates L, there are no unmeasured confounders that influence both the mediator M and the outcome Y. For instance, if we are studying the effect of exercise (exposure) on weight loss (outcome) mediated by calorie intake (mediator), and age and gender are our covariates L, this assumption would mean that there are no other factors, not accounted for in L, that influence both the calorie intake and the weight loss.\nNo unmeasured exposure-mediator confounders given L\nThis assumption is denoted by M(a) \\coprod A | L. It implies that when we control for the covariates L, there are no unmeasured confounders that influence both the exposure A and the mediator M. Using the previous example, this assumption would mean that there are no other factors, not accounted for in L, that influence both the decision to exercise and the calorie intake.\nNo mediator-outcome confounder affected by the exposure (no red arrow)\n\nThis assumption is denoted by Y(a,m) \\coprod M^{a*} | L. It implies that there are no variables that confound the relationship between the mediator and the outcome that are affected by the exposure. For example, if we are studying the effect of education (exposure) on income (outcome) mediated by job type (mediator), this assumption would mean that there are no factors that influence both job type and income that are affected by the level of education.\nThese assumptions are fundamental for the identification of causal mediation effects. If these assumptions are violated, the estimates of the mediation effect can be biased. Importantly, these assumptions cannot be fully tested with observed data. They require substantive knowledge about the underlying causal process. Note that when assumption 4 is violated, natural direct and indirect effects are not identified in the data. [Cite Tyler here]\n\n\n\n\n\n\n\n\nFigure 13: Assumptions for mediation analysis\n\n\n\n\n\n\n\nAdvice for modelling repeated exposures in longitudinal data (confounder-treatment feedback)?\nCausal mediation is a special case in which we have multiple sequential exposures.\nFor example, consider temporally fixed multiple exposures. The counterfactual outcomes may be denoted Y(a_{t1} ,a_{t2}). There are four counterfactual outcomes corresponding to the four fixed “treatment regimes”:\n\nAlways treat (Y(1,1)): This regime involves providing the treatment at every opportunity.\nNever treat (Y(0,0)): This regime involves abstaining from providing the treatment at any opportunity.\nTreat once first (Y(1,0)): This regime involves providing the treatment only at the first opportunity and not at subsequent one.\nTreat once second (Y(0,1)): This regime involves abstaining from providing the treatment at the first opportunity, but then providing it at the second one.\n\nThere are six causal contrasts that we might compute.1\n\nAlways treat vs. Never treat\nAlways treat vs. Treat once first\nAlways treat vs. Treat once second\nNever treat vs. Treat once first\nNever treat vs. Treat once second\nTreat once first vs. Treat once second\n\nWe might also consider treatment to be a function of the previous outcome. For example, we might Treat once first and then treat again or do not treat again depending on the outcome of the previous treatment. This is called “time-varying treatment regimes.”\nNote that to estimate the “effect” of a treatment regime, we must compare the counterfactual quantities of interest. The same conditions that apply for causal identification in mediation analysis apply to causal idenification in multiple treatment settings. And notice, just as mediation opens the possibility of time-varying confounding (condition 4, in which the exposure effects the confounders of the mediator/outcome path), so too we find that with time-varying treatments comes the problem of time-varying confounding. Unlike traditional causal mediation analysis, the sequence of treatement regimes that we might consider is indefinitely long.\nTemporally organised causal diagrammes help us to discover the problems with traditional multi-level regression analysis and structural equation modelling. Suppose we are interested in the question of whether beliefs in big Gods affect social complexity.\nFirst consider fixed regimes Suppose we have well-defined concept of social complexity and excellent measurements over time. Suppose we want to compare the effects of beliefs on big Gods on Social complexity using historical data measured over two centuries. Our question is whether the introduction and persistence of such beliefs differs from having no such beliefs. The treatment strategies are: “always believe in big Gods” versus “never believe in big Gods” on the level of social complexity. The a causal diagram illustrates two time points in our study the study.\nHere, A_{tx} represents the cultural belief in “big Gods” at time x, and Y_{tx} is the outcome, social complexity, at time x. Economic trade, denoted as L_{tx}, is a time-varying confounder because it varies over time and confounds the effect of A on Y at several time points x. To complete our causal diagramme we include an unmeasured confounder U, such as geographical constraints, which might influence both the belief in “big Gods” and social complexity.\nWe know that the level of economic trade at time 0, L_{t0}, influences the belief in “big Gods” at time 1, A_{t1}. We therefore draw an arrow from L_{t0} to A_{t1}. But we also know that the belief in “big Gods”, A_{t1}, affects the future level of economic trade, L_{t(2)}. This means that we need to add an arrow from A_{t1} to L_{t(2)}. This causal graph represents a feedback process between the time-varying exposure A and the time-varying confounder L. This is the simplest graph with exposure-confounder feedback. In real world setting there could be arrows. However, our DAG however need show the minimum number of arrows to exhibit the problem of exposure-confounder feedback.\nWhat happens if we condition on the time-varying confounder L_{t3}. Two things occur. First, we block all the backdoor paths between the exposure A_{t2} and the outcome. We need to block those paths to eliminate confounding. Therefore, condition on the time-varying confounding would appear to be essential. Second, paths that were previously blocked are now open. For example, the path A_{t1}, L_{t2}, U, Y_{t(4)}, which was previous closed is opened because the time varying confounder is the common effect of A_{t1} and U. Conditioning opens the path A_{t1}, L_{t2}, U, Y_{3}. The same problem occurs if the time-varying exposure and time-varying confounder share a common cause (without the exposure affecting the confounder). And the problem is only more entrenched when the exposures A_{t1} affects the outcome Y_{t4}. Because L_{t2} is along the path from A_{t1} to Y_{t4} conditioning on L_{t2} partially blocks the path between the exposure and the outcome. Conditioning on L_{t2} in this setting induces both collider stratification bias and mediator bias. Yet we must conditoin on L_{t2} to block the open backdoor path between L_{t2} and Y_{t4}. The general problem of xposure-confounder feedback is described in detail in [@hernan2023]. This problem presents a serious issue for cultural evolutionary studies. The bad news is that nearly traditional regresion based methods cannot address this problem. The good new is that\n\n\n\n\n\n\n\n\nFigure 14: Exposure confounder feedback is a problem for time-series models. Unfortunately, this problem cannot be addressed with regression-based methods, whatever the combination of Bayesian, multi-level, and phylogentic sophistication. We may only estimate controlled (simulated) effects in these setting using G-methods. Currently, outside of epidemiology, g-methods are rarely used.\n\n\n\n\n\nMore about SWIGS…"
  },
  {
    "objectID": "content/common_graphs.html#footnotes",
    "href": "content/common_graphs.html#footnotes",
    "title": "Temporal Causal Diagrams: Unveiling Causal Order and Indexing Nodes by Time",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe may compute the combination of contrasts by C(n, r) = \\frac{n!}{(n-r)! \\cdot r!}↩︎"
  },
  {
    "objectID": "content/10-content.html",
    "href": "content/10-content.html",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "",
    "text": "Today, we dive deep into data analysis for causal inference ast it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/10-content.html#overview",
    "href": "content/10-content.html#overview",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "",
    "text": "Today, we dive deep into data analysis for causal inference ast it applies to observational cultural psychology. By the end, you will:\n\nBetter understand how to integrate measurement theory with causal inference\nEnhance your proficiency in causal analysis using doubly robust methods\nGain insights into the application of sensitivity analysis using E-Values"
  },
  {
    "objectID": "content/10-content.html#set-up-your-workspace-preparing-for-the-journey",
    "href": "content/10-content.html#set-up-your-workspace-preparing-for-the-journey",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Set up your workspace Preparing for the Journey",
    "text": "Set up your workspace Preparing for the Journey\nTo kick things off, we will set up our environment. We’ll source essential functions, load necessary libraries, and import synthetic data for our exploration.\n\n\nCode\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/libs2.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/funs.R\")\n\n\nsource(\"/Users/joseph/GIT/templates/functions/experimental_funs.R\")\n\n\n# ALERT: UNCOMMENT THIS AND DOWNLOAD THE FUNCTIONS FROM JB's GITHUB\n# source(\n#   \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n# )\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables.\ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2, ], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n# extra packages we need\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\n\nImport the data\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\n\nnzavs_synth &lt;- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt is alwasy a good idea to plot the data (do on your own time.)"
  },
  {
    "objectID": "content/10-content.html#revisit-the-checklist",
    "href": "content/10-content.html#revisit-the-checklist",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Revisit the checklist",
    "text": "Revisit the checklist\nIt is essential to remember our checklist:\n\nClearly state your question.\nExplain its relevance.\nEnsure your question is causal.\nDevelop a subgroup analysis question if applicable.\n\nOur discussion today revolves around two main questions:\n\nDoes exercise influence anxiety/depression?\nDo these effects differ among NZ Europeans and Māori?\n\nWhile these questions offer a starting point, they lack specificity. We need to clarify:\n\nThe amount, regularity, and duration of exercise\nThe measures of depression to be used\nThe expected timeline for observing the effects\n\nRemember, we can clarify these by emulating a hypothetical experiment, a concept we call the Target Trial.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification.\nThis analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\nOur initial responses will be guided by the NZAVS measure of exercise, focusing on the hours of activity per week, the 1-year effect on Kessler-6 depression after initiating a change in exercise, and a particular emphasis on effect-modification by NZ European and Māori ethnic identification. This analysis has practical motivation, as the effects of exercise on mental health and possible differences between cultural groups remain largely uncharted territory.\n\nSculpting the Data: A Hands-On Approach\nAs we venture further, we’ll perform a series of transformations to shape our data according to our needs. Our process will involve:\n\nConstructing a Kessler 6 average score\nBuilding a Kessler 6 sum score\nCoarsening the Exercise score\n\nConsider the ambiguity in the NZAVS exercise question: “During the past week, list ‘Hours spent exercising/physical activity’.” Different people interpret physical activity differently; John may consider any wakeful time as physical activity, while Jane counts only aerobic exercise. Such variation underlines the importance of the consistency assumption in causal inference. But we’ll delve deeper into that later.\nFor now, let’s transform our indicators.\n\n# create sum score of kessler 6\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(lifesat_composite  = mean(\n    c(lifesat_satlife,                         \n    lifesat_ideal) ))|&gt; \n  mutate(kessler_6  = mean(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )\n  )) |&gt;\n  mutate(kessler_6_sum = round(sum(\n    c (\n      kessler_depressed,\n      kessler_hopeless,\n      kessler_nervous,\n      kessler_effort,\n      kessler_restless,\n      kessler_worthless\n    )\n  ),\n  digits = 0)) |&gt;  ungroup() |&gt;\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"),\n      ordered = TRUE\n    ))|&gt;\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\nWhy do we coarsen the exposure? Recall the consistency assumption of causal inference:\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nWhat is th hypothetical experiment here for change in exercise?\nThrough data wrangling, we can answer our research questions more effectively by manipulating variables into more meaningful and digestible forms. We imagine an experiment in which people were within one band of the coarsened exercise band and we\nThese data checks will ensure the accuracy and reliability of our transformations, setting the foundation for solid data analysis.\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\n\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nCreate variables for the latent factors\nLet’s next get the data into shape for analysis. Here we create a variable for the two factors (see Appendix)\n\n# get two factors from data\ndt_start2 &lt;- dt_start |&gt;\n  arrange(id, wave) |&gt;\n  rowwise() |&gt;\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\nInspect the data: anxiety\n\n#hist(dt_start2$kessler_latent_anxiety, by = dt_start2$eth_cat)\n\ncreate_histograms_anxiety &lt;- function(df) {\n  # require patchwork\n  library(patchwork)\n  \n  # separate the data by eth_cat\n  df1 &lt;- df %&gt;% filter(eth_cat == \"euro\") # replace \"level_1\" with actual level\n  df2 &lt;- df %&gt;% filter(eth_cat == \"māori\") # replace \"level_2\" with actual level\n  \n  # create the histograms\n  p1 &lt;- ggplot(df1, aes(x=kessler_latent_anxiety)) +\n    geom_histogram(binwidth = 1, fill = \"dodgerblue\", color = \"black\") +\n    ggtitle(\"Kessler Latent Anxiety: NZ Euro\") +\n    xlab(\"kessler_latent_anxiety\") +\n    ylab(\"Count\")\n  \n  p2 &lt;- ggplot(df2, aes(x=kessler_latent_anxiety)) +\n    geom_histogram(binwidth = 1, fill = \"brown\", color = \"black\") +\n    ggtitle(\"Kessler Latent Anxiety: Māori\") +\n    xlab(\"kessler_latent_anxiety\") +\n    ylab(\"Count\")\n  \n  # plot the histograms\n p1 + p2 + plot_annotation(tag_levels = \"a\", title = \"comparison of anxiety histograms\")\n}\n\ncreate_histograms_anxiety(dt_start2)\n\n\n\n\n\n\n\n\n\ncreate_histograms_depression &lt;- function(df) {\n  # require patchwork\n  library(patchwork)\n  \n  # separate the data by eth_cat\n  df11 &lt;- df %&gt;% filter(eth_cat == \"euro\") # replace \"level_1\" with actual level\n  df22 &lt;- df %&gt;% filter(eth_cat == \"māori\") # replace \"level_2\" with actual level\n  \n  # create the histograms\n  p11 &lt;- ggplot(df11, aes(x=kessler_latent_depression)) +\n    geom_histogram(binwidth = 1, fill = \"dodgerblue\", color = \"black\") +\n    ggtitle(\"Kessler Latent Depression: NZ Euro\") +\n    xlab(\"kessler_latent_depression\") +\n    ylab(\"Count\") + theme_classic()\n  \n  p22 &lt;- ggplot(df22, aes(x=kessler_latent_depression)) +\n    geom_histogram(binwidth = 1, fill = \"brown\", color = \"black\") +\n    ggtitle(\"Kessler Latent Depression: Māori\") +\n    xlab(\"kessler_latent_depression\") +\n    ylab(\"Count\") + theme_classic()\n  \n  # plot the histograms\n p11 + p22 + plot_annotation(tag_levels = \"a\", title = \"comparison of depression histograms\")\n}\n\ncreate_histograms_depression(dt_start2)\n\n\n\n\n\n\n\n\nWhat do you make of these histograms?"
  },
  {
    "objectID": "content/10-content.html#investigate-assumption-of-positivity",
    "href": "content/10-content.html#investigate-assumption-of-positivity",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Investigate assumption of positivity:",
    "text": "Investigate assumption of positivity:\nRecall the positive assumption:\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nNot this is just a description of the the summary scores. We do not assess change within indivuals\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start2 |&gt;\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table_2(out, state_names)\n\n$explanation\n[1] \"The table presents a transition matrix that describes stability and movement between the treatment from the baseline wave to the treatment wave. Entries on the diagonal (in bold) indicate the number of individuals who stayed in their initial state. In contrast, the off-diagonal shows the transitions from the initial state (bold) to another state the following wave (off diagnal). A cell located at the intersection of row $i$ and column $j$, where $i \\neq j$, presents the count of individuals moving from state $i$ to state $j$.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active  |\n|:---------------:|:--------:|:---------------:|:-------:|\n|    Inactive     | **2186** |      1324       |   295   |\n| Somewhat Active |   1019   |    **2512**     |   811   |\n|     Active      |   204    |       668       | **981** |\n\n\nNext consider Māori only\n\n# Maori only\n\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table_2( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\nThe table presents a transition matrix that describes stability and movement between the treatment from the baseline wave to the treatment wave. Entries on the diagonal (in bold) indicate the number of individuals who stayed in their initial state. In contrast, the off-diagonal shows the transitions from the initial state (bold) to another state the following wave (off diagnal). A cell located at the intersection of row $i$ and column $j$, where $i \neq j$, presents the count of individuals moving from state $i$ to state $j$.\n\nprint(t_tab_m$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     | **187**  |       108       |   24   |\n| Somewhat Active |    92    |     **188**     |   61   |\n|     Active      |    28    |       58        | **75** |\n\n\n\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\nt_tab_e &lt;- transition_table_2( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\nThe table presents a transition matrix that describes stability and movement between the treatment from the baseline wave to the treatment wave. Entries on the diagonal (in bold) indicate the number of individuals who stayed in their initial state. In contrast, the off-diagonal shows the transitions from the initial state (bold) to another state the following wave (off diagnal). A cell located at the intersection of row $i$ and column $j$, where $i \neq j$, presents the count of individuals moving from state $i$ to state $j$.\n\n# table\nprint(t_tab_e$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active  |\n|:---------------:|:--------:|:---------------:|:-------:|\n|    Inactive     | **1843** |      1136       |   259   |\n| Somewhat Active |   870    |    **2208**     |   712   |\n|     Active      |   167    |       583       | **863** |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\nCreate wide data frame for analysis\nRecall, I wrote a function for you that will put the data into temporal order such that measurement of the exposure and outcome appear at baseline, along with a rich set of baseline confounders, the exposure appears in the following wave, and the outcome appears in the wave following the exposure.\n\n\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design\n\n\n\n\n\nThe graph encodes our assumptions about the world. It is a qualitative instrument to help us understand how to move from our assumptions to decisions about our analysis, in the first instance, the decision about whether to proceed with an analysis.\nIt is perhaps useful here to stop and consider what does this graph implies.\nQuestion: 1. Does the graph imply unmeasured confounding?\nQuestion 2. If there is unmeasured confounding, should we proceed?\n\n\nE-value\n\nThe minimum strength of association on the risk ratio scale that an unmeasured confounder would need to have with both the exposure and the outcome, conditional on the measured covariates, to fully explain away a specific exposure-outcome association\n\nSee: [VanderWeele, Mathur, and Chen (2020)](Mathur et al. 2018)\nFor example, suppose that the lower bound of the the E-value was 1.3 with the lower bound of the confidence interval = 1.12, we might then write:\n\nWith an observed risk ratio of RR=1.3, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 1.3-fold each (or 30%), above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 1.12-fold (or 12%) each could do so, but weaker joint confounder associations could not.\n\nThe equations are as follows (for risk ratios)\n\nE-value_{RR} = RR + \\sqrt{RR \\times (RR - 1)}\n \nE-value_{LCL} = LCL + \\sqrt{LCL \\times (LCL - 1)}\n\nHere is an R function that will calculate E-values\n\ncalculate_e_value &lt;- function(rr, lcl) {\n  e_value_rr = rr + sqrt(rr*(rr - 1))\n  e_value_lcl = lcl + sqrt(lcl*(lcl - 1))\n  \n  list(e_value_rr = e_value_rr, e_value_lcl = e_value_lcl)\n}\n\n# e.g. smoking causes cancer\n\n# finding   RR = 10.73 (95% CI: 8.02, 14.36)\n\ncalculate_e_value(10.73, 8.02)\n\n$e_value_rr\n[1] 20.94777\n\n$e_value_lcl\n[1] 15.52336\n\n\nWe write:\n\nWith an observed risk ratio of RR=10.7, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 20.9-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 15.5-fold each could do so, but weaker joint confounder associations could not.\n\nNote that in this class, most of the outcomes will be (standardised) continuous outcomes. Here’s a function and LaTeX code to describe the approximation.\nThis function takes a linear regression coefficient estimate (est), its standard error (se), the standard deviation of the outcome (sd), a contrast of interest in the exposure (delta, which defaults to 1), and a “true” standardized mean difference (true, which defaults to 0). It calculates the odds ratio using the formula from Chinn (2000) and VanderWeele (2017), and then uses this to calculate the E-value.\n\n#| label: evalue_ols\n\ncompute_evalue_ols &lt;- function(est, se, delta = 1, true = 0) {\n  # Rescale estimate and SE to reflect a contrast of size delta\n  est &lt;- est / delta\n  se &lt;- se / delta\n\n  # Compute transformed odds ratio and confidence intervals\n  odds_ratio &lt;- exp(0.91 * est)\n  lo &lt;- exp(0.91 * est - 1.78 * se)\n  hi &lt;- exp(0.91 * est + 1.78 * se)\n\n  # Compute E-Values based on the RR values\n  evalue_point_estimate &lt;- odds_ratio * sqrt(odds_ratio + 1)\n  evalue_lower_ci &lt;- lo * sqrt(lo + 1)\n\n  # Return the E-values\n  return(list(EValue_PointEstimate = evalue_point_estimate,\n              EValue_LowerCI = evalue_lower_ci))\n}\n\n\n\n\n# exampl:\n# suppose we have an estimate of 0.5, a standard error of 0.1, and a standard deviation of 1.\n# This would correspond to a half a standard deviation increase in the outcome per unit increase in the exposure.\nresults &lt;- compute_evalue_ols(est = 0.5, se = 0.1, delta = 1)\nprint(results)\n\n$EValue_PointEstimate\n[1] 2.529831\n\n$EValue_LowerCI\n[1] 2.008933\n\n\nWe write:\n\nWith an observed risk ratio of RR=2.92, an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of 2.92-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of 2.23-fold each could do so, but weaker joint confounder associations could not.\n\nNote the E-values package will do the computational work for us (note we get slightly different estimates)\nNote:\nFirst, the fucntion converts the estimate to an odds ratio:\n\nOdds Ratio Conversion:\n(OddsRatio = e^{ })\n\nThen, it calculates the confidence intervals for the odds ratio:\n\nConfidence Intervals:\n(LowerConfidenceInterval = e^{log(OddsRatio) - 1.78 SE})\n(UpperConfidenceInterval = e^{log(OddsRatio) + 1.78 SE})\n\nFinally, it calculates the E-value for the point estimate and the lower confidence interval:\n\nE-Values Calculation:\n(EValue_{PointEstimate} = OddsRatio + )\n(EValue_{LowerCI} = LowerConfidenceInterval + )\n\n[[JB: NEED TO CHECK]]\nGenerally, best to use the EValue function.\n\nlibrary(EValue)\n\nEValue::evalues.OLS(est = 0.5, se = 0.1, sd = 1, delta = 1, true = 0)\n\n            point    lower    upper\nRR       1.576173 1.319166 1.883252\nE-values 2.529142 1.968037       NA\n\n\n\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )"
  },
  {
    "objectID": "content/10-content.html#descriptive-table",
    "href": "content/10-content.html#descriptive-table",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Descriptive table",
    "text": "Descriptive table\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n\n\n\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n# another method for making a table\n# x &lt;- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |&gt;\n#   kable_material(c(\"striped\", \"hover\"))\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n# view object\nskimr::skim(dt)\n\n\n\nData summary\n\n\n\n\nName\n\n\ndt\n\n\n\n\nNumber of rows\n\n\n9462\n\n\n\n\nNumber of columns\n\n\n26\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n7\n\n\n\n\nnumeric\n\n\n19\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\nVariable type: factor\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nid\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n9462\n\n\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\n\nt0_male\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nNot: 5767, Mal: 3695\n\n\n\n\nt0_eth_cat\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\neur: 8641, māo: 821\n\n\n\n\nt0_gen_cohort\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n5\n\n\nGen: 4107, Gen: 3311, Gen: 1716, Gen: 164\n\n\n\n\nt0_urban\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nurb: 7762, rur: 1700\n\n\n\n\nt0_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4131, ina: 3557, ver: 1774\n\n\n\n\nt1_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4281, ina: 3187, ver: 1994\n\n\n\n\nVariable type: numeric\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nt0_edu_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.29\n\n\n-1.05\n\n\n0.44\n\n\n0.82\n\n\n1.66\n\n\n▂▃▃▇▂\n\n\n\n\nt0_employed_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.26\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n▂▁▁▁▇\n\n\n\n\nt0_nz_dep2018_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.36\n\n\n-0.92\n\n\n-0.16\n\n\n0.63\n\n\n2.17\n\n\n▇▆▆▅▂\n\n\n\n\nt0_nzsei13_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.94\n\n\n-0.75\n\n\n0.25\n\n\n0.81\n\n\n2.07\n\n\n▁▃▅▇▁\n\n\n\n\nt0_partner_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.99\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n▂▁▁▁▇\n\n\n\n\nt0_parent_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.58\n\n\n-1.58\n\n\n0.63\n\n\n0.63\n\n\n0.63\n\n\n▃▁▁▁▇\n\n\n\n\nt0_pol_orient_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.87\n\n\n-1.02\n\n\n-0.28\n\n\n0.44\n\n\n2.62\n\n\n▇▆▇▅▂\n\n\n\n\nt0_agreeableness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.46\n\n\n-0.62\n\n\n0.12\n\n\n0.68\n\n\n1.79\n\n\n▁▁▃▇▆\n\n\n\n\nt0_conscientiousness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.13\n\n\n-0.65\n\n\n0.08\n\n\n0.76\n\n\n1.91\n\n\n▁▁▅▇▅\n\n\n\n\nt0_extraversion_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.48\n\n\n-0.71\n\n\n-0.04\n\n\n0.72\n\n\n2.67\n\n\n▂▆▇▅▁\n\n\n\n\nt0_honesty_humility_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.95\n\n\n-0.69\n\n\n0.17\n\n\n0.84\n\n\n1.45\n\n\n▁▁▃▆▇\n\n\n\n\nt0_openness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.76\n\n\n-0.71\n\n\n0.05\n\n\n0.81\n\n\n1.90\n\n\n▁▂▆▇▅\n\n\n\n\nt0_neuroticism_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.18\n\n\n-0.76\n\n\n-0.09\n\n\n0.71\n\n\n3.14\n\n\n▃▇▇▃▁\n\n\n\n\nt0_modesty_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.67\n\n\n-0.66\n\n\n0.19\n\n\n0.83\n\n\n1.26\n\n\n▁▁▂▅▇\n\n\n\n\nt0_religion_identification_level_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-0.56\n\n\n-0.56\n\n\n-0.56\n\n\n-0.08\n\n\n2.37\n\n\n▇▁▁▁▂\n\n\n\n\nt0_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.72\n\n\n-0.69\n\n\n-0.19\n\n\n0.70\n\n\n4.01\n\n\n▇▇▆▁▁\n\n\n\n\nt0_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.21\n\n\n-0.63\n\n\n-0.13\n\n\n0.42\n\n\n4.83\n\n\n▇▂▂▁▁\n\n\n\n\nt2_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.23\n\n\n-0.65\n\n\n-0.16\n\n\n0.39\n\n\n4.75\n\n\n▇▃▂▁▁\n\n\n\n\nt2_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.74\n\n\n-0.70\n\n\n-0.20\n\n\n0.68\n\n\n3.97\n\n\n▇▇▆▁▁\n\n\n\n\n\n\n\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))"
  },
  {
    "objectID": "content/10-content.html#propensity-scores",
    "href": "content/10-content.html#propensity-scores",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Propensity scores",
    "text": "Propensity scores\nNext we generate propensity scores. Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\nThe first step is to obtain the baseline variables. note that we must remove “t0_eth_cat” because we are performing separate weighting for each stratum within this variable.\n\n# read  data -- you may start here if you need to repeat the analysis\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use ps (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\nResults, first for Europeans\n\n#dt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\n#dt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n#bal.tab(dt_match_ps$euro)   #  not as good\n\n# here we show only the best tab, but you should put all information into an appendix\n\nResults for Maori\n\n# who only Ebal\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n#bal.tab(dt_match_ps$māori)   #  not good\n\n\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2310 |---------------------------| 7.0511\nactive      0.5769  |----|                       1.9603\nvery_active 0.1601 |----------------------|      5.9191\n\n- Units with the 5 most extreme weights by group:\n                                               \n               6560      9   7209   4878   5105\n    inactive 5.1084 5.1312 5.1642 5.3517 7.0511\n               3279   1867   4754   2783   7057\n      active 1.7467 1.7654 1.7701 1.8692 1.9603\n               5977   4293    700   2352   4765\n very_active 5.1495 5.3064 5.4829 5.7273 5.9191\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.743 0.536   0.212       0\nactive            0.270 0.248   0.036       0\nvery_active       0.862 0.637   0.302       0\n\n- Effective Sample Sizes:\n\n           inactive  active very_active\nUnweighted  2880.   3927.       1834.  \nWeighted    1855.89 3659.59     1052.01\n\n# summary maori\nsum_m\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2213  |---------------|            3.8101\nactive      0.3995   |-----|                     1.9800\nvery_active 0.0719 |---------------------------| 6.2941\n\n- Units with the 5 most extreme weights by group:\n                                               \n                296    322    355    758    812\n    inactive 3.0104  3.407 3.6372 3.7101 3.8101\n                 95    783    473    703    699\n      active 1.8319 1.8387 1.9395 1.9436   1.98\n                745    149     78    226    718\n very_active 4.0921 4.3405 4.4111 4.6833 6.2941\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.627 0.475   0.170       0\nactive            0.321 0.264   0.050       0\nvery_active       1.050 0.732   0.411       0\n\n- Effective Sample Sizes:\n\n           inactive active very_active\nUnweighted   307.   354.        160.  \nWeighted     220.54 321.09       76.39\n\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\")\n\n# plot\nlove_plot_e \n\n\n\n\n\n\n\n\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori Weighting: method e-balance\")\n# plot\nlove_plot_m\n\n\n\n\n\n\n\n\n\nExample Summary NZ Euro Propensity scores.\nWe estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\nThe effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\nThe weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nWe also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nWe plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\nOverall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n\nExample Summary Maori Propensity scores.\nResults:\nThe entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\nThe effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\nThe weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\nThe study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\nIn conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n\nMore data wrangling\nNote that we need to attach the weights from the propensity score model back to the data.\nHowever, because our weighting analysis estimates a model for the exposure, we only need to do this analysis once, no matter how many outcomes we investigate. So there’s a little good news.\n\n# prepare nz_euro data\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe."
  },
  {
    "objectID": "content/10-content.html#graph-of-the-result",
    "href": "content/10-content.html#graph-of-the-result",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Graph of the result",
    "text": "Graph of the result\nI’ve create a function you can use to graph your results. Here is the code, adjust to suit.\n\n# group tables\nsub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\nReport the anxiety result.\n\nFor the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate.\n\n\nE-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect.\n\n\nTurning to the Māori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect.\n\n\nThus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and Māori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture.\n\nHere’s a function that will do much of this work for you. However, you’ll need to adjust it, and supply your own interpretation.\n\n#|label: interpretation function\n#| eval: false\ninterpret_results_subgroup &lt;- function(df, outcome, exposure) {\n  df &lt;- df %&gt;%\n    mutate(\n      report = case_when(\n        E_Val_bound &gt; 1.2 & E_Val_bound &lt; 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\"\n        ),\n        E_Val_bound &gt;= 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\"\n        ),\n        E_Val_bound &lt; 1.2 & E_Val_bound &gt; 1 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"\n        ),\n        E_Val_bound == 1 ~ paste0(\n          \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \",\n          \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n        )\n      )\n    )\n  return(df$report)\n}\n\nYou run the function like this:\n\ninterpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\")\n\n[1] \"For the NZ Euro Anxiety, our results suggest that Excercise may potentially influence Anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077.\\nThe associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate. The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of 1.352 with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of 1.167 to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"                                                                                                                                      \n[2] \"For the Māori Anxiety, the data suggests a potential effect of Excercise on Anxiety, with a causal contrast value of 0.027.\\nHowever, the confidence interval for this estimate, ranging from -0.114 to 0.188, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Anxiety and the Excercise by a risk ratio of 1.183 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Excercise on Anxiety for the Māori Anxiety, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n\n\nEasy!\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make the sumamry into a dataframe so we can make a table\ndf &lt;- as.data.frame(summary(est_all_anxiety))\n\n# get rownames for selecting the correct row\ndf$RowName &lt;- row.names(df)\n\n# select the correct row -- the group contrast\nfiltered_df &lt;- df |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# pring the filtered data frame\nlibrary(kableExtra)\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\n\n\nAnother option for making the table using markdown. This would be useful if you were writing your article using qaurto.\n\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3, format = \"markdown\")\n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\nReport result along the following lines:\n\nThe estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and Māori is reliable. It’s possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for Māori. Thus, while there’s an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution."
  },
  {
    "objectID": "content/10-content.html#depression-analysis-and-results",
    "href": "content/10-content.html#depression-analysis-and-results",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Depression Analysis and Results",
    "text": "Depression Analysis and Results\n\n### SUBGROUP analysis\ndt_ref_all &lt;- readRDS(here::here(\"data\", \"dt_ref_all\"))\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\"))\nsaveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\"))\n\n\nReport anxiety results\n\n# return stored estimates \nsim_estimand_all_e_d &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\"))\nsim_estimand_all_m_d&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\"))\n\n# create individual summaries \nsum_e_d &lt;- summary(sim_estimand_all_e_d)\nsum_m_d &lt;- summary(sim_estimand_all_m_d)\n\n\n# create individual tables\ntab_ed &lt;- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\")\ntab_md &lt;- sub_tab_ate(sum_m_d, new_name = \"Māori Depression\")\n\n\n# expand tables \nplot_ed &lt;- sub_group_tab(tab_ed, type= \"RD\")\nplot_md &lt;- sub_group_tab(tab_md, type= \"RD\")\n\nbig_tab_d &lt;- rbind(plot_ed,plot_md)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab_d |&gt; \n  kbl(format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nE[Y(1)]-E[Y(0)]\n2.5 %\n97.5 %\nE_Value\nE_Val_bound\nEstimate\nestimate_lab\n\n\n\n\nNZ Euro Depression\n-0.039\n-0.099\n0.019\n1.231\n1\nunreliable\n-0.039 (-0.099-0.019) [EV 1.231/1]\n\n\nMāori Depression\n0.028\n-0.125\n0.176\n1.190\n1\nunreliable\n0.028 (-0.125-0.176) [EV 1.19/1]\n\n\n\n\n\n\n\nGraph Anxiety result\n\n# group tables\nsub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\n\nInterpretation\nUse the function, again, modify the outputs to fit with your study and results and provide your own interpretation.\n\ninterpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\")\n\n[1] \"For the NZ Euro Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of -0.039.\\nHowever, the confidence interval for this estimate, ranging from -0.099 to 0.019, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.231 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the NZ Euro Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n[2] \"For the Māori Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of 0.028.\\nHowever, the confidence interval for this estimate, ranging from -0.125 to 0.176, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.19 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the Māori Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"      \n\n\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_d &lt;- readRDS( here::here(\"data\",\"est_all_d\"))\n\n# make the sumamry into a dataframe so we can make a table\ndfd &lt;- as.data.frame(summary(est_all_d))\n\n# get rownames for selecting the correct row\ndfd$RowName &lt;- row.names(dfd)\n\n# select the correct row -- the group contrast\nfiltered_dfd &lt;- dfd |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# Print the filtered data frame\nlibrary(kableExtra)\nfiltered_dfd  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.068\n-0.09\n0.229\n\n\n\n\n\n\n\nReporting might be:\n\nThe estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and Māori is statistically significant. It’s possible that the true difference could be zero or even negative, implying a greater depression reduction for Māori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution.\n\n\n\nDiscusion\nYou’ll need to do this yourself. Here’s a start:\n\nIn our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups – New Zealand Europeans and Māori. This method has the advantage of providing reliable results even if our underlying assumptions aren’t entirely accurate – a likely scenario given the complexity of real-world data. However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn’t mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have.\n\n\nExercise and Anxiety\n\nOur analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to Māori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it’s crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in Māori. Hence, while there’s an indication of a difference, we should interpret it cautiously given the wide range of uncertainty.\n\n\nThus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns.\n\n\n\nExercise and Depression\n\nIn addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise – as self-reported – on depression.\n\n\n\nProviso\n\nIt is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.\\dots nevertheless, \\dots"
  },
  {
    "objectID": "content/10-content.html#exercises",
    "href": "content/10-content.html#exercises",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Exercises",
    "text": "Exercises\n\nGenerate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\n\nand also:\n\nCreate a variable for the log of exercise hour\nTake Home: estimate whether exercise causally affects nervousness, using the single item of the kessler 6 score. Briefly write up your results."
  },
  {
    "objectID": "content/10-content.html#appendix-mg-cfa",
    "href": "content/10-content.html#appendix-mg-cfa",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Appendix: MG-CFA",
    "text": "Appendix: MG-CFA\n\nCFA for Kessler 6\nWe have learned how to do confirmatory factor analysis. Let’s put this knowledge to use but clarifying the underlying factor structure of Kessler-6\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\n# select the columns we need. \ndt_only_k6 &lt;- dt_start |&gt; select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 70564.23, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.86). The individual KMO scores are: kessler_depressed (0.83), kessler_effort (0.89), kessler_hopeless (0.85), kessler_worthless (0.85), kessler_nervous (0.88), kessler_restless (0.85).\n\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors.\n\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) %&gt;%\n  model_parameters(sort = TRUE, threshold = \"max\")\n\nefa\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          | MR1  | MR2  | MR3  | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_depressed | 0.85 |      |      |    1.01    |    0.33   \nkessler_worthless | 0.79 |      |      |    1.00    |    0.35   \nkessler_hopeless  | 0.75 |      |      |    1.02    |    0.33   \nkessler_nervous   |      | 1.00 |      |    1.00    |  5.00e-03 \nkessler_restless  |      |      | 0.69 |    1.02    |    0.52   \nkessler_effort    |      |      | 0.48 |    1.66    |    0.50   \n\nThe 3 latent factors (oblimin rotation) accounted for 66.05% of the total variance of the original data (MR1 = 35.14%, MR2 = 17.17%, MR3 = 13.73%).\n\n\nThis output describes an exploratory factor analysis (EFA) with 3 factors conducted on the Kessler 6 (K6) scale data. The K6 scale is used to measure psychological distress.\nThe analysis identifies three latent factors, labeled MR1, MR2, and MR3, which collectively account for 66.05% of the variance in the K6 data. The factors MR1, MR2, and MR3 explain 35.14%, 17.17%, and 13.73% of the variance respectively.\nFactor loadings, indicating the strength and direction of the relationship between the K6 items and the latent factors, are as follows:\n\nFactor MR1 is strongly associated with ‘kessler_depressed’, ‘kessler_worthless’, and ‘kessler_hopeless’ with loadings of 0.85, 0.79, and 0.75 respectively.\nFactor MR2 is exclusively linked with ‘kessler_nervous’ with a loading of 1.00.\nFactor MR3 relates to ‘kessler_restless’ and ‘kessler_effort’ with loadings of 0.69 and 0.48 respectively.\n\nThe ‘Uniqueness’ values show the proportion of each variable’s variance that isn’t shared with the other variables.\nThe ‘Complexity’ values give a measure of how each item loads on more than one factor. All the items are either loading exclusively on one factor (complexity=1.00) or slightly more than one factor. ‘kessler_effort’ with complexity of 1.66 shows it’s the item most shared between the factors.\nThe analysis suggests these K6 items measure may measure three somewhat distinct, yet related, factors of psychological distress.\nHowever, the meaning of these factors would need to be interpreted in the context of the variables and the theoretical framework of the study.\nNotably, there are many many theoretical frameworks for in measurement theory. Here is a brief description of the different conclusions one might make, depending on one’s preferred theory.\n\n\nCode\nn &lt;- n_factors(dt_only_k6)\n\n# plot\nplot(n) + theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nConfirmatory factor analysis (ignoring groups)\n\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .7, seed = 123)\n\n\n# set up training data\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n# one factor model\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three factor model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_two\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n\nstructure_k6_three\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_restless\nMR3 =~ kessler_nervous + .row_id\n\n\nNext we perform the confirmatory factor analysis.\n\n# fit and compare models\n\n# one latent model\none_latent &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\n\n# two latents model\ntwo_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\n\n# three latents model\nthree_latents &lt;-\n  suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\n\n# compare models\ncompare &lt;-\n  performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare) |&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent\nlavaan\n1359.7168\n14\n0\n159746.19\n21\n0\n0.9533955\n0.9067909\n0.9914883\n0.9873622\n0.9915748\n0.1033455\n0.0987385\n0.1080285\n0.0000000\n36.00334\n0.0493327\n0.9872324\n0.6609922\n0.9915752\n0.9915748\n-151483.7\n302995.3\n0\n303094.8\n0\n303050.3\n\n\ntwo_latents\nlavaan\n317.9709\n13\n0\n30915.75\n21\n0\n0.9900793\n0.9786322\n0.9897149\n0.9840541\n0.9901287\n0.0510548\n0.0462789\n0.0559908\n0.3499758\n36.31236\n0.0226983\n0.9833856\n0.6126807\n0.9901313\n0.9901287\n-150962.8\n301955.6\n1\n302062.2\n1\n302014.5\n\n\nthree_latents\nlavaan\n747.8723\n12\n0\n20903.30\n21\n0\n0.9763317\n0.9447739\n0.9642223\n0.9383317\n0.9647609\n0.0825447\n0.0775761\n0.0876237\n0.0000000\n37.13824\n0.0377955\n0.9373890\n0.5509842\n0.9647761\n0.9647609\n-151177.7\n302387.5\n0\n302501.2\n0\n302450.3\n\n\n\n\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\nOne_latent CFA:\nThis model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p&lt;0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can’t solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n\nTwo_latents CFA\nThis model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n\nThree_latents CFA\nThis model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It’s also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically.\n\n\n\nMulti-group Confirmatory Factor Analysis\nThis script runs multi-group confirmatory factor analysis (MG-CFA)\n\n# select needed columns plus 'ethnicity'\n# filter dataset for only 'euro' and 'maori' ethnic categories\ndt_eth_k6_eth &lt;- dt_start |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n         kessler_worthless, kessler_nervous,\n         kessler_restless, eth_cat)\n\n# partition the dataset into training and test subsets\n# stratify by ethnic category to ensure balanced representation\npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .7, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n# run confirmatory factor analysis (CFA) models for configural invariance across ethnic groups\n# models specify one, two, and three latent variables\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\n# compare model performances for configural invariance\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n# run CFA models for metric invariance, holding factor loadings equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\n# compare model performances for metric invariance\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric, verbose = FALSE)\n\n# run CFA models for scalar invariance, holding factor loadings and intercepts equal across groups\n# models specify one, two, and three latent variables\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\n# compare model performances for scalar invariance\ncompare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar, verbose = FALSE)\n\nRecall, in the context of measurement and factor analysis, the concepts of configural, metric, and scalar invariance relate to the comparability of a measurement instrument, such as a survey or test, across different groups.\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\nLet’s first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\nConfigural invariance refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying or “latent” constructs (factors) are defined the same way for different groups. This doesn’t mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.\n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\nMetric invariance (also known as “weak invariance”) refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.\n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.\n\nScalar invariance (also known as “strong invariance”) involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\nConfigural invariance\n\nas.data.frame(compare_eth_configural)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_configural\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_configural\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_configural\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model. We will return to these issues next week.\n\n\nMetric Equivalence\n\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_metric\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_metric\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_metric\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Statistically non-significant values at the traditional threshold (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is statistically non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nScalar Equivalence\n\n# view as html table\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_scalar\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_scalar\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_scalar\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, we must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model (a matter to which we will return next week.)\nOverall it seems that we have good evidence for the two-factor model of Kessler-6."
  },
  {
    "objectID": "content/10-content.html#solutions",
    "href": "content/10-content.html#solutions",
    "title": "Causal Inference: reconsidering cross-cultural experiments",
    "section": "Solutions",
    "text": "Solutions\n\nGenerate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\n\nand also:\n\nCreate a variable for the log of exercise hour\n\n\n# functions \n#source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\n#source(\n # \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n#)\n\n\n\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\ndt_new &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(kessler_6  = mean(sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )\n  ))) |&gt;\n  mutate(kessler_6_sum = round(sum(\n    c (\n      kessler_depressed,\n      kessler_hopeless,\n      kessler_nervous,\n      kessler_effort,\n      kessler_restless,\n      kessler_worthless\n    )\n  ),\n  digits = 0)) |&gt;  ungroup() |&gt;\n  # Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n      labels = c(\"not_depressed\",\n                 \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n\n\nTake Home: estimate whether exercise causally affects nervousness, using the single item of the kessler 6 score. Briefly write up your results.\n\nTo be reviewed next week."
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Figure 1: Causal graph: we will refer to this image in the lecture and begin reviewing causal graphs in Week 2"
  },
  {
    "objectID": "content/01-content.html#overview",
    "href": "content/01-content.html#overview",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "content/01-content.html#slides",
    "href": "content/01-content.html#slides",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "content/01-content.html#background-readings",
    "href": "content/01-content.html#background-readings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Background Readings",
    "text": "Background Readings\nNone today:"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html",
    "href": "content/v_2_tempate_causal_estimation.html",
    "title": "Template: Causal Estimatatio",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#intoduction",
    "href": "content/v_2_tempate_causal_estimation.html#intoduction",
    "title": "Template: Causal Estimatatio",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#methods",
    "href": "content/v_2_tempate_causal_estimation.html#methods",
    "title": "Template: Causal Estimatatio",
    "section": "Methods",
    "text": "Methods\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 9.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\n\nState what E-values are and how you will use them to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#results",
    "href": "content/v_2_tempate_causal_estimation.html#results",
    "title": "Template: Causal Estimatatio",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n\nReport the E-value: how sensitive are your results to unmeasured confounding? Hint I gave you code that will create a table for you: See here\n\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#discussion",
    "href": "content/v_2_tempate_causal_estimation.html#discussion",
    "title": "Template: Causal Estimatatio",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#example-anlaysis-week-10",
    "href": "content/v_2_tempate_causal_estimation.html#example-anlaysis-week-10",
    "title": "Template: Causal Estimatatio",
    "section": "Example anlaysis (week 10)",
    "text": "Example anlaysis (week 10)"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#load-libraries",
    "href": "content/v_2_tempate_causal_estimation.html#load-libraries",
    "title": "Template: Causal Estimatatio",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/libs2.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/funs.R\")\n\n\nsource(\"/Users/joseph/GIT/templates/functions/experimental_funs.R\")\n\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n\n\nImport data\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\n\nnzavs_synth &lt;- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\n\nFind column names\n\n\nTransform indicators\n\nWhat is the effect of exercise as measured by the NZAVS exercise scale on (1) depression and (2) anxiety.\n\n\n# questions are:\n      # kessler_depressed,\n      # # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      # kessler_hopeless,\n      # # During the last 30 days, how often did you feel hopeless?\n      # kessler_nervous,\n      # # During the last 30 days, how often did you feel nervous?\n      # kessler_effort,\n      # # During the last 30 days, how often did you feel that everything was an effort?\n      # kessler_restless,\n      # # During the last 30 days, how often did you feel restless or fidgety ?\n      # kessler_worthless  # During the last 30 days, how often did you feel worthless?\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    # see week 10 appendix 1 for a detailed explanation of how we obtain these 2 x factors\n    kessler_latent_depression = mean(\n      c(kessler_depressed, kessler_hopeless, kessler_effort),\n      na.rm = TRUE\n    ),\n    kessler_latent_anxiety  = mean(c(\n      kessler_effort, kessler_nervous, kessler_restless\n    ), na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt;\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |&gt;\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\n\n\nInspect your data\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\n\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nInvestigate assumption of positivity:\nRecall the positive assumption:\nPositivity: Can we intervene on the exposure at all levels of the covariates?\nThese are the data by wave, but they don’t track who changed.\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start |&gt;\n  \n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n  \n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n  \n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n\n\nNext consider Māori only\n\n# Maori only\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\nprint(t_tab_m$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n\n\n\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n# creat transition table.\nt_tab_e &lt;- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n# table\nprint(t_tab_e$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#draw-dag",
    "href": "content/v_2_tempate_causal_estimation.html#draw-dag",
    "title": "Template: Causal Estimatatio",
    "section": "Draw Dag",
    "text": "Draw Dag\n\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{shapes.geometric}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations}\n\\tikzstyle{Arrow} = [-&gt;, thin, preaction = {decorate}]\n\\tikzset{&gt;=latex}\n\n\\begin{tikzpicture}[{every node/.append style}=draw]\n\\node [rectangle, draw=white] (U) at (0, 0) {U};\n\\node [rectangle, draw=black, align=left] (L) at (2, 0) {t0/L \\\\t0/A \\\\t0/Y};\n\\node [rectangle, draw=white] (A) at (4, 0) {t1/A};\n\\node [ellipse, draw=white] (Y) at (6, 0) {t2/Y};\n\\draw [-latex, draw=black] (U) to (L);\n\\draw [-latex, draw=black] (L) to (A);\n\\draw [-latex, draw=red, dotted] (A) to (Y);\n\\draw [-latex, bend left=50, draw =black] (L) to (Y);\n\\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);\n\\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);\n\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design\n\n\n\n\n\n\nCreate wide data frame for analysis\nI’ve written a function\n\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\",\n  \"nzsei13\",\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n  \"urban\",\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exclude_vars)\n\n  # Now:\n  data %&gt;% select(all_of(exclude_vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(t0_column_order)\n\n  # Now:\n  data %&gt;% select(all_of(t0_column_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n# ignore warning"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#descriptive-table",
    "href": "content/v_2_tempate_causal_estimation.html#descriptive-table",
    "title": "Template: Causal Estimatatio",
    "section": "Descriptive table",
    "text": "Descriptive table\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n\n\n\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))\n\n\nCalculate propensity scores\nNext we generate propensity scores.\n\n# read  data -- you may start here if you need to repeat the analysis\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_hours_exercise_log_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use ps (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\nResults, first for Europeans\n\n#dt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\n#dt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n#bal.tab(dt_match_ps$euro)   #  not as good\n\n# here we show only the best tab, but you should put all information into an appendix\n\nResults for Maori\n\n# who only Ebal\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n#bal.tab(dt_match_ps$māori)   #  not good\n\n\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2310 |---------------------------| 7.0511\nactive      0.5769  |----|                       1.9603\nvery_active 0.1601 |----------------------|      5.9191\n\n- Units with the 5 most extreme weights by group:\n                                               \n               6560      9   7209   4878   5105\n    inactive 5.1084 5.1312 5.1642 5.3517 7.0511\n               3279   1867   4754   2783   7057\n      active 1.7467 1.7654 1.7701 1.8692 1.9603\n               5977   4293    700   2352   4765\n very_active 5.1495 5.3064 5.4829 5.7273 5.9191\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.743 0.536   0.212       0\nactive            0.270 0.248   0.036       0\nvery_active       0.862 0.637   0.302       0\n\n- Effective Sample Sizes:\n\n           inactive  active very_active\nUnweighted  2880.   3927.       1834.  \nWeighted    1855.89 3659.59     1052.01\n\n# summary maori\nsum_m\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2213  |---------------|            3.8101\nactive      0.3995   |-----|                     1.9800\nvery_active 0.0719 |---------------------------| 6.2941\n\n- Units with the 5 most extreme weights by group:\n                                               \n                296    322    355    758    812\n    inactive 3.0104  3.407 3.6372 3.7101 3.8101\n                 95    783    473    703    699\n      active 1.8319 1.8387 1.9395 1.9436   1.98\n                745    149     78    226    718\n very_active 4.0921 4.3405 4.4111 4.6833 6.2941\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.627 0.475   0.170       0\nactive            0.321 0.264   0.050       0\nvery_active       1.050 0.732   0.411       0\n\n- Effective Sample Sizes:\n\n           inactive active very_active\nUnweighted   307.   354.        160.  \nWeighted     220.54 321.09       76.39\n\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\")\n\n# plot\nlove_plot_e \n\n\n\n\n\n\n\n\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori Weighting: method e-balance\")\n# plot\nlove_plot_m\n\n\n\n\n\n\n\n\n\n\nExample Summary NZ Euro Propensity scores.\n\nWe estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\n\n\nThe effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\n\n\nThe weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n\nWe also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n\nWe plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\n\n\nOverall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n\n\nExample Summary Maori Propensity scores.\nResults:\n\nThe entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\n\n\nThe effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\n\n\nThe weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n\nThe study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n\nIn conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n\n\nMore data wrangling\nNote that we need to attach the weights from the propensity score model back to the data.\nHowever, because our weighting analysis estimates a model for the exposure, we only need to do this analysis once, no matter how many outcomes we investigate. So there’s a little good news.\n\n# prepare nz_euro data\ndt&lt;- readRDS(here::here(\"data\", \"dt\")) # original data subset only nz europeans\n\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n\n# save data for later use, if needed\nsaveRDS(dt_ref_all, here::here(\"data\",\"dt_ref_all\"))"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#anxiety-analysis-and-results",
    "href": "content/v_2_tempate_causal_estimation.html#anxiety-analysis-and-results",
    "title": "Template: Causal Estimatatio",
    "section": "Anxiety Analysis and Results",
    "text": "Anxiety Analysis and Results\nThis is the analysis code\n\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n  # formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nsummary( sim_estimand_all_e )\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(sim_estimand_all_e, here::here(\"data\",\"sim_estimand_all_e\"))\nsaveRDS(sim_estimand_all_m, here::here(\"data\",\"sim_estimand_all_m\"))\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#table-of-subgroup-results-plus-evalues",
    "href": "content/v_2_tempate_causal_estimation.html#table-of-subgroup-results-plus-evalues",
    "title": "Template: Causal Estimatatio",
    "section": "Table of Subgroup Results plus Evalues",
    "text": "Table of Subgroup Results plus Evalues\n\n\n\n\nI’ve created functions for reporting results so you can use this code, changing it to suit your analysis.\n\n# return stored estimates \nsim_estimand_all_e &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e\"))\nsim_estimand_all_m&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m\"))\n\n# create individual summaries \nsum_e &lt;- summary(sim_estimand_all_e)\nsum_m &lt;- summary(sim_estimand_all_m)\n\n\n# create individual tables\ntab_e &lt;- sub_tab_ate(sum_e, new_name = \"NZ Euro Anxiety\")\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 4)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\ntab_m &lt;- sub_tab_ate(sum_m, new_name = \"Māori Anxiety\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n# expand tables \nplot_e &lt;- sub_group_tab(tab_e, type= \"RD\")\nplot_m &lt;- sub_group_tab(tab_m, type= \"RD\")\n\nbig_tab &lt;- rbind(plot_e,plot_m)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab |&gt; \n  kbl(format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nE[Y(1)]-E[Y(0)]\n2.5 %\n97.5 %\nE_Value\nE_Val_bound\nEstimate\nestimate_lab\n\n\n\n\nNZ Euro Anxiety\n-0.077\n-0.131\n-0.022\n1.352\n1.167\nnegative\n-0.077 (-0.131–0.022) [EV 1.352/1.167]\n\n\nMāori Anxiety\n0.027\n-0.114\n0.188\n1.183\n1.000\nunreliable\n0.027 (-0.114-0.188) [EV 1.183/1]"
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#graph-of-the-result",
    "href": "content/v_2_tempate_causal_estimation.html#graph-of-the-result",
    "title": "Template: Causal Estimatatio",
    "section": "Graph of the result",
    "text": "Graph of the result\nI’ve create a function you can use to graph your results. Here is the code, adjust to suit.\n\n# group tables\nsub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\nReport the anxiety result.\n\nFor the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate.\n\n\nE-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect.\n\n\nTurning to the Māori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect.\n\n\nThus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and Māori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture.\n\nHere’s a function that will do much of this work for you. However, you’ll need to adjust it, and supply your own interpretation.\n\n#|label: interpretation function\n#| eval: false\ninterpret_results_subgroup &lt;- function(df, outcome, exposure) {\n  df &lt;- df %&gt;%\n    mutate(\n      report = case_when(\n        E_Val_bound &gt; 1.2 & E_Val_bound &lt; 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\"\n        ),\n        E_Val_bound &gt;= 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\"\n        ),\n        E_Val_bound &lt; 1.2 & E_Val_bound &gt; 1 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"\n        ),\n        E_Val_bound == 1 ~ paste0(\n          \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \",\n          \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n        )\n      )\n    )\n  return(df$report)\n}\n\nYou run the function like this:\n\ninterpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\")\n\n[1] \"For the NZ Euro Anxiety, our results suggest that Excercise may potentially influence Anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077.\\nThe associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate. The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of 1.352 with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of 1.167 to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"                                                                                                                                      \n[2] \"For the Māori Anxiety, the data suggests a potential effect of Excercise on Anxiety, with a causal contrast value of 0.027.\\nHowever, the confidence interval for this estimate, ranging from -0.114 to 0.188, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Anxiety and the Excercise by a risk ratio of 1.183 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Excercise on Anxiety for the Māori Anxiety, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n\n\nEasy!\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make the sumamry into a dataframe so we can make a table\ndf &lt;- as.data.frame(summary(est_all_anxiety))\n\n# get rownames for selecting the correct row\ndf$RowName &lt;- row.names(df)\n\n# select the correct row -- the group contrast\nfiltered_df &lt;- df |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# pring the filtered data frame\nlibrary(kableExtra)\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\n\n\nAnother option for making the table using markdown. This would be useful if you were writing your article using qaurto.\n\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3, format = \"markdown\")\n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\nReport result along the following lines:\n\nThe estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and Māori is reliable. It’s possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for Māori. Thus, while there’s an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution."
  },
  {
    "objectID": "content/v_2_tempate_causal_estimation.html#depression-analysis-and-results",
    "href": "content/v_2_tempate_causal_estimation.html#depression-analysis-and-results",
    "title": "Template: Causal Estimatatio",
    "section": "Depression Analysis and Results",
    "text": "Depression Analysis and Results\n\n### SUBGROUP analysis\ndt_ref_all &lt;- readRDS(here::here(\"data\", \"dt_ref_all\"))\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\"))\nsaveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\"))\n\n\nReport anxiety results\n\n# return stored estimates \nsim_estimand_all_e_d &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\"))\nsim_estimand_all_m_d&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\"))\n\n# create individual summaries \nsum_e_d &lt;- summary(sim_estimand_all_e_d)\nsum_m_d &lt;- summary(sim_estimand_all_m_d)\n\n\n# create individual tables\ntab_ed &lt;- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\ntab_md &lt;- sub_tab_ate(sum_m_d, new_name = \"Māori Depression\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n# expand tables \nplot_ed &lt;- sub_group_tab(tab_ed, type= \"RD\")\nplot_md &lt;- sub_group_tab(tab_md, type= \"RD\")\n\nbig_tab_d &lt;- rbind(plot_ed,plot_md)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab_d |&gt; \n  kbl(format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nE[Y(1)]-E[Y(0)]\n2.5 %\n97.5 %\nE_Value\nE_Val_bound\nEstimate\nestimate_lab\n\n\n\n\nNZ Euro Depression\n-0.039\n-0.099\n0.019\n1.231\n1\nunreliable\n-0.039 (-0.099-0.019) [EV 1.231/1]\n\n\nMāori Depression\n0.028\n-0.125\n0.176\n1.190\n1\nunreliable\n0.028 (-0.125-0.176) [EV 1.19/1]\n\n\n\n\n\n\n\nGraph Anxiety result\n\n# group tables\nsub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\n\nInterpretation\nUse the function, again, modify the outputs to fit with your study and results and provide your own interpretation.\n\ninterpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\")\n\n[1] \"For the NZ Euro Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of -0.039.\\nHowever, the confidence interval for this estimate, ranging from -0.099 to 0.019, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.231 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the NZ Euro Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n[2] \"For the Māori Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of 0.028.\\nHowever, the confidence interval for this estimate, ranging from -0.125 to 0.176, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.19 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the Māori Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"      \n\n\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_d &lt;- readRDS( here::here(\"data\",\"est_all_d\"))\n\n# make the sumamry into a dataframe so we can make a table\ndfd &lt;- as.data.frame(summary(est_all_d))\n\n# get rownames for selecting the correct row\ndfd$RowName &lt;- row.names(dfd)\n\n# select the correct row -- the group contrast\nfiltered_dfd &lt;- dfd |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# Print the filtered data frame\nlibrary(kableExtra)\nfiltered_dfd  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.068\n-0.09\n0.229\n\n\n\n\n\n\n\nReporting might be:\n\nThe estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and Māori is statistically significant. It’s possible that the true difference could be zero or even negative, implying a greater depression reduction for Māori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution.\n\n\n\nDiscusion\nYou’ll need to do this yourself. Here’s a start:\n\nIn our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups – New Zealand Europeans and Māori. This method has the advantage of providing reliable results even if our underlying assumptions aren’t entirely accurate – a likely scenario given the complexity of real-world data. However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn’t mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have.\n\n\nExercise and Anxiety\n\nOur analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to Māori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it’s crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in Māori. Hence, while there’s an indication of a difference, we should interpret it cautiously given the wide range of uncertainty.\n\n\nThus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns.\n\n\n\nExercise and Depression\n\nIn addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise – as self-reported – on depression.\n\n\n\nProviso\n\nIt is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.\\dots nevertheless, \\dots\n\nAgain, you will need to come up with your own discussion, but follow the step-by-step instructions above as a guide."
  },
  {
    "objectID": "slides/01-slides.html#goals",
    "href": "slides/01-slides.html#goals",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Goals",
    "text": "Goals\nBy the end of this seminar you will be able to:\n\nUnderstand the special problems that cultural research presents for measurement.\nUnderstand the deeper, and prior concept of confounding.\n\nKey Concepts\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "slides/01-slides.html#group-discussion-ethics-brainstorm",
    "href": "slides/01-slides.html#group-discussion-ethics-brainstorm",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Group discussion: Ethics brainstorm",
    "text": "Group discussion: Ethics brainstorm\n\nWhat are some ethical considerations that researchers need to take into account when conducting cross-cultural research? How do these considerations differ from those in other types of research?\nHow can cultural differences in values and beliefs impact the ethical considerations in cross-cultural research? What are some examples of cultural differences that researchers need to be aware of?\nHow can researchers ensure that their cross-cultural research is ethical, while still being scientifically rigorous and producing valuable insights? What are some best practices or guidelines that researchers can follow?"
  },
  {
    "objectID": "slides/01-slides.html#where-does-psychology-start",
    "href": "slides/01-slides.html#where-does-psychology-start",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Where Does Psychology Start?",
    "text": "Where Does Psychology Start?\nPsychology starts with a question about how people think or behave."
  },
  {
    "objectID": "slides/01-slides.html#examples-of-psychological-questions",
    "href": "slides/01-slides.html#examples-of-psychological-questions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Examples of Psychological Questions?",
    "text": "Examples of Psychological Questions?\n\nHow does early childhood experience affect personality and behaviour?\nWhat are the effects of social media on self-esteem?\nWhy do some people believe in a God or gods and others do not?\nWhy are some people motivated to sacrifice for others?\nDoes marriage make people happy?"
  },
  {
    "objectID": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "href": "slides/01-slides.html#example-of-psychological-questions-in-cross-cultural-psychology",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Example of Psychological Questions in Cross-Cultural Psychology",
    "text": "Example of Psychological Questions in Cross-Cultural Psychology\n\nHow do early childhood experiences differ across cultures, and how do these differences impact personality and behavior development?\nAre there cultural differences in the way social media use affects self-esteem and body image?\nHow do cultural and religious beliefs shape individual attitudes towards the concept of God or gods?\nWhat are the cultural and individual factors that motivate people to engage in acts of altruism or sacrifice for others, and how do these factors vary across cultures?\nAre there cultural differences in the factors that contribute to marital satisfaction and happiness, and how do cultural expectations and values surrounding marriage play a role?"
  },
  {
    "objectID": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "href": "slides/01-slides.html#how-might-psychological-scientists-answer-whether-marriage-makes-people-happy",
    "title": "Asking questions in cross-cultural psychology",
    "section": "How might Psychological Scientists answer whether marriage makes people happy?",
    "text": "How might Psychological Scientists answer whether marriage makes people happy?"
  },
  {
    "objectID": "slides/01-slides.html#wrong-answers-only",
    "href": "slides/01-slides.html#wrong-answers-only",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Wrong Answers Only",
    "text": "Wrong Answers Only\n\n“Ask my married parents if they are happy.”\n“Consult a palm reader.”\nAssert: “Yes marriage always makes people happy”; “No marriage can’t possibly make anyone happy”\nIntuit: “It depends on the gender of the individual. Men are always happier in marriage, while women are never happier.”\nIntuition: “It depends on the cultural background of the individuals. Couples from Western cultures are always happier in marriage, while couples from Eastern cultures are never happier.”\nConduct a literature review of previous research on the association between marriage and happiness, including cross-cultural studies that compare different cultural attitudes and practices regarding marriage and their relationship to happiness.\nConduct a survey of a large and diverse sample of individuals to assess their happiness levels, as well as their marital status assess the relationship."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-literature-review-of-previous-research",
    "href": "slides/01-slides.html#conduct-a-literature-review-of-previous-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a literature review of previous research",
    "text": "Conduct a literature review of previous research\n\nWhat would this tell us?\n\nwhat other researchers have found.\n\nWhat would this not tell us?\n\nwhat other researchers have not found.\nwhat other researchers got wrong."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "href": "slides/01-slides.html#conduct-a-survey-of-a-large-and-diverse-sample-of-individuals",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a survey of a large and diverse sample of individuals",
    "text": "Conduct a survey of a large and diverse sample of individuals\n\nWhere to begin?"
  },
  {
    "objectID": "slides/01-slides.html#measurement",
    "href": "slides/01-slides.html#measurement",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Measurement",
    "text": "Measurement\nDefinitions:\n\n“Measurement is the numerical quantifcation of the attributes of an object or event, which can be used to compare with other objects or events” [Conventional]\n“Measurement is the assignment of numerals to objects or events according to rules.” [Psychological]\n“Measurement is the process of experimentally obtaining one or more quantity values that can reasonably be attributed to a quantity.” [Metrological] from (Briggs 2021)"
  },
  {
    "objectID": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "href": "slides/01-slides.html#on-a-scale-of-1-7-how-happy-are-you",
    "title": "Asking questions in cross-cultural psychology",
    "section": "“On a scale of 1-7, how happy are you?”",
    "text": "“On a scale of 1-7, how happy are you?”\nHow might this go wrong?\n\nAmbiguity: Respondents may interpret “happiness” in different ways. -e.g. Some people may equate happiness with a momentary positive emotion -e.g. Others may think of happiness as a long-term state of contentment.\nSocial Desirability Bias: Respondents may want to tell you what they think you want to hear. Others might want to frustrate you.\nMood: the context in which the question is asked and the respondent’s current mood might affect the answer (reliability)\nLimited range of scale\nWhat do the endpoints mean?\nCultural Differences:\n\nin meanings\nin constructs themselves"
  },
  {
    "objectID": "slides/01-slides.html#in-survey-research",
    "href": "slides/01-slides.html#in-survey-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "In Survey Research",
    "text": "In Survey Research\nConstruct: e.g. Happiness - a subjective experience of positive emotions, such as joy, contentment, and satisfaction with life/\nItem: A question or statement used to measure an aspect of happiness. Example: “I feel content with my life.”\nScale: A collection of items designed to measure a construct. Example: a happiness scale might consist of items rated on a Likert-type scale, with an overall score reflecting the level of happiness being measured. Example:\n\n“In general, how happy do you feel?” (rated on a scale from 1 - not at all happy to 5 - extremely happy)\n“How often do you feel positive emotions, such as joy or contentment?” (rated on a scale from 1 - very rarely or never to 5 - very often or always)\n“How satisfied are you with your life as a whole?” (rated on a scale from 1 - very dissatisfied to 5 - very satisfied)"
  },
  {
    "objectID": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "href": "slides/01-slides.html#concept-of-validity-in-psychometric-research",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Concept of Validity in Psychometric Research",
    "text": "Concept of Validity in Psychometric Research\n\nContent Validity: The degree an instrument measures what it is intended to measure (the “construct”).\nConstruct Validity: Whether the construct assumed to measure is accurately defined and operationalised.\nCriterion Validity: Whether an instrument accurately predicts performance.\nFace Validity: whether an instrument measure what it is intended to measure, as assessed by experts.\nEcological Validity: whether an instrument accurately reflects real-world situations and behavior (Bandalos 2018)\n\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/01-slides.html#what-can-we-do-about-it",
    "href": "slides/01-slides.html#what-can-we-do-about-it",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What can we do about it?",
    "text": "What can we do about it?\n-Week 10:\n\nMetric equivalence: Factor loadings are similar across groups. Configural equivalence: The factor structure is the same across groups in a multi-group confirmatory factor analysis. Scalar equivalence: Values/Means are also equivalent across groups."
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong",
    "href": "slides/01-slides.html#what-else-might-go-wrong",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nSuppose we are confident in measurement, administer survey, and find an relationship.\n\nDoes marriage cause happiness cross culturally?"
  },
  {
    "objectID": "slides/01-slides.html#what-might-go-wrong",
    "href": "slides/01-slides.html#what-might-go-wrong",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What might go wrong?",
    "text": "What might go wrong?\nHappiness might cause marriage"
  },
  {
    "objectID": "slides/01-slides.html#happiness-might-cause-marriage",
    "href": "slides/01-slides.html#happiness-might-cause-marriage",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Happiness might cause marriage",
    "text": "Happiness might cause marriage"
  },
  {
    "objectID": "slides/01-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "href": "slides/01-slides.html#age-might-be-a-confounder-of-both-marriage-and-happiness",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Age might be a confounder of both Marriage and Happiness",
    "text": "Age might be a confounder of both Marriage and Happiness"
  },
  {
    "objectID": "slides/01-slides.html#many-psychologists-will-simply-control-for-age",
    "href": "slides/01-slides.html#many-psychologists-will-simply-control-for-age",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Many psychologists will simply “control for age”",
    "text": "Many psychologists will simply “control for age”\n\n\nThis doesn’t work if there is reverse causation:\nRegression coefficient for Marriage (M) is biased.\n\n\n\n\nCode\n## Simulate data \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 1000\n  A &lt;- rnorm(n, 1) # simulates age,\n  H &lt;- rnorm(n , A) #  simulates happy as function of age\n  M &lt;- rnorm(n , A) + .2 * H # simulate marriage as a function of age + happiness\n\n  \n# Simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  A = A, \n  H = H,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + A, data = simdat_B)\nsim_B  # returns output\n}\n\n# Replication 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results\nparameters::pool_parameters(r_lm_B)\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | Statistic |     df |      p\n------------------------------------------------------------------------------\n(Intercept) |    4.12e-03 | 0.06 | [-0.12, 0.13] |      0.07 | 220.99 | 0.947 \nM           |        0.19 | 0.04 | [ 0.10, 0.27] |      4.42 | 234.19 | &lt; .001\nA           |        0.77 | 0.07 | [ 0.64, 0.91] |     11.17 | 211.13 | &lt; .001"
  },
  {
    "objectID": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "href": "slides/01-slides.html#the-problem-is-confounding-what-can-we-do-about-it",
    "title": "Asking questions in cross-cultural psychology",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\n\nWeeks 2-4: Causal Diagrams"
  },
  {
    "objectID": "slides/01-slides.html#course-conclusions",
    "href": "slides/01-slides.html#course-conclusions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Course conclusions",
    "text": "Course conclusions\nWeek 11 Ethics and Ownership (Joseph Bulbulia & Guests). Can causal inference help us to improve ethical reasoning? Why is data protection important? How do the ethics of data protection relate to the ethics of open science?\nWeek 12 Future Horizons (Joseph Bulbulia, Torven, Inkuk):"
  },
  {
    "objectID": "slides/01-slides.html#what-have-we-learned",
    "href": "slides/01-slides.html#what-have-we-learned",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What have we learned?",
    "text": "What have we learned?\n\n\nMeasurement: attaching magnitudes to features of the world.\nValidity: obtaining accurate scientific inference: a complex set of interrelated problems.\nConfounding; association in data is spurious"
  },
  {
    "objectID": "slides/01-slides.html#references",
    "href": "slides/01-slides.html#references",
    "title": "Asking questions in cross-cultural psychology",
    "section": "References",
    "text": "References\nFor an account of the history of measurement in psychological research, see: (Briggs 2021)\nFor an account of key concepts and current debates in psychometrics, see: (Bandalos 2018)\n\nFor an accessible introduction to causal inference and its history see: (Pearl and Mackenzie 2018)\nBibliography\n\n\n\n\n\n\n\n\nBandalos, Deborah L. 2018. Measurement Theory and Applications for the Social Sciences. Guilford Publications.\n\n\nBriggs, Derek C. 2021. Historical and Conceptual Foundations of Measurement in the Human Sciences: Credos and Controversies. Routledge.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic books.\n\n\nVanderWeele, Tyler J. 2022. “Constructed Measures and Causal Inference: Towards a New Model of Measurement for Psychosocial Constructs.” Epidemiology 33 (1): 141. https://doi.org/10.1097/EDE.0000000000001434."
  },
  {
    "objectID": "slides/01-slides.html",
    "href": "slides/01-slides.html",
    "title": "Asking questions in cross-cultural psychology",
    "section": "",
    "text": "Understand the special problems that cultural research presents for measurement.\nUnderstand the deeper, and prior concept of confounding.\n\n\n\n\n\nMeasurement\nValidity\nA confounder"
  },
  {
    "objectID": "content/test.html",
    "href": "content/test.html",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "",
    "text": "Recall that psychology begings with a question about cognition and behavior. What do we want to know? Before all else, we must ask, and motivate this question.\nSuppose we have defined a question. How can we address it using observational data?\nThis is the topic of today’s siminar."
  },
  {
    "objectID": "content/test.html#overview",
    "href": "content/test.html#overview",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "",
    "text": "Recall that psychology begings with a question about cognition and behavior. What do we want to know? Before all else, we must ask, and motivate this question.\nSuppose we have defined a question. How can we address it using observational data?\nThis is the topic of today’s siminar."
  },
  {
    "objectID": "content/test.html#how-do-we-do-causal-estimation",
    "href": "content/test.html#how-do-we-do-causal-estimation",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "How do we do causal estimation?",
    "text": "How do we do causal estimation?\nThere are two steps to causal estimation:\n\nState a causal question\nAnswer that question [Cite Hernan]"
  },
  {
    "objectID": "content/test.html#step-1-state-a-causal-question",
    "href": "content/test.html#step-1-state-a-causal-question",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "Step 1: State a causal question",
    "text": "Step 1: State a causal question\nStating a causal question requires describing: a. outcome(s), b. exposure, c. measured confounders, d. (suspected) unmeasured confounders, e. scale of causal contrasts, f. target population for whom the inferences apply.\nWe consider each of these processes in turn.\n\na. Identify the outcome(s) of interest\nWe use Y to denote an outcome of interest. This is the “effect” of interest.\n\nConsider:\n\nThe outcome might be binary (severely distressed/not severely distressed), continuous (the average of the sum of the indicators), or a rate variable (the sum of the indicators).\nIt is crucial to specify the units in which the outcome is measured.\nTransforming the outcome into standard deviation units can be beneficial.\nThe outcome must occur after the exposure or treatment.\nWe must designate a time period within which the outcome occurs, e.g, “the one-year effect of a treatment on well-being as measured by Kessler-6.”\nWe may be interested in multiple outcomes. This is the rational behind outcomewide science [cite tyler]\nWe must remember that the outcome might be measured with error, that such errors may be affected by the treatment or correlated with the measurement error of the treatment. (A topic of future seminars, which we will set to the side for now)\n\nHere, imagine we are interested in understanding only one outcome in our study: well-being as measured by the Kessler-6 depression/anxiety scale.\n\n\n\nb. Define the exposure or intervention\nWe use A to denote the the exposure or treatment. This is the variable that we hypothesise might affect the outcome. This variable denotes the cause of interest. We will restrict our focus to consider in which there is only one treatment (i.e. we will not consider complex multi-treatment regimes.)\nHere, imagine we are interested in understanding the causal effect of ‘Church attendance’.\n\nConsider:\n\nTo affect Y, A must occur before Y.\nWhat does A indicate? We may characterise ‘Church Attendance’ as a binary exposure (attend/not attend) or as a continuous exposure (attend weekly/attend monthly). The exposure, in this instance, remains nebulous until we delineate the unit change in A we are interested in investigating.\nFor theoretical and practical purpose we might want to trunctate the variable into categories: (none/some; none/less than weekly, weekly or more)… Why? Because our causal question requires stating the contrast between the states of the world in which we are interested. Additionally, experts often make decisions on the basis of discrete thresholds (has risk of depression/does not).\nWe must remember that an exposure or treatment might be measured with error, that such errors might be correlated with the measurement error of the outcome or affect it. (Again, a topic of future seminars, which we will set to the side for now).\n\n\n\n\nc. Identify pre-exposure covariates for confounding control\nWe use L to denote the set of measured baseline confounders of the the exposure-outcome association. For a three-wave panel design, we employ VanderWeele’s modified disjunctive cause criterion, we advises: control for any covariate that is a cause of the exposure, the outcome, or both, excluding any instrumental variable and incorporating any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome.\n\nConsider:\n\nTo affect Y, L must occur before Y.\n\nTo affect A, L must occur before A.\nAlthough we may gain precision by including an L that affects Y but is unrelated to A, including variables that occur after A will be hazardous if it is possible that A affects L (or its measurement).\nA useful set of default counfounders in NZAVS studies is given in your workbooks.\nNote we have left out the concept of “selection bias.” There are indeed sources of bias that may occure after A. This is another topic for the week ahead.\n\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See ?@fig-dag-outcomewide for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar.\n\n\n\n\nChoose the scale for a causal contrast\nAverage causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is expressed:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE is expressed:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) :\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\nAnother common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\nwhere f is a function that incorporates weights W into the estimation of the expected outcomes. These weights may correspond to the inverse probability of being sampled or in the case of NZAVS data, the survey weights are given from census estimates for the wider population. Note: I will show you how to use weights in future seminars.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Denote a stratum of interest by S. We may then compute:\nATE_{S,\\text{risk difference}} = E[Y(1) - Y(0)|S, L]\nATE_{S,\\text{risk ratio}} = \\frac{E[Y(1)|S, L]}{E[Y(0)|S, L]}\n\nConsider:\n\n** In this course, we are interested in stratum specific comparisons **\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs from that of intereaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise.\nThe potential outcomes literature in causal inference distinguishes between the concepts of generalisability and transportability.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\\text{Generalizability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/test.html#step-2-answer-a-causal-question",
    "href": "content/test.html#step-2-answer-a-causal-question",
    "title": "Causal Inference: reconsidering measurement and selection biases",
    "section": "STEP 2: ANSWER A CAUSAL QUESTION",
    "text": "STEP 2: ANSWER A CAUSAL QUESTION\n\nObtain longitudinal data\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\nRecall we have defined.\n\nA: Our exposure or treatment variable, denoted as A. Here we consider the example of ‘Church attendance’.\nY: The outcome variable we are interested in, represented by Y, is psychological distress. We operationalise this variable through the ‘Kessler-6’ distress scale.\nL: The confounding variables, collectively referred to as L, represent factors that can independently influence both A and Y. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\nt \\in T: Let t denote within a multiwave panel study with T measurement intervals.\n\nWhere t/\\text{{exposure}} denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that: t\\text{{baseline}}/\\mathbf{L} &lt; t\\text{{exposure}}/\\text{{A}} &lt; t\\text{{exposure}}/\\mathbf{Y}.\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of \\mathbf{L} preceding A and the latest values of A before Y such that: t\\text{{0}}/\\mathbf{L} &lt; t\\text{{1}}/\\text{{A}} &lt; t\\text{{2}}/\\mathbf{Y}.\n\n\nInclude the measured outcome with baseline covariates\nControlling for previous outcome is crucial because the outcome at baseline is often the most potent confounder of both the exposure post-exposure-outcome association.\n\n\nInclude the measured exposure with baseline covariates\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the “incidence exposure” rather than the “prevalence exposure” [Hernan Danaei, Tavakkoli and Hernán, 2012, Hernán, 2015]. It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to dismiss the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\n\nState the eligibility criteria for participation\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\nConsider:\n\nGeneralisability: we cannot evaluate inferences to a target group from the source popuation if we do not describe the source population\nEligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country’s electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\n(Aside: discuss per protocol effects/ intention to treat effects)\n\n\n\n\nHandling of missing data\n\nAs we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n\n\nState a statistical model\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n:\n\nG-computation for Subgroup Analysis Algorithm\nStep 1 Estimate the outcome model. Fit a model for the outcome Y, conditional on both the exposure A, the covariates L, and subgroup indicator G. This model can be linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.\n E(Y|A,L,G) = f_Y(A,L,G; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, and subgroup G as modeled by the function f_Y with parameters \\theta_Y. This formulation allows for the prediction of the average outcome Y given certain values of A, L, and G.\nStep 2 Simulate potential outcomes. For each individual i in each subgroup g, predict their potential outcome \\hat{Y}_{i,g}(a) under the intervention A=a, and \\hat{Y}_{i,g}(a^{\\prime}) under the intervention A=a^{\\prime}, using the estimated outcome model. This is done by replacing the observed value of the exposure A in the outcome model with the counterfactual exposure values a and a^{\\prime}, while keeping the values of the covariates L and subgroup G unchanged:\n[_{i,g}(a) = E[Y|A=a,L_i,G=g; _Y]]\n[_{i,g}(a^{}) = E[Y|A=a^{},L_i,G=g; _Y]]\nThis step involves simulating the outcomes under each intervention level for every individual within each subgroup, as if they were exposed to treatment level a or a^{\\prime}.\nStep 3 Estimate the average causal effect for each subgroup. Compute the expected value of the potential outcomes under each intervention level for each subgroup g:\n[E[Y(a)|G=g] = {G_i=g}^N {i,g}(a)]\n[E[Y(a^{})|G=g] = {G_i=g}^N {i,g}(a^{})]\nand calculate the difference for each subgroup g:\n[_g = E[Y(a)|G=g] - E[Y(a^{})|G=g]]\nThis difference represents the average causal effect of changing the exposure from level a^{\\prime} to level a within subgroup g.\nTo obtain the standard errors and confidence intervals for the subgroup-specific causal effects, simulation-based inference methods, such as the clarify package[@greifer2023], can be used.\n\n\n\nWhat we have not done\nWe have not addressed: - measurement error. Our graphs do not incorporate it. - selection bias - Outcome-wide studies - Sampling weights."
  },
  {
    "objectID": "content/09-content.html",
    "href": "content/09-content.html",
    "title": "Causal inference: a step by step guide",
    "section": "",
    "text": "link to template for option 2 assignment 3"
  },
  {
    "objectID": "content/09-content.html#link-to-workbook-for-this-week",
    "href": "content/09-content.html#link-to-workbook-for-this-week",
    "title": "Causal inference: a step by step guide",
    "section": "",
    "text": "link to template for option 2 assignment 3"
  },
  {
    "objectID": "content/09-content.html#overview",
    "href": "content/09-content.html#overview",
    "title": "Causal inference: a step by step guide",
    "section": "Overview",
    "text": "Overview\nRecall that psychology begins with a question. What do I want to know about thought and behaviour? In cross-cultural psychology, these questions relate to differences, and similarities, between groups.\nSuppose we have asked a question. How can we address it using observational data?\nToo fast.\nOur question must be made precise.\nToday we will consider how to make psychological questions precise, and how to answer them, using 3-wave panel designs (VanderWeele, Mathur, and Chen 2020).\nThe order is as follows:\n\nMotivate Three Wave Longitudinal Designs Using Causal Graphs\nChecklist For Causal Estimation in Three Wave Longitudinal Designs\nExplanation of the the Checklist\n\nLet’s dive in!"
  },
  {
    "objectID": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "href": "content/09-content.html#motivations-for-a-three-wave-longitudinal-design-for-observational-causal-inference.",
    "title": "Causal inference: a step by step guide",
    "section": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.",
    "text": "Motivations for a Three-Wave Longitudinal Design for Observational Causal Inference.\nREVIEW: Causal Diagrammes (DAGS) are a remarkably powerful and simple tool for understanding confounding https://go-bayes.github.io/psych-434-2023/content/common_graphs.html\n\nCommon cause of exposure and outcome.\nOur question: does visiting a clinical psychologist reduce the 10 year incidence of heart attacks?\n\n\n\n\n\n\n\n\nFigure 1: Common cause of exposure and outcome: example\n\n\n\n\n\n\n\nSolution: Adjust for Confounder\n\n\n\n\n\n\n\n\nFigure 2: Solution to this problem.\n\n\n\n\n\n\n\nBias: exposure at baseline is a common cause of the exposure at t1 and outcome at t2\n\n\n\n\n\n\n\n\nFigure 3: Causal graph reveals bias from pre-exosure indicator\n\n\n\n\n\n\n\nSolution: adjust for confounder at baseline\n\n\n\n\n\n\n\n\nFigure 4: Solution to this problem\n\n\n\n\n\n\n\nA more thorough confounding control\n\n\n\n\n\n\n\n\nFigure 5: Causal graph:more general panel design\n\n\n\n\n\n\n\nGeneric 3-wave panel design (VanderWeeele 2020)\n\n\n\n\n\n\n\n\nFigure 6: Causal graph: three-wave panel design"
  },
  {
    "objectID": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2",
    "href": "content/09-content.html#comprehensive-checklist-for-detailed-reporting-of-a-causal-inferenctial-study-e.g.-assessment-3-option-2",
    "title": "Causal inference: a step by step guide",
    "section": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study (E.g. Assessment 3 option 2)",
    "text": "Comprehensive Checklist for Detailed Reporting of a Causal Inferenctial Study (E.g. Assessment 3 option 2)"
  },
  {
    "objectID": "content/09-content.html#step-1-formulate-the-research-question",
    "href": "content/09-content.html#step-1-formulate-the-research-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 1 Formulate the Research Question",
    "text": "STEP 1 Formulate the Research Question\n\nStating the Question: Is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nEthical Considerations How might this question affect people? How might not investigating this question affect people?\nCausality of the Question: Is my question causal? If not, refine your question.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nUnderstanding the Framework: Can I explain the potential outcomes framework, individual causal effects, the experimental method to obtain average causal effects, the fundamental assumptions of causal inference, and the estimation of causal effects in observational data? If not, review course materials.\n\n\nData Requirements\n\nType of Data: Are my data experimental? If yes, your project may not fit this course.\nTime-Series Data: Are my data time-series? If not, reconsider your causal question.\nData Waves: Do I have at least three waves of data? If not, beware of confounding control issues.\nData Source: Are my data from the NZAVS simulated data set? If not, consult with me.\n\n\n\nDefining the Outcome\n\nOutcome Variable: Is the outcome variable Y defined? If not, define it.\nMultiple Outcomes: Are there multiple outcomes? If yes, write them down.\nOutcome Relevance: Can I explain how the outcome variable/s relate to my question? If not, clarify.\nOutcome Type: Is my outcome binary and rare? If yes, consider logistic regression. If my outcome is continuous, consider z-transforming it or categorising it (consult an expert).\nOutcome Timing: Does the outcome appear after the exposure? It should.\n\n\n\nDetermining the Exposure\n\nExposure Variable: Is the exposure variable A defined? If not, define it.\nMultiple Exposures: Are there multiple exposures? If yes, reassess; if only one exposure, proceed.\nExposure Relevance: Can I explain how the exposure variable relates to my question? If not, clarify.\nPositivity: Can we intervene on the exposure at all levels of the covariates? We should be able to.\nConsistency: Can I interpret what it means to intervene on the exposure? I should be able to.\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? They should be.\nExposure Type: Is the exposure binary or continuous? If continuous, z-transform it or consider categorising it (consult an expert).\nExposure Timing: Does the exposure appear before the outcome? It should.\n\n\n\nAccounting for Confounders\n\nBaseline Confounders: Have I defined my baseline confounders L? I should have.\nJustification: Can I explain how the baseline confounders could affect both A and Y? I should be able to.\nTiming: Are the baseline confounders measured before the exposure? They should be.\nInclusion: Is the baseline measure of the exposure and the baseline outcome included in the set of baseline confounders? They should be.\nSufficiency: Are the baseline confounders sufficient to ensure balance on the exposure, such that A is independent of Y given L? If not, plan a sensitivity analysis.\nConfounder Type: Are the confounders continuous or binary? If so, consider converting them to z-scores. If they are categorical with three or more levels, do not convert them to z-scores.\n\n\n\nDrawing a Causal Diagram with Unmeasured Confounders\n\nUnmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? If not, expand your understanding.\nCausal Diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding? I should have.\nM-Bias: Have I considered the possibility of M-Bias? If not familiar, we’ll discuss later.\nMeasurement Error: Have I described potential biases from measurement errors? If not, we’ll discuss later.\nTemporal Order: Does my DAG have time indicators to ensure correct temporal order? It should.\nTime Consistency: Is my DAG organized so that time follows in a consistent direction? It should.\n\n\n\nIdentifying the Estimand\n\nCausal Estimand: Is my causal estimand one of the following:\n\nATE_{G,(A,A')} = E[Y(1) - Y(0)|G, L]\nATE_{G,(A/A')} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\nIf yes, you’re on the right track.\n\n\nUnderstanding Source and Target Populations\n\nPopulations Identified: Have I differentiated between my source and target populations? I should have.\nGeneralisability and Transportability: Have I considered whether my results generalise to the source population and transport to a different population? I should have.\n\n\n\nSetting Eligibility Criteria\n\nCriteria Stated: Have I stated the eligibility criteria for the study? I should have.\n\n\n\nDescribing Sample Characteristics\n\nDescriptive Statistics: Have I provided descriptive statistics for demographic information taken at baseline? I should have.\nExposure Change: Have I demonstrated the magnitudes of change in the exposure from baseline to the exposure interval? I should have.\nReferences: Have I included references for more information about the sample? I should have.\n\n\n\nAddressing Missing Data\n\nMissing Data Check: Have I checked for missing data? I should have.\nMissing Data Plan: If there is missing data, have I described how I will address it? I should have.\n\n\n\nSelecting the Model Approach\n\nApproach Decision: Have I decided on using G-computation, IPTW, or Doubly-Robust Estimation? I should have.\nInteraction Inclusion: Have I included the interaction of the exposure and baseline covariates? I should have.\nLarge Data Set: If I have a large data set, should I include the interaction of the exposure, group, and baseline confounders? I should consider it.\nModel Specification: Have I double-checked the model specification? I should.\nOutcome Specifics: If the outcome is rare and binary, have I specified logistic regression? If it’s continuous, have I considered converting it to z-scores?\nSensitivity Analysis: Am I planning a sensitivity analysis using simulation? If yes, describe it (e.g. E-values.)\n\n\n\nd. Highlight unmeasured pre-treatment covariates\nLet U denoted unmeasured pre-treatment covariates that may potentially bias the statistical association between A and Y independently of the measured covariates.\n\nConsider:\n\nTo affect Y and A, U must occur before A.\nIt is useful to draw a causal diagramme to illustrate all potential sources of bias.\nCausal diagrammes are qualitative tools that require specialist expertise. We cannot typically obtain a causal graph from the data.\nA causal diagramme should include only as much information as is required to assess confounding. See Figure 7 for an example.\nBecause we cannot ensure the absence of unmeasured confounders in observational settings, it is vital to conduct sensitivity analyses for the results. For sensitivity analyeses, we use E-values, a topic for a latter seminar.\n\n\n\n\n\n\n\n\n\nFigure 7: Causal graph: three-wave panel design.\n\n\n\n\n\n\n\n\ne. Choose the scale for a causal contrast\nAverage causal effects can be inferred by contrasting the expected outcome when a population is exposed to an exposure level, E[Y(A = a)], with the expected outcome under a different exposure level, E[Y(A=a')].\nFor a binary treatment with levels A=0 and A=1, the Average Treatment Effect (ATE), on the difference scale, is expressed:\nATE_{\\text{risk difference}} = E[Y(1)|L] - E[Y(0)|L]\nOn the risk ratio scale, the ATE is expressed:\nATE_{\\text{risk ratio}} = \\frac{E[Y(1)|L]}{E[Y(0)|L]}\nOther effect scales, such as the incidence rate ratio, incidence rate difference, or hazard ratio, might also be of interest. We can also define the Average Treatment Effect on the Treated (ATT) :\nATT_{\\text{risk difference}} = E[Y(1) - Y(0)|A=1,L]\nATT_{\\text{risk ratio}} = \\frac{E[Y(1)|A=1,L]}{E[Y(0)|A=1, L]}\nAnother common estimand is the Population Average Treatment Effect (PATE), which denotes the effect the treatment would have on the entire population if applied universally to that population. This quantity can be expressed:\nPATE_{\\text{risk difference}} = f(E[Y(1) - Y(0)|L], W)\nPATE_{\\text{risk ratio}} = f\\left(\\frac{E[Y(1)|L]}{E[Y(0)|L]}, W\\right)\nwhere f is a function that incorporates weights W into the estimation of the expected outcomes. These weights are given from census estimates for the wider population. Note: I will show you how to use weights in future seminars.\nWe might also be interested in identifying effects specific to certain strata, such as risk differences or risk ratios, as they are modified by baseline indicators. Denote a stratum of interest by G. We may then compute:\nATE_{G,\\text{risk difference}} = E[Y(1) - Y(0)|G, L]\nATE_{G,\\text{risk ratio}} = \\frac{E[Y(1)|G, L]}{E[Y(0)|G, L]}\n\nConsider:\n\nIn this course, we are interested in stratum specific comparisons\nIn the causal inference literature, the concept we use to make sense of stratum specific comparisons is called “effect modification.”\nBy inferring effects within stratums, we may evaluate whether the effects of different exposures or treatments on some well-defined outcome (measured in some well-defined time-period after the exposure) differ depending on group measurement.\nThe logic of effect modification differs slightly from that of interaction.\n\n\n\nAside: extensions\nFor continuous exposures, we must stipulate the level of contrast for the exposure (e.g. weekly versus monthly church attendance):\nATE_{A,A'} = E[Y(A) - Y(A')| L]\nThis essentially denotes an average treatment effect comparing the outcome under treatment level A to the outcome under treatment level A'.\nLikewise:\nATE_{A/A'} = \\frac{E[Y(A)| L]}{E[Y(A')| L]}\nThis defines the contrast of A and A' on a ratio scale.\n\n\nf. Describe the population(s) for whom the intended study is meant to generalise by distinguishing between source and target populations.\nConsider the following concepts:\n\nSource population: A source population is where we gather our data for a study. We pull our specific sample from this group. It needs to mirror the broader group for our conclusions to be valid and widely applicable.\nTarget population: The target population is the larger group we aim to apply our study’s results to. It could be defined by location, demographics, or specific conditions. The closer the source matches the target in ways that are relevant to our causal questions, the stronger our causal inferences about the target population will be.\n\nGeneralisability refers to the ability to apply the causal effects estimated from a sample to the population it was drawn from. In simpler terms, it deals with the extrapolation of causal knowledge from a sample to the broader population. This concept is also called “external validity”.\n\n\n\\text{Generalisability} = PATE \\approx ATE_{\\text{sample}}\n\nTransportability refers to the ability to extrapolate causal effects learned from a source population to a target population when certain conditions are met. It deals with the transfer of causal knowledge across different settings or populations.\n\n\\text{Transportability} = ATE_{\\text{target}} \\approx f(ATE_{\\text{source}}, T)\nwhere f is a function and T is a function that maps the results from our source population to another population. To achieve transportability, we need information about the source and target populations and an understanding of how the relationships between treatment, outcome, and covariates differ between the populations. Assessing transportability requires scientific knowledge.\n\n\n\nSummary Step 1: Consider how much we need to do when asking a causal question!\nWe discover that asking a causal question is a multifaceted task. It demands careful definition of the outcome, including its timing, the exposure, and covariates. It also requires selecting the appropriate scale for causal contrast, controlling for confounding, and potentially adjusting for sample weights or stratification. Finally, when asking a causal question, we must consider for whom the results apply. Only after following these steps can we then ask: “How may we answer this causal question?”"
  },
  {
    "objectID": "content/09-content.html#step-2-answer-a-causal-question",
    "href": "content/09-content.html#step-2-answer-a-causal-question",
    "title": "Causal inference: a step by step guide",
    "section": "STEP 2: ANSWER A CAUSAL QUESTION",
    "text": "STEP 2: ANSWER A CAUSAL QUESTION\n\nObtain longitudinal data\nNote that causal inference from observational data turns on the appropriate temporal ordering of the key variables involved in the study.\nRecall we have defined.\n\nA: Our exposure or treatment variable, denoted as A. Here we consider the example of ‘Church attendance’.\nY: The outcome variable we are interested in, represented by Y, is psychological distress. We operationalise this variable through the ‘Kessler-6’ distress scale.\nL: The confounding variables, collectively referred to as L, represent factors that can independently influence both A and Y. For example, socio-economic status could be a confounder that impacts both the likelihood of church attendance and the levels of psychological distress.\n\nGiven the importance of temporal ordering, we must now define time:\n\nt \\in T: Let t denote within a multiwave panel study with T measurement intervals.\n\nWhere t/\\text{{exposure}} denotes the measurement interval for the exposure. Longitudinal data collection provides us the ability to establish a causal model such that:\nt_{confounders} &lt; t_{exposure}&lt; t_{outcome}\nTo minimise the posibility of time-varying confounding and obtain the clearest effect estimates, we should acquire the most recent values of \\mathbf{L} preceding A and the latest values of A before Y.\nNote in Figure 7, We use the prefixes “t0, t1, and t2” to denote temporal ordering. We include in the set of baseline confounders the pre-exposure measurement of A and Y. This allows for more substantial confounding control. For unmeasured confounder to affect both the exposure and the outcome, it would need to do so independently of the pre-exposure confounders. Additionally, including the baseline exposure gives us an effect estimate for the incidence exposure, rather than the prevelance of the exposure. This helps us to assess the expected change in the outcome were we to initate a change in the exposure.\n\n\nInclude the measured exposure with baseline covariates\nControlling for prior exposure enables the interpretation of the effect estimate as a change in the exposure in a manner akin to a randomised trial. We propose that the effect estimate with prior control for the exposure estimates the “incidence exposure” rather than the “prevalence exposure” (Danaei, Tavakkoli, and Hernán 2012). It is crucial to estimate the incidence exposure because if the effects of an exposure are harmful in the short term such that these effects are not subsequently measured, a failure to adjust for prior exposure will yield the illusion that the exposure is beneficial. Furthermore, this approach aids in controlling for unmeasured confounding. For such a confounder to explain away the observed exposure-outcome association, it would need to do so independently of the prior level of the exposure and outcome.\n\n\nState the eligibility criteria for participation\nThis step is invaluable for assessing whether we are answering the causal question that we have asked.\n\nConsider:\n\nGeneralisability: we cannot evaluate inferences to a target group from the source population if we do not describe the source population\nEligibility criteria will help us to ensure whether we have correctly evaluated potential measurement bias/error in our instruments.\n\nFor example, the New Zealand Attitudes and Values Study is a National Probability study of New Zealanders. The details provided in the supplementary materials describe how individuals were randomly selected from the country’s electoral roll. From these invitations there was typically less than 15% response rate. How might this process of recruitment affect generalisability and transportability of our results?\n\nAside: discuss per protocol effects/ intention to treat effects\n\n\n\n\nDetermine how missing data will be handled\n\nAs we will consider in the upcoming weeks, loss to follow up and non-response opens sources for bias. We must develop a strategy for handling missing data.\n\n\n\nState a statistical model\nThe models we have considered in this course are G-computation, Inverse Probability of Treatement Weighting, and Doubly-Robust estimation.\n\n\nReporting\nConsider the following ideas about how to report one’s model:\n\nEstimator: Doubly robust where possible.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations.\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process.\nMethod Variations: Report if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nContinuous Exposures: Highlight that for continuous exposures, only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation.\nSubgroup Interaction: Address whether the subgroup was included separately as an interaction in the outcome model, and if the model successfully converged.\nModel Coefficients: Note that the model coefficients should not be interpreted, as they are not meaningful in this context.\nConfidence Intervals and Standard Errors: Describe the methods used to derive confidence intervals and standard errors, noting the use of the ‘clarify’ package in R for simulation based inference.\n\n\n\nExample of how to report a doubly robust method in your report\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\n\nInference\nConsider the following ideas about what to discuss in one’s findings: Consider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/09-content.html#appendix-a-details-of-estimation-approaches",
    "href": "content/09-content.html#appendix-a-details-of-estimation-approaches",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix A: Details of Estimation Approaches",
    "text": "Appendix A: Details of Estimation Approaches\n\nG-computation for Subgroup Analysis Estimator\nStep 1: Estimate the outcome model. Fit a model for the outcome Y, conditional on the exposure A, the covariates L, and subgroup indicator G. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, and subgroups.\n \\hat{E}(Y|A,L,G) = f_Y(A,L,G; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, and subgroup G, as modelled by the function f_Y with parameters \\theta_Y. This formulation allows for the prediction of the average outcome Y given certain values of A, L, and G.\nStep 2: Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention A=a using the estimated outcome model:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g; \\hat{\\theta}_Y]\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g; \\hat{\\theta}_Y]\nIn these equations, Y represents the potential outcome, A is the intervention, L are the covariates, G=g represents the subgroup, and \\theta_Y are the parameters of the outcome model.\nStep 3: Calculate the estimated difference for each subgroup g:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThis difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup g.\nWe use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\nStep 4: Compare differences in causal effects by subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nThis difference \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g'. It measures the difference in effect of the exposure A within subgroup G on the outcome Y.\n1\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nInverse Probability of Treatment Weighting (IPTW) for Subgroup Analysis Estimator\nStep 1: Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nStep 2: Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and subgroup G. This can be represented as:\n \\hat{E}(Y|A, G; V) = f_Y(A, G ; \\theta_Y, V) \nIn this model, f_Y is a function (such as a weighted regression model) with parameters θ_Y.\nStep 3: Simulate potential outcomes. For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; \\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; \\hat{\\theta}_Y, v]\nStep 4: Estimate the average causal effect for each subgroup as the difference in the predicted outcomes:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect within group g.\nStep 5: Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nThis \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g'.\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023).\n\n\nDoubly Robust Estimation for Subgroup Analysis Estimator\nIt appears that the Doubly Robust Estimation explanation for subgroup analysis is already clear and correct, covering all the necessary steps in the process. Nevertheless, there’s a slight confusion in step 4. The difference \\delta_g is not defined within the document. I assume that you intended to write \\hat{\\delta}_g. Here’s the corrected version:\n\n\nDoubly Robust Estimation for Subgroup Analysis Estimator\nDoubly Robust Estimation is a powerful technique that combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\nStep 1 Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A, covariates L, and subgroup G.\n \\hat{E}(Y|A, L, G; V) = f_Y(A, L, G ; \\theta_Y, V) \nStep 3 For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup.\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#appendix-b-g-computation-for-subgroup-analysis-estimator-with-non-additive-effects",
    "href": "content/09-content.html#appendix-b-g-computation-for-subgroup-analysis-estimator-with-non-additive-effects",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix B: G-computation for Subgroup Analysis Estimator with Non-Additive Effects",
    "text": "Appendix B: G-computation for Subgroup Analysis Estimator with Non-Additive Effects\nStep 1: Estimate the outcome model. Fit a model for the outcome Y, conditional on the exposure A, the covariates L, subgroup indicator G, and interactions between A and G. This model can be a linear regression, logistic regression, or another statistical model. The goal is to capture the relationship between the outcome, exposure, confounders, subgroups, and their interactions.\n \\hat{E}(Y|A,L,G,AG) = f_Y(A,L,G,AG; \\theta_Y) \nThis equation represents the expected value of the outcome Y given the exposure A, covariates L, subgroup G, and interaction term AG, as modeled by the function f_Y with parameters \\theta_Y.\nStep 2: Simulate potential outcomes. For each individual in each subgroup, predict their potential outcome under the intervention A=a using the estimated outcome model:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,L,G=g,AG=ag; \\hat{\\theta}_Y]\nWe also predict the potential outcome for everyone in each subgroup under the causal contrast, setting the intervention for everyone in that group to A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',L,G=g,AG=a'g; \\hat{\\theta}_Y]\nStep 3: Calculate the estimated difference for each subgroup g:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nStep 4: Compare differences in causal effects by subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a^{\\prime})|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a^{\\prime})|G=g^{\\prime}]- \\hat{E}[Y(a)|G=g^{\\prime}]\\big)}^{\\hat{\\delta_{g^{\\prime}}}}\nThis difference \\hat{\\gamma} represents the difference in the average causal effects between the subgroups g and g', taking into account the interaction effect of the exposure A and the subgroup G on the outcome Y.\nNote that the interaction term AG (or ag and a'g in the potential outcomes) stands for the interaction between the exposure level and the subgroup. This term is necessary to accommodate the non-additive effects in the model. As before, we must ensure that potential confounders L are sufficient to control for confounding."
  },
  {
    "objectID": "content/09-content.html#appendix-c-doubly-robust-estimation-for-subgroup-analysis-estimator-with-interaction",
    "href": "content/09-content.html#appendix-c-doubly-robust-estimation-for-subgroup-analysis-estimator-with-interaction",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix C: Doubly Robust Estimation for Subgroup Analysis Estimator with Interaction",
    "text": "Appendix C: Doubly Robust Estimation for Subgroup Analysis Estimator with Interaction\nAgain, Doubly Robust Estimation combines the strengths of both the IPTW and G-computation methods. It uses both the propensity score model and the outcome model, which makes it doubly robust: it produces unbiased estimates if either one of the models is correctly specified.\nStep 1 Estimate the propensity score. The propensity score e(L, G) is the conditional probability of the exposure A = 1, given the covariates L and subgroup indicator G. This can be modeled using logistic regression or other suitable methods, depending on the nature of the data and the exposure.\ne = P(A = 1 | L, G) = f_A(L, G; \\theta_A)\nHere, f_A(L, G; \\theta_A) is a function (statistical model) that estimates the probability of the exposure A = 1 given covariates L and subgroup G. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A, covariates L, subgroup G and the interaction between A and G.\n \\hat{E}(Y|A, L, G, AG; V) = f_Y(A, L, G, AG ; \\theta_Y, V) \nStep 3 For each individual in each subgroup, simulate their potential outcome under the hypothetical scenario where everyone in the subgroup is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)|G=g) = \\hat{E}[Y|A=a,G=g, AG=ag; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone in each subgroup is exposed to intervention A=a':\n\\hat{E}(Y(a')|G=g) = \\hat{E}[Y|A=a',G=g, AG=a'g; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for each subgroup. Compute the estimated expected value of the potential outcomes under each intervention level for each subgroup:\n\\hat{\\delta}_g = \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g]\nThe estimated difference \\hat{\\delta}_g represents the average causal effect of changing the exposure from level a' to level a within each subgroup.\nStep 5 Compare differences in causal effects by groups. Compute the differences in the estimated causal effects between different subgroups:\n\\hat{\\gamma} = \\hat{\\delta}_g - \\hat{\\delta}_{g'}\nwhere,\n\\hat{\\gamma} = \\overbrace{\\big( \\hat{E}[Y(a)|G=g] - \\hat{E}[Y(a')|G=g] \\big)}^{\\hat{\\delta_g}} - \\overbrace{\\big(\\hat{E}[Y(a')|G=g']- \\hat{E}[Y(a)|G=g']\\big)}^{\\hat{\\delta_{g'}}}\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#appendix-d-marginal-structural-models-for-estimating-population-average-treatment-effect-with-interaction-doubly-robust",
    "href": "content/09-content.html#appendix-d-marginal-structural-models-for-estimating-population-average-treatment-effect-with-interaction-doubly-robust",
    "title": "Causal inference: a step by step guide",
    "section": "Appendix D: Marginal Structural Models for Estimating Population Average Treatment Effect with Interaction (Doubly Robust)",
    "text": "Appendix D: Marginal Structural Models for Estimating Population Average Treatment Effect with Interaction (Doubly Robust)\nSometimes we will only wish to estimate a marginal effect. In that case.\nStep 1 Estimate the propensity score. The propensity score e(L) is the conditional probability of the exposure A = 1, given the covariates L which contains the subgroup G. This can be modelled using logistic regression or other functions as described in Greifer et al. (2023)\ne = P(A = 1 | L) = f_A(L; \\theta_A)\nHere, f_A(L; \\theta_A) is a function (a statistical model) that estimates the probability of the exposure A = 1 given covariates L. Then, we calculate the weights for each individual, denoted as v, using the estimated propensity score:\n\nv =\n\\begin{cases}\n\\frac{1}{e} & \\text{if } A = 1 \\\\\n\\frac{1}{1-e} & \\text{if } A = 0\n\\end{cases}\n\nStep 2 Fit a weighted outcome model. Using the weights calculated from the estimated propensity scores, fit a model for the outcome Y, conditional on the exposure A and covariates L.\n \\hat{E}(Y|A, L; V) = f_Y(A, L; \\theta_Y, V) \nThis model should include terms for both the main effects of A and L and their interaction AL.\nStep 3 For the entire population, simulate the potential outcome under the hypothetical scenario where everyone is exposed to the intervention A=a regardless of their actual exposure level:\n\\hat{E}(Y(a)) = \\hat{E}[Y|A=a; L,\\hat{\\theta}_Y, v]\nAnd also under the hypothetical scenario where everyone is exposed to intervention A=a':\n\\hat{E}(Y(a')) = \\hat{E}[Y|A=a'; L; \\hat{\\theta}_Y, v]\nStep 4 Estimate the average causal effect for the entire population. Compute the estimated expected value of the potential outcomes under each intervention level for the entire population:\n\\hat{\\delta} = \\hat{E}[Y(a)] - \\hat{E}[Y(a')]\nThe estimated difference \\hat{\\delta} represents the average causal effect of changing the exposure from level a' to level a in the entire population.\nWe again use simulation-based inference methods to compute standard errors and confidence intervals (Greifer et al. 2023)."
  },
  {
    "objectID": "content/09-content.html#footnotes",
    "href": "content/09-content.html#footnotes",
    "title": "Causal inference: a step by step guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA and G on Y might not be additive. We assume that the potential confounders L are sufficient to control for confounding. See Appendix↩︎"
  },
  {
    "objectID": "content/tempate_causal_estimation.html",
    "href": "content/tempate_causal_estimation.html",
    "title": "Template: Causal Estimatatio",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#intoduction",
    "href": "content/tempate_causal_estimation.html#intoduction",
    "title": "Template: Causal Estimatatio",
    "section": "",
    "text": "Answer the following:\n\nState the Question: is my question clearly stated? If not, state it.\nRelevance of the Question: Have I explained its importance? If not, explain.\nSubgroup Analysis: Does my question involve a subgroup (e.g., cultural group)? If not, develop a subgroup analysis question.\nCausality of the Question: Is my question causal? Briefly explain what this means with reference to the potential outcomes framework.\nState how you will use time-series data to address causality.\nDefine your exposure.\nDefine your outcome(s)\nExplain how the the exposure and outcome is relevant to your question.\nDefine your causal estimand (see: lecture 9). Hint: it is ATE_g_risk difference = E[Y(1)-(0)|G,L], where G is your multiple-group indicator and L is your set of baseline confounders."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#methods",
    "href": "content/tempate_causal_estimation.html#methods",
    "title": "Template: Causal Estimatatio",
    "section": "Methods",
    "text": "Methods\n\nConsider any ethical implications.\nExplain the sample. Provide descriptive statistics\nDiscuss inclusion criteria.\nDiscuss how your sample relates to the “source population” (lecture 9.)\nExplain NZAVS measures. State the questions used in the items\nIn your own words describe how the data meet the following assumptions required for causal inference:\nPositivity: Can we intervene on the exposure at all levels of the covariates? Use the code I provided to test whether there is change in the exposure from the baseline in the source population(s)\nConsistency: Can I interpret what it means to intervene on the exposure?\nExchangeability: Are different versions of the exposure conditionally exchangeable given measured baseline confounders? This requires stating baseline confounders and explaining how they may be related to both the exposure and outcome. As part of this, you must explain why the baseline measure of your exposure and outcome are included as potential confounders.\nNote: Unmeasured Confounders: Does previous science suggest the presence of unmeasured confounders? (e.g. childhood exposures that are not measured).\nDraw a causal diagram: Have I drawn a causal diagram (DAG) to highlight both measured and unmeasured sources of confounding?\nMeasurement Error: Have I described potential biases from measurement errors? Return to lecture 11.\nState that you do not have missing data in this synthetic dataset, but that ordinarily missing data would need to be handled.\nState what your estimator will be. Note I’ve given you the following text to modify:\n\n\nThe Doubly Robust Estimation method for Subgroup Analysis Estimator is a sophisticated tool combining features of both IPTW and G-computation methods, providing unbiased estimates if either the propensity score or outcome model is correctly specified. The process involves five main steps:\n\n\nStep 1 involves the estimation of the propensity score, a measure of the conditional probability of exposure given the covariates and the subgroup indicator. This score is calculated using statistical models such as logistic regression, with the model choice depending on the nature of the data and exposure. Weights for each individual are then calculated using this propensity score. These weights depend on the exposure status and are computed differently for exposed and unexposed individuals. The estimation of propensity scores is performed separately within each subgroup stratum.\n\n\nStep 2 focuses on fitting a weighted outcome model, making use of the previously calculated weights from the propensity scores. This model estimates the outcome conditional on exposure, covariates, and subgroup, integrating the weights into the estimation process. Unlike in propensity score model estimation, covariates are included as variables in the outcome model. This inclusion makes the method doubly robust - providing a consistent effect estimate if either the propensity score or the outcome model is correctly specified, thereby reducing the assumption of correct model specification.\n\n\nStep 3 entails the simulation of potential outcomes for each individual in each subgroup. These hypothetical scenarios assume universal exposure to the intervention within each subgroup, regardless of actual exposure levels. The expectation of potential outcomes is calculated for each individual in each subgroup, using individual-specific weights. These scenarios are performed for both the current and alternative interventions.\n\n\nStep 4 is the estimation of the average causal effect for each subgroup, achieved by comparing the computed expected values of potential outcomes under each intervention level. The difference represents the average causal effect of changing the exposure within each subgroup.\n\n\nStep 5 involves comparing differences in causal effects across groups by calculating the differences in the estimated causal effects between different subgroups. Confidence intervals and standard errors for these calculations are determined using simulation-based inference methods (Greifer et al. 2023). This step allows for a comprehensive comparison of the impact of different interventions across various subgroups, while encorporating uncertainty.\n\nAlso see the appendix here\n\nState what E-values are and how you will use them to clarify the risk of unmeasured confounding."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#results",
    "href": "content/tempate_causal_estimation.html#results",
    "title": "Template: Causal Estimatatio",
    "section": "Results",
    "text": "Results\n\nUse the scripts I have provided as a template for your analysis.\nPropensity Score Reporting: Detail the process of propensity score derivation, including the model used and any variable transformations: e.g.: A ~ x1 + x2 + x3 + .... using logistic regression, all continuous predictors were transformed to z-scores\n\nWeightIt Package Utilisation: Explicitly mention the use of the ‘WeightIt’ package in R, including any specific options or parameters used in the propensity score estimation process (Greifer 2023).\nReport if different methods were used to obtain propensity scores, and the reasons behind the choice of methods such as ‘ebal’, ‘energy’, and ‘ps’.\nIf your exposure is continuous only the ‘energy’ option was used for propensity score estimation.\nSubgroup Estimation: Confirm that the propensity scores for subgroups were estimated separately, and discuss how the weights were subsequently combined with the original data.\nCovariate Balance: Include a Love plot to visually represent covariate balance on the exposure both before and after weighting. The script will generate these plots.\nWeighting Algorithm Statistics: Report the statistics for the weighting algorithms as provided by the WeightIt package, including any measures of balance or fit. The script I gave you will generate this information\n\n\nExample:\n\nWe estimated propensity scores by fitting a model for the exposure A as it is predicted by the set of baseline covariates defined by L. Because we are interested in effect modification by group, we fit different propensity score models for within strata of G using the subgroup command of the WeightIt package. Thus the propensity score is the the probability of receiving a value of a treatment (A=a) conditional on the covariates L, and stratum within G. We compared balance using the following methods of weighting: “ebal” or entropy balancing, “energy” or energy balancing, and “ps” or traditional inverse probability of weighting balancing. Of these methods “ebal” performed the best. Table X and Figure Y present the results of the ebalancing method.\n\n\nInterpretation of Propensity Scores: we interpret the proposensity scores as yeilding good balance across the exposure conditions.\nOutcome Regression Model: Clearly report the type of regression model used to estimate outcome model coefficients (e.g., linear regression, Poisson, binomial), and mention if the exposure was interacted with the baseline covariates. Do not report model coefficients as these have no interpretation. Example\n\n\nWe fit a linear model using maximum likelihood estimation with the outcome Y predicted by the exposure A. We interacted the exposure with all baseline confounders L. Continuous baseline confounders were converted to z-scores, whereas categorical exposures were not. Also interacted with all baseline confounders was a term for the subgroup interactoin. This allowed uas to flexibily fit non-linearities for the modification of the effect of the exposure within levels of the levels of the cultural group strata of interest. We note that model coefficients have no interpretation in this context so are not reported. The remaining steps of Doubly-Robust estimation were performed as outlined in the Method section. We calculated confidence intervals and standard errors, using the clarify package in R, which relies on simulation based inference for these quantities of interest (Greifer et al. 2023)\n\n\nReport the causal estimates.\n\nATE contrasts for groups in setting the exposure to for each group in setting level A = a and A = a*\ndifferences between groups in the magnitude of the effects. (ATE_group 1 - ATE_group_2)\n\nReport the E-value: how sensitive are your results to unmeasured confounding? Hint I gave you code that will create a table for you: See here\n\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#discussion",
    "href": "content/tempate_causal_estimation.html#discussion",
    "title": "Template: Causal Estimatatio",
    "section": "Discussion",
    "text": "Discussion\nMake sure to hit these points:\nConsider the following ideas about what to discuss in one’s findings. The order of exposition might be different.\n\nSummary of results: What did you find?\nInterpretation of E-values: Interpret the E-values used for sensitivity analysis. State what they represent in terms of the robustness of the findings to potential unmeasured confounding.\nCausal Effect Interpretation: What is the interest of the effect, if any, if an effect was observed? Interpret the average causal effect of changing the exposure level within each subgroup, and discuss its relevance to the research question.\nComparison of Subgroups: Discuss how differences in causal effect estimates between different subgroups, if observed, or if not observed, contribute to the overall findings of the study.\nUncertainty and Confidence Intervals: Consider the uncertainty around the estimated causal effects, and interpret the confidence intervals to understand the precision of the estimates.\nGeneralisability and Transportability: Reflect on the generalizability of the study results to other contexts or populations. Discuss any factors that might influence the transportability of the causal effects found in the study. (Again see lecture 9.)\nAssumptions and Limitations: Reflect on the assumptions made during the study and identify any limitations in the methodology that could affect the interpretation of results. State that the implications of different intervention levels on potential outcomes are not analysed.\nTheoretical Relevance: How are these findings relevant to existing theories.\nReplication and Future Research: Consider how the study could be replicated or expanded upon in future research, and how the findings contribute to the existing body of knowledge in the field.\nReal-world Implications: Discuss the real-world implications of the findings, and how they could be applied in policy, practice, or further research."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#example-anlaysis-week-10",
    "href": "content/tempate_causal_estimation.html#example-anlaysis-week-10",
    "title": "Template: Causal Estimatatio",
    "section": "Example anlaysis (week 10)",
    "text": "Example anlaysis (week 10)"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#load-libraries",
    "href": "content/tempate_causal_estimation.html#load-libraries",
    "title": "Template: Causal Estimatatio",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# functions\nsource(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\n\n\n# experimental functions (more functions)\nsource(\n  \"https://raw.githubusercontent.com/go-bayes/templates/main/functions/experimental_funs.R\"\n)\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n\n\nImport data\n\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\n\nnzavs_synth &lt;- arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\n\nFind column names\n\n\nTransform indicators\n\nWhat is the effect of exercise as measured by the NZAVS exercise scale on (1) depression and (2) anxiety.\n\n\n# questions are:\n      # kessler_depressed,\n      # # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      # kessler_hopeless,\n      # # During the last 30 days, how often did you feel hopeless?\n      # kessler_nervous,\n      # # During the last 30 days, how often did you feel nervous?\n      # kessler_effort,\n      # # During the last 30 days, how often did you feel that everything was an effort?\n      # kessler_restless,\n      # # During the last 30 days, how often did you feel restless or fidgety ?\n      # kessler_worthless  # During the last 30 days, how often did you feel worthless?\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    # see week 10 appendix 1 for a detailed explanation of how we obtain these 2 x factors\n    kessler_latent_depression = mean(\n      c(kessler_depressed, kessler_hopeless, kessler_effort),\n      na.rm = TRUE\n    ),\n    kessler_latent_anxiety  = mean(c(\n      kessler_effort, kessler_nervous, kessler_restless\n    ), na.rm = TRUE)\n  ) |&gt;\n  ungroup() |&gt;\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\"inactive\",\n                 \"active\",\n                 \"very_active\"),\n      # Define thresholds for categories\n      levels = c(\"(-1,3]\", \"(3,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |&gt;\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\n\n\nInspect your data\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax(dt_start$hours_exercise)\nmin(dt_start$hours_exercise)\n# checks\n\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\nhist(as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nInvestigate assumption of positivity:\nRecall the positive assumption:\nPositivity: Can we intervene on the exposure at all levels of the covariates?\nThese are the data by wave, but they don’t track who changed.\n\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start |&gt;\n  \n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n  \n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n  \n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n\n\nNext consider Māori only\n\n# Maori only\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\nprint(t_tab_m$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n\n\n\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n# creat transition table.\nt_tab_e &lt;- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n# table\nprint(t_tab_e$table)\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#draw-dag",
    "href": "content/tempate_causal_estimation.html#draw-dag",
    "title": "Template: Causal Estimatatio",
    "section": "Draw Dag",
    "text": "Draw Dag\n\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{shapes.geometric}\n\\usetikzlibrary{arrows}\n\\usetikzlibrary{decorations}\n\\tikzstyle{Arrow} = [-&gt;, thin, preaction = {decorate}]\n\\tikzset{&gt;=latex}\n\n\\begin{tikzpicture}[{every node/.append style}=draw]\n\\node [rectangle, draw=white] (U) at (0, 0) {U};\n\\node [rectangle, draw=black, align=left] (L) at (2, 0) {t0/L \\\\t0/A \\\\t0/Y};\n\\node [rectangle, draw=white] (A) at (4, 0) {t1/A};\n\\node [ellipse, draw=white] (Y) at (6, 0) {t2/Y};\n\\draw [-latex, draw=black] (U) to (L);\n\\draw [-latex, draw=black] (L) to (A);\n\\draw [-latex, draw=red, dotted] (A) to (Y);\n\\draw [-latex, bend left=50, draw =black] (L) to (Y);\n\\draw [-latex, bend right=50, draw =black, dotted] (U) to (Y);\n\\draw [-latex, bend left=50, draw =black, dotted] (U) to (A);\n\n\n\\end{tikzpicture}\n\n\n\n\n\n\n\nFigure 1: Causal graph: three-wave panel design\n\n\n\n\n\n\nCreate wide data frame for analysis\nI’ve written a function\n\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# Note again that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\",\n  \"nzsei13\",\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n  \"urban\",\n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exclude_vars)\n\n  # Now:\n  data %&gt;% select(all_of(exclude_vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(t0_column_order)\n\n  # Now:\n  data %&gt;% select(all_of(t0_column_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n# ignore warning"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#descriptive-table",
    "href": "content/tempate_causal_estimation.html#descriptive-table",
    "title": "Template: Causal Estimatatio",
    "section": "Descriptive table",
    "text": "Descriptive table\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\nbaseline_table(dt_prepare, output_format = \"markdown\")\n\n# but it is not very nice. Next up, is a better table\n\n\n\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all( ~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n\nWe need to do some more data wrangling, alas! Data wrangling is the majority of data analysis. The good news is that R makes wrangling relatively straightforward.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))\n\n\nCalculate propensity scores\nNext we generate propensity scores.\n\n# read  data -- you may start here if you need to repeat the analysis\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_hours_exercise_log_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use ps (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\ndt_match_energy &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  #focal = \"high\", # for use with ATT\n  method = \"energy\"\n)\nsaveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\nResults, first for Europeans\n\n#dt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\n#dt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\n#bal.tab(dt_match_energy$euro)   #  good\nbal.tab(dt_match_ebal$euro)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n#bal.tab(dt_match_ps$euro)   #  not as good\n\n# here we show only the best tab, but you should put all information into an appendix\n\nResults for Maori\n\n# who only Ebal\n#bal.tab(dt_match_energy$māori)   #  good\nbal.tab(dt_match_ebal$māori)   #  best\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n#bal.tab(dt_match_ps$māori)   #  not good\n\n\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n# summary euro\nsum_e\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2310 |---------------------------| 7.0511\nactive      0.5769  |----|                       1.9603\nvery_active 0.1601 |----------------------|      5.9191\n\n- Units with the 5 most extreme weights by group:\n                                               \n               6560      9   7209   4878   5105\n    inactive 5.1084 5.1312 5.1642 5.3517 7.0511\n               3279   1867   4754   2783   7057\n      active 1.7467 1.7654 1.7701 1.8692 1.9603\n               5977   4293    700   2352   4765\n very_active 5.1495 5.3064 5.4829 5.7273 5.9191\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.743 0.536   0.212       0\nactive            0.270 0.248   0.036       0\nvery_active       0.862 0.637   0.302       0\n\n- Effective Sample Sizes:\n\n           inactive  active very_active\nUnweighted  2880.   3927.       1834.  \nWeighted    1855.89 3659.59     1052.01\n\n# summary maori\nsum_m\n\n                 Summary of weights\n\n- Weight ranges:\n\n               Min                                  Max\ninactive    0.2213  |---------------|            3.8101\nactive      0.3995   |-----|                     1.9800\nvery_active 0.0719 |---------------------------| 6.2941\n\n- Units with the 5 most extreme weights by group:\n                                               \n                296    322    355    758    812\n    inactive 3.0104  3.407 3.6372 3.7101 3.8101\n                 95    783    473    703    699\n      active 1.8319 1.8387 1.9395 1.9436   1.98\n                745    149     78    226    718\n very_active 4.0921 4.3405 4.4111 4.6833 6.2941\n\n- Weight statistics:\n\n            Coef of Var   MAD Entropy # Zeros\ninactive          0.627 0.475   0.170       0\nactive            0.321 0.264   0.050       0\nvery_active       1.050 0.732   0.411       0\n\n- Effective Sample Sizes:\n\n           inactive active very_active\nUnweighted   307.   354.        160.  \nWeighted     220.54 321.09       76.39\n\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro Weighting: method e-balance\")\n\n# plot\nlove_plot_e \n\n\n\n\n\n\n\n\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori Weighting: method e-balance\")\n# plot\nlove_plot_m\n\n\n\n\n\n\n\n\n\n\nExample Summary NZ Euro Propensity scores.\n\nWe estimated propensity score analysis using entropy balancing, energy balancing and traditional propensity scores. Of these approaches, entropy balancing provided the best balance. The results indicate an excellent balance across all variables, with Max.Diff.Adj values significantly below the target threshold of 0.05 across a range of binary and continuous baseline confounders, including gender, generation cohort, urban_location, exercise hours (coarsened, baseline), education, employment status, depression, anxiety, and various personality traits. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs.\n\n\nThe effective sample sizes were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 2880, 3927, and 1834, respectively. After adjustment, the effective sample sizes were reduced to 1855.89, 3659.59, and 1052.01, respectively.\n\n\nThe weight ranges for the inactive, active, and very active groups varied, with the inactive group showing the widest range (0.2310 to 7.0511) and the active group showing the narrowest range (0.5769 to 1.9603). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n\nWe also identified the units with the five most extreme weights by group. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n\nWe plotted these results using love plots, visually confirming both the balance in the propensity score model using entropy balanced weights, and the imbalance in the model that does not adjust for baseline confounders.\n\n\nOverall, these findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, conditional on the measured covariates included in the model.\n\n\n\nExample Summary Maori Propensity scores.\nResults:\n\nThe entropy balancing method was also the best performing method that was applied to a subgroup analysis of the Māori population. Similar to the NZ European subgroup analysis, the method achieved a high level of balance across all treatment pairs for the Māori subgroup. The Max.Diff.Adj values for all variables were well below the target threshold of 0.05, with most variables achieving a Max.Diff.Adj of 0.0001 or lower. This indicates a high level of balance across all treatment pairs for the Māori subgroup.\n\n\nThe effective sample sizes for the Māori subgroup were also adjusted using entropy balancing. The unadjusted sample sizes for the inactive, active, and very active groups were 307, 354, and 160, respectively. After adjustment, the effective sample sizes were reduced to 220.54, 321.09, and 76.39, respectively\n\n\nThe weight ranges for the inactive, active, and very active groups in the Māori subgroup varied, with the inactive group showing the widest range (0.2213 to 3.8101) and the active group showing the narrowest range (0.3995 to 1.9800). Despite these variations, the coefficient of variation, mean absolute deviation (MAD), and entropy were all within acceptable limits for each group, indicating a good balance of weights.\n\n\nThe study also identified the units with the five most extreme weights by group for the Māori subgroup. These units exhibited higher weights compared to the rest of the units in their respective groups, but they did not significantly affect the overall balance of weights.\n\n\nIn conclusion, the results of the Māori subgroup analysis are consistent with the overall analysis. The entropy balancing method achieved a high level of balance across all treatment pairs, with Max.Diff.Adj values significantly below the target threshold. These findings support the use of entropy balancing in propensity score analysis to ensure a balanced distribution of covariates across treatment groups, even in subgroup analyses.\n\n\n\nMore data wrangling\nNote that we need to attach the weights from the propensity score model back to the data.\nHowever, because our weighting analysis estimates a model for the exposure, we only need to do this analysis once, no matter how many outcomes we investigate. So there’s a little good news.\n\n# prepare nz_euro data\ndt&lt;- readRDS(here::here(\"data\", \"dt\")) # original data subset only nz europeans\n\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\n# add weights\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare maori data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\n\n# add weights\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n\n# save data for later use, if needed\nsaveRDS(dt_ref_all, here::here(\"data\",\"dt_ref_all\"))"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#anxiety-analysis-and-results",
    "href": "content/tempate_causal_estimation.html#anxiety-analysis-and-results",
    "title": "Template: Causal Estimatatio",
    "section": "Anxiety Analysis and Results",
    "text": "Anxiety Analysis and Results\nThis is the analysis code\n\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n  # formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nsummary( sim_estimand_all_e )\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(sim_estimand_all_e, here::here(\"data\",\"sim_estimand_all_e\"))\nsaveRDS(sim_estimand_all_m, here::here(\"data\",\"sim_estimand_all_m\"))\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#table-of-subgroup-results-plus-evalues",
    "href": "content/tempate_causal_estimation.html#table-of-subgroup-results-plus-evalues",
    "title": "Template: Causal Estimatatio",
    "section": "Table of Subgroup Results plus Evalues",
    "text": "Table of Subgroup Results plus Evalues\n\n\n\n\nI’ve created functions for reporting results so you can use this code, changing it to suit your analysis.\n\n# return stored estimates \nsim_estimand_all_e &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e\"))\nsim_estimand_all_m&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m\"))\n\n# create individual summaries \nsum_e &lt;- summary(sim_estimand_all_e)\nsum_m &lt;- summary(sim_estimand_all_m)\n\n\n# create individual tables\ntab_e &lt;- sub_tab_ate(sum_e, new_name = \"NZ Euro Anxiety\")\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 4)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\ntab_m &lt;- sub_tab_ate(sum_m, new_name = \"Māori Anxiety\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n# expand tables \nplot_e &lt;- sub_group_tab(tab_e, type= \"RD\")\nplot_m &lt;- sub_group_tab(tab_m, type= \"RD\")\n\nbig_tab &lt;- rbind(plot_e,plot_m)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab |&gt; \n  kbl(format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nE[Y(1)]-E[Y(0)]\n2.5 %\n97.5 %\nE_Value\nE_Val_bound\nEstimate\nestimate_lab\n\n\n\n\nNZ Euro Anxiety\n-0.077\n-0.131\n-0.022\n1.352\n1.167\nnegative\n-0.077 (-0.131–0.022) [EV 1.352/1.167]\n\n\nMāori Anxiety\n0.027\n-0.114\n0.188\n1.183\n1.000\nunreliable\n0.027 (-0.114-0.188) [EV 1.183/1]"
  },
  {
    "objectID": "content/tempate_causal_estimation.html#graph-of-the-result",
    "href": "content/tempate_causal_estimation.html#graph-of-the-result",
    "title": "Template: Causal Estimatatio",
    "section": "Graph of the result",
    "text": "Graph of the result\nI’ve create a function you can use to graph your results. Here is the code, adjust to suit.\n\n# group tables\nsub_group_plot_ate(big_tab, title = \"Effect of Exercise on Anxiety\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\nReport the anxiety result.\n\nFor the New Zealand European group, our results suggest that exercise potentially reduces anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077. The associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate.\n\n\nE-values quantify the minimum strength of association that an unmeasured confounding variable would need to have with both the treatment and outcome, to fully explain away our observed effect. In this case, any unmeasured confounder would need to be associated with both exercise and anxiety reduction, with a risk ratio of at least 1.352 to explain away the observed effect, and at least 1.167 to shift the confidence interval to include a null effect.\n\n\nTurning to the Māori group, the data suggest a possible reducing effect of exercise on anxiety, with a causal contrast value of 0.027. Yet, the confidence interval for this estimate (-0.114 to 0.188) also crosses zero, indicating similar uncertainties. An unmeasured confounder would need to have a risk ratio of at least 1.183 with both exercise and anxiety to account for our observed effect, and a risk ratio of at least 1 to render the confidence interval inclusive of a null effect.\n\n\nThus, while our analysis suggests that exercise could potentially reduce anxiety in both New Zealand Europeans and Māori, we advise caution in interpretation. The confidence intervals crossing zero reflect substantial uncertainties, and the possible impact of unmeasured confounding factors further complicates the picture.\n\nHere’s a function that will do much of this work for you. However, you’ll need to adjust it, and supply your own interpretation.\n\n#|label: interpretation function\n#| eval: false\ninterpret_results_subgroup &lt;- function(df, outcome, exposure) {\n  df &lt;- df %&gt;%\n    mutate(\n      report = case_when(\n        E_Val_bound &gt; 1.2 & E_Val_bound &lt; 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests stronger confidence in our findings.\"\n        ),\n        E_Val_bound &gt;= 2 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"With an observed risk ratio of RR = \", E_Value, \", an unmeasured confounder that was associated with both the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each, above and beyond the measured confounders, could explain away the estimate, but weaker joint confounder associations could not; to move the confidence interval to include the null, an unmeasured confounder that was associated with the outcome and the exposure by a risk ratio of \", E_Val_bound, \"-fold each could do so, but weaker joint confounder associations could not. Here we find stronger evidence that the result is robust to unmeasured confounding.\"\n        ),\n        E_Val_bound &lt; 1.2 & E_Val_bound &gt; 1 ~ paste0(\n          \"For the \", group, \", our results suggest that \", exposure, \" may potentially influence \", outcome, \", with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"The associated confidence interval, ranging from \", `2.5 %`, \" to \", `97.5 %`, \", does not cross zero, providing more certainty in our estimate. \",\n          \"The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of \", E_Value, \" with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of \", E_Val_bound, \" to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"\n        ),\n        E_Val_bound == 1 ~ paste0(\n          \"For the \", group, \", the data suggests a potential effect of \", exposure, \" on \", outcome, \", with a causal contrast value of \", `E[Y(1)]-E[Y(0)]`, \".\\n\",\n          \"However, the confidence interval for this estimate, ranging from \", `2.5 %`,\" to \", `97.5 %`, \", crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the \", outcome, \" and the \", exposure, \" by a risk ratio of \", E_Value, \" could explain away the observed associations, even after accounting for the measured confounders. \",\n          \"This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of \", exposure, \" on \", outcome, \" for the \", group, \", the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n        )\n      )\n    )\n  return(df$report)\n}\n\nYou run the function like this:\n\ninterpret_results_subgroup(big_tab, outcome = \"Anxiety\", exposure = \"Excercise\")\n\n[1] \"For the NZ Euro Anxiety, our results suggest that Excercise may potentially influence Anxiety, with an estimated causal contrast value (E[Y(1)]-E[Y(0)]) of -0.077.\\nThe associated confidence interval, ranging from -0.131 to -0.022, does not cross zero, providing more certainty in our estimate. The E-values indicate that any unmeasured confounder would need to have a minimum risk ratio of 1.352 with both the treatment and outcome to explain away the observed effect, and a minimum risk ratio of 1.167 to shift the confidence interval to include the null effect. This suggests we should interpret these findings with caution given uncertainty in the model.\"                                                                                                                                      \n[2] \"For the Māori Anxiety, the data suggests a potential effect of Excercise on Anxiety, with a causal contrast value of 0.027.\\nHowever, the confidence interval for this estimate, ranging from -0.114 to 0.188, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Anxiety and the Excercise by a risk ratio of 1.183 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Excercise on Anxiety for the Māori Anxiety, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n\n\nEasy!\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make the sumamry into a dataframe so we can make a table\ndf &lt;- as.data.frame(summary(est_all_anxiety))\n\n# get rownames for selecting the correct row\ndf$RowName &lt;- row.names(df)\n\n# select the correct row -- the group contrast\nfiltered_df &lt;- df |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# pring the filtered data frame\nlibrary(kableExtra)\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\n\n\nAnother option for making the table using markdown. This would be useful if you were writing your article using qaurto.\n\nfiltered_df  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3, format = \"markdown\")\n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279\n\n\n\n\n\nReport result along the following lines:\n\nThe estimated reduction of anxiety from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is indicated by the estimated risk difference (RD_m - RD_e) of 0.104. However, there is uncertainty in this estimate, as the confidence interval (-0.042 to 0.279) crosses zero. This indicates that we cannot be confident that the difference in anxiety reduction between New Zealand Europeans and Māori is reliable. It’s possible that the true difference could be zero or even negative, suggesting higher anxiety reduction for Māori. Thus, while there’s an indication of higher anxiety reduction for New Zealand Europeans, the uncertainty in the estimate means we should interpret this difference with caution."
  },
  {
    "objectID": "content/tempate_causal_estimation.html#depression-analysis-and-results",
    "href": "content/tempate_causal_estimation.html#depression-analysis-and-results",
    "title": "Template: Causal Estimatatio",
    "section": "Depression Analysis and Results",
    "text": "Depression Analysis and Results\n\n### SUBGROUP analysis\ndt_ref_all &lt;- readRDS(here::here(\"data\", \"dt_ref_all\"))\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = TRUE)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = TRUE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(very_active)]` - `E[Y(inactive)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(sim_estimand_all_m_d, here::here(\"data\", \"sim_estimand_all_m_d\"))\nsaveRDS(sim_estimand_all_e_d, here::here(\"data\", \"sim_estimand_all_e_d\"))\n\n\nReport anxiety results\n\n# return stored estimates \nsim_estimand_all_e_d &lt;- readRDS(here::here(\"data\",\"sim_estimand_all_e_d\"))\nsim_estimand_all_m_d&lt;- readRDS(here::here(\"data\",\"sim_estimand_all_m_d\"))\n\n# create individual summaries \nsum_e_d &lt;- summary(sim_estimand_all_e_d)\nsum_m_d &lt;- summary(sim_estimand_all_m_d)\n\n\n# create individual tables\ntab_ed &lt;- sub_tab_ate(sum_e_d, new_name = \"NZ Euro Depression\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\ntab_md &lt;- sub_tab_ate(sum_m_d, new_name = \"Māori Depression\")\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n# expand tables \nplot_ed &lt;- sub_group_tab(tab_ed, type= \"RD\")\nplot_md &lt;- sub_group_tab(tab_md, type= \"RD\")\n\nbig_tab_d &lt;- rbind(plot_ed,plot_md)\n\n\n# table for anxiety outcome --format as \"markdown\" if you are using quarto documents\nbig_tab_d |&gt; \n  kbl(format=\"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nE[Y(1)]-E[Y(0)]\n2.5 %\n97.5 %\nE_Value\nE_Val_bound\nEstimate\nestimate_lab\n\n\n\n\nNZ Euro Depression\n-0.039\n-0.099\n0.019\n1.231\n1\nunreliable\n-0.039 (-0.099-0.019) [EV 1.231/1]\n\n\nMāori Depression\n0.028\n-0.125\n0.176\n1.190\n1\nunreliable\n0.028 (-0.125-0.176) [EV 1.19/1]\n\n\n\n\n\n\n\nGraph Anxiety result\n\n# group tables\nsub_group_plot_ate(big_tab_d, title = \"Effect of Exercise on Depression\", subtitle = \"Subgroup Analysis: NZ Euro and Māori\", xlab = \"Groups\", ylab = \"Effects\",\n                 x_offset = -1,\n                           x_lim_lo = -1,\n                           x_lim_hi = 1.5)\n\n\n\n\n\n\n\n\n\n\nInterpretation\nUse the function, again, modify the outputs to fit with your study and results and provide your own interpretation.\n\ninterpret_results_subgroup(big_tab_d, exposure = \"Exercise\", outcome = \"Depression\")\n\n[1] \"For the NZ Euro Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of -0.039.\\nHowever, the confidence interval for this estimate, ranging from -0.099 to 0.019, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.231 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the NZ Euro Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"\n[2] \"For the Māori Depression, the data suggests a potential effect of Exercise on Depression, with a causal contrast value of 0.028.\\nHowever, the confidence interval for this estimate, ranging from -0.125 to 0.176, crosses zero, indicating considerable uncertainties. The E-values indicate that an unmeasured confounder that is associated with both the Depression and the Exercise by a risk ratio of 1.19 could explain away the observed associations, even after accounting for the measured confounders. This finding further reduces confidence in a true causal effect. Hence, while the estimates suggest a potential effect of Exercise on Depression for the Māori Depression, the substantial uncertainty and possible influence of unmeasured confounders mean these findings should be interpreted with caution.\"      \n\n\n\n\nEstimate the subgroup contrast\n\n# calculated above\nest_all_d &lt;- readRDS( here::here(\"data\",\"est_all_d\"))\n\n# make the sumamry into a dataframe so we can make a table\ndfd &lt;- as.data.frame(summary(est_all_d))\n\n# get rownames for selecting the correct row\ndfd$RowName &lt;- row.names(dfd)\n\n# select the correct row -- the group contrast\nfiltered_dfd &lt;- dfd |&gt; \n  dplyr::filter(RowName == \"RD_m - RD_e\") \n\n\n# Print the filtered data frame\nlibrary(kableExtra)\nfiltered_dfd  |&gt; \n  select(-RowName) |&gt; \n  kbl(digits = 3) |&gt; \n  kable_material(c(\"striped\", \"hover\")) \n\n\n\n\n\nEstimate\n2.5 %\n97.5 %\n\n\n\n\nRD_m - RD_e\n0.068\n-0.09\n0.229\n\n\n\n\n\n\n\nReporting might be:\n\nThe estimated reduction of depression from exercise is higher overall for New Zealand Europeans (RD_e) compared to Māori (RD_m). This is suggested by the estimated risk difference (RD_m - RD_e) of 0.068. However, there is a degree of uncertainty in this estimate, as the confidence interval (-0.09 to 0.229) crosses zero. This suggests that we cannot be confident that the difference in depression reduction between New Zealand Europeans and Māori is statistically significant. It’s possible that the true difference could be zero or even negative, implying a greater depression reduction for Māori than New Zealand Europeans. Thus, while the results hint at a larger depression reduction for New Zealand Europeans, the uncertainty in this estimate urges us to interpret this difference with caution.\n\n\n\nDiscusion\nYou’ll need to do this yourself. Here’s a start:\n\nIn our study, we employed a robust statistical method that helps us estimate the impact of exercise on reducing anxiety among different population groups – New Zealand Europeans and Māori. This method has the advantage of providing reliable results even if our underlying assumptions aren’t entirely accurate – a likely scenario given the complexity of real-world data. However, this robustness comes with a trade-off: it gives us wider ranges of uncertainty in our estimates. This doesn’t mean the analysis is flawed; rather, it accurately represents our level of certainty given the data we have.\n\n\nExercise and Anxiety\n\nOur analysis suggests that exercise may have a greater effect in reducing anxiety among New Zealand Europeans compared to Māori. This conclusion comes from our primary causal estimate, the risk difference, which is 0.104. However, it’s crucial to consider our uncertainty in this value. We represent this uncertainty as a range, also known as a confidence interval. In this case, the interval ranges from -0.042 to 0.279. What this means is, given our current data and method, the true effect could plausibly be anywhere within this range. While our best estimate shows a higher reduction in anxiety for New Zealand Europeans, the range of plausible values includes zero and even negative values. This implies that the true effect could be no difference between the two groups or even a higher reduction in Māori. Hence, while there’s an indication of a difference, we should interpret it cautiously given the wide range of uncertainty.\n\n\nThus, although our analysis points towards a potential difference in how exercise reduces anxiety among these groups, the level of uncertainty means we should be careful about drawing firm conclusions. More research is needed to further explore these patterns.\n\n\n\nExercise and Depression\n\nIn addition to anxiety, we also examined the effect of exercise on depression. We do not find evidence for reduction of depression from exercise in either group. We do not find evidence for the effect of weekly exercise – as self-reported – on depression.\n\n\n\nProviso\n\nIt is important to bear in mind that statistical results are only one piece of a larger scientific puzzle about the relationship between excercise and well-being. Other pieces include understanding the context, incorporating subject matter knowledge, and considering the implications of the findings. In the present study, wide confidence intervals suggest the possibility of considerable individual differences.\\dots nevertheless, \\dots\n\nAgain, you will need to come up with your own discussion, but follow the step-by-step instructions above as a guide."
  },
  {
    "objectID": "data/readme.html",
    "href": "data/readme.html",
    "title": "Getting started with R",
    "section": "",
    "text": "This is a synthetic data file. The data are simulated from the a sub sample of the New Zealand Attitudes and Values Study (NZAVS).\nThe code to create the synthetic data set, before noise was added to the variables is given below."
  },
  {
    "objectID": "data/readme.html#read-me",
    "href": "data/readme.html#read-me",
    "title": "Getting started with R",
    "section": "",
    "text": "This is a synthetic data file. The data are simulated from the a sub sample of the New Zealand Attitudes and Values Study (NZAVS).\nThe code to create the synthetic data set, before noise was added to the variables is given below."
  },
  {
    "objectID": "data/readme.html#where-can-i-go-for-further-information",
    "href": "data/readme.html#where-can-i-go-for-further-information",
    "title": "Getting started with R",
    "section": "Where can I go for further information?",
    "text": "Where can I go for further information?\nThe NZAVS website is maintained by Prof Chris Sibley. Here is a Link"
  },
  {
    "objectID": "data/readme.html#link-to-data-dictionary-and-questions",
    "href": "data/readme.html#link-to-data-dictionary-and-questions",
    "title": "Getting started with R",
    "section": "Link to data dictionary and questions",
    "text": "Link to data dictionary and questions\nLink to full data dictionary\nLink to questions only\n\ndat_long &lt;- dat |&gt;\n  arrange(id, wave) |&gt;\n  # select variables\n  rename(religion_religious = religious) |&gt;\n  select(\n    \"wave\",\n    \"year_measured\",\n    \"sample_frame\",# see NZAVS\n    \"id\",\n    \"edu\",\n    # Ordinal-Rank 0-10 NZREG codes (with overseas school quals coded as Level 3, and all other ancillary categories coded as missing) See:https://www.nzqa.govt.nz/assets/Studying-in-NZ/New-Zealand-Qualification-Framework/requirements-nzqf.pdf\n    \"male\",\n    # 0 = female, 0.5 = neither female nor male, 1 = male.\n    \"born_nz\",\n    # value label 0    No 1   Yes\n    \"eth_cat\",\n    #factor(EthCat, labels = c(\"Euro\", \"Maori\", \"Pacific\", \"Asian\")),\n    \"employed\",\n    # Are you currently employed? (this includes self-employment or casual work)\n    \"gen_cohort\",\n    #What is your gender? (open-ended)\n    \"household_inc\",\n    # Please estimate your total household income (before tax) for the last year.\n    \"nz_dep2018\",\n    # see nzavs materials\n    \"nzsei13\",\n    # see nzavs materials\n    \"partner\",\n    # 0 = no, 1 = yes\n    \"parent\",\n    # 0 = no, 1 = yes\n    \"pol_orient\",\n    #Please rate how politically liberal versus conservative you see yourself as being.\n    \"pol_wing\",\n    # Please rate how politically left-wing versus right-wing you see yourself as being.\n    \"rural_gch2018\",\n    # see NZAVS\n    \"agreeableness\",\n    # Mini-IPIP6 Agreeableness (also modelled as empathy facet)\n    # Sympathize with others' feelings.\n    # Am not interested in other people's problems.\n    # Feel others' emotions.\n    # Am not really interested in others.\n    \"conscientiousness\",\n    # see mini ipip6\n    # Get chores done right away.\n    # Like order.\n    # Make a mess of things.\n    # Often forget to put things back in their proper place.\n    \"extraversion\",\n    # Mini-IPIP6 Extraversion\n    # Am the life of the party.\n    # Don't talk a lot.\n    # Keep in the background.\n    # Talk to a lot of different people at parties.\n    \"honesty_humility\",\n    # see mini ipip6\n    # Would like to be seen driving around in a very expensive car.\n    # Would get a lot of pleasure from owning expensive luxury goods.\n    # Feel entitled to more of everything.\n    # Deserve more things in life.\n    \"openness\",\n    # see mini ipip6\n    # Have a vivid imagination.\n    # Have difficulty understanding abstract ideas.\n    # Do not have a good imagination.\n    # Am not interested in abstract ideas.\n    \"neuroticism\",\n    # see mini ipip6\n    # Have frequent mood swings.\n    # Am relaxed most of the time.\n    # Get upset easily.\n    # Seldom feel blue.\n    \"modesty\",\n    # see mini ipip6\n    # I want people to know that I am an important person of high status,\n    # I am an ordinary person who is no better than others.\n    # I wouldn’t want people to treat me as though I were superior to them.\n    # I think that I am entitled to more respect than the average person is.\n    \"religion_religious\",\n    # Do you identify with a religion and/or spiritual group?\n    \"religion_identification_level\",\n    #How important is your religion to how you see yourself?\"\n    # \"religion_church_binary\", # at least 1 time per month = 1, 0.\n    #  \"religion_prayer\", # How many times did you pray in the last week?\n    #  \"religion_scripture\", # How many times did you read religious scripture in the last week?\n    #  \"religion_church2\", # How many times did you attend a church or place of worship in the last month?\n    \"religion_believe_spirit\",\n    #Do you believe in some form of spirit or lifeforce?\n    \"religion_believe_god\",\n    #Do you believe in a God\n    # \"religion_spiritual_identification\", #w8,w10,w12-13 \"I identify as a spiritual person.\"\n    \"religion_perceive_religious_discrim\",\n    #   I feel that I am often discriminated against because of my religious/spiritual beliefs.\n    \"bigger_doms\",\n    #What religion or spiritual group?#  Not_Rel, Anglican , Buddist, Catholic , Christian_nfd, Christian_Others, Hindu, Jewish           Muslim, PresbyCongReform, TheOthers\n    \"w_gend_age_euro\",\n    # sample_weights\n    \"alcohol_frequency\",\n    #\"How often do you have a drink containing alcohol?\"\n    \"alcohol_intensity\",\n    # How many drinks containing alcohol do you have on a typical day when drinking?\n    \"hlth_bmi\",\n    # \" What is your height? (metres)\\nWhat is your weight? (kg)\\nKg\n    \"hours_exercise\",\n    # Hours spent … exercising/physical activity\n    # \"sfhealth\",\n    \"sfhealth_your_health\",\n    # \"In general, would you say your health is...\n    \"sfhealth_get_sick_easier\",\n    #\\nI seem to get sick a little easier than other people.\n    \"sfhealth_expect_worse_health\",\n    #\\nI expect my health to get worse.\" ****\n    \"hlth_sleep_hours\",\n    #During the past month, on average, how many hours of actual sleep did you get per night?\n    \"smoker\",\n    #Do you currently smoke?\n    \"hlth_fatigue\",\n    #During the last 30 days, how often did.... you feel exhausted?\n    \"rumination\",\n    # During the last 30 days, how often did.... you have negative thoughts that repeated over and over?\n    \"kessler_depressed\",\n    #During the last 30 days, how often did.... you feel so depressed that nothing could cheer you up?\n    \"kessler_effort\",\n    #During the last 30 days, how often did.... you feel that everything was an effort?\n    \"kessler_hopeless\",\n    # During the last 30 days, how often did.... you feel hopeless?\n    \"kessler_nervous\",\n    #During the last 30 days, how often did.... you feel nervous?\n    \"kessler_restless\",\n    #During the last 30 days, how often did.... you feel restless or fidgety?\n    \"kessler_worthless\",\n    # During the last 30 days, how often did.... you feel worthless?\n    \"sexual_satisfaction\",\n    #  How satisfied are you with your sex life?\n    \"bodysat\",\n    ## Am satisfied with the appearance, size and shape of my body.\n    \"vengeful_rumin\",\n    # Sometimes I can't sleep because of thinking about past wrongs I have suffered.//# I can usually forgive and forget when someone does me wrong.# I find myself regularly thinking about past times that I have been wronged.\n    \"perfectionism\",\n    # # Doing my best never seems to be enough./# My performance rarely measures up to my standards.\n    # I am hardly ever satisfied with my performance.\n    \"power_self_nocontrol\",\n    # I do not have enough power or control over\\nimportant parts of my life.\n    \"power_others_control\",\n    # Other people have too much power or control over\\nimportant parts of my life\n    \"selfesteem_satself\",\n    #  On the whole am satisfied with myself.\n    \"selfesteem_postiveself\",\n    # Take a positive attitude toward myself\n    \"selfesteem_rfailure\",\n    # Am inclined to feel that I am a failure. (r)\n    \"self_control_have_lots\",\n    #In general, I have a lot of self-control.\n    \"self_control_wish_more_r\",\n    #I wish I had more self-discipline.(r)\n    \"emotion_regulation_out_control\",\n    # When I feel negative emotions, my emotions feel out of control. w10 - w13\n    \"emotion_regulation_hide_neg_emotions\",\n    # When I feel negative emotions, I suppress or hide my emotions. w10 - w13\n    \"emotion_regulation_change_thinking_to_calm\",\n    # When I feel negative emotions, I change the way I think to help me stay calm. w10 - w13\n    #  \"emp_work_life_balance\",# I have a good balance between work and other important things in my life.\n    \"gratitude\",\n    ## I have much in my life to be thankful for. # When I look at the world, I don’t see much to be grateful for. # I am grateful to a wide variety of people.\n    \"pwi_health\",\n    #Your health.\n    \"pwi_relationships\",\n    #Your personal relationships.\n    \"pwi_security\",\n    #Your future security.\n    \"pwi_standardliving\",\n    #Your standard of living.\n    \"lifesat_satlife\",\n    # I am satisfied with my life.\n    \"lifesat_ideal\",\n    # In most ways my life is close to ideal.\n    \"meaning_purpose\",\n    # My life has a clear sense of purpose.\n    \"meaning_sense\",\n    # I have a good sense of what makes my life meaningful.\n    \"permeability_individual\",\n    #I believe I am capable, as an individual\\nof improving my status in society.\n    \"impermeability_group\",\n    #The current income gap between New Zealand Europeans and other ethnic groups would be very hard to change.\n    \"neighbourhood_community\",\n    #I feel a sense of community with others in my local neighbourhood.\n    \"support_help\",\n    # 'There are people I can depend on to help me if I really need it.\n    \"support_turnto\",\n    # There is no one I can turn to for guidance in times of stress.\n    \"support_rnoguidance\",\n    #There is no one I can turn to for guidance in times of stress.\n    \"belong_accept\",\n    #Know that people in my life accept and value me.\n    \"belong_routsider\",\n    # Feel like an outsider.\n    \"belong_beliefs\",\n    # Know that people around me share my attitudes and beliefs.\n    \"charity_donate\",\n    #How much money have you donated to charity in the last year?\n    \"hours_charity\",\n    #Hours spent in activities/Hours spent … voluntary/charitable work\n    \"nwi\",\n    # The economic situation in New Zealand./# The social conditions in New Zealand. # Business in New Zealand.\n  ) |&gt;\n  dplyr::rename(sample_weights = w_gend_age_euro) |&gt;\n  dplyr::filter((wave == 2018 & year_measured  == 1) |\n                  (wave == 2019  &\n                     year_measured  == 1) |\n                  (wave == 2020  &  year_measured  == 1)\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(id) |&gt;\n  filter(n_distinct(wave) == 3) |&gt; # not used to avoid missing data -- for teaching purposes only\n  # dplyr::mutate(k_18 =  ifelse(wave == 2018, 1, 0)) |&gt;   # creating an indicator for the first wave\n  # dplyr::mutate(h_18 = mean(k_18, na.rm = TRUE)) |&gt;   # hack\n  # dplyr::mutate(k_19 =  ifelse(wave == 2019 &\n  #                                year_measured == 1, 1, 0)) |&gt;   # creating an indicator for the first wave; note that we allow people t\n  # dplyr::mutate(h_19 = mean(k_19, na.rm = TRUE)) |&gt;  # Hack\n  # dplyr::filter(h_18 &gt; 0) |&gt;  # hack to enable repeat of baseline\n  # dplyr::filter(h_19 &gt; 0) |&gt;  # hack to enable repeat of baseline\n  ungroup() |&gt;\n  droplevels() |&gt;\n  drop_na() |&gt;\n  arrange(id, wave)"
  },
  {
    "objectID": "scripts/experiment_template.html",
    "href": "scripts/experiment_template.html",
    "title": "experiment_template.qmd",
    "section": "",
    "text": "Today we will do an analysis, proceeding in a step-by-step way.\nRecall our checklist:"
  },
  {
    "objectID": "scripts/experiment_template.html#questions",
    "href": "scripts/experiment_template.html#questions",
    "title": "experiment_template.qmd",
    "section": "Questions",
    "text": "Questions\nQuestion 1. Does exercise affect anxiety/depression?\nQuestion 2: Do these effects vary among NZ Europeans and Māori?\nAre these questions clearly stated? No, they are vague:\n\nHow much exercise?\nBy which measures of depression?\nWhen should the effects be observed?\n\nRecall we can help to clarify these questions by attempting to emulate an experiment.\nWe shall use the measure of exercise in the NZAVS: hours of activity per week (we shall see, the question remains vague).\nWe will assess the 1-year effect on Kessler-6 depression after initiating a change in exercise (intention-to-treat).\nWe will investigate effect-modification by NZ European and Māori ethnic identification."
  },
  {
    "objectID": "scripts/experiment_template.html#preliminaries-source-functions-import-data.",
    "href": "scripts/experiment_template.html#preliminaries-source-functions-import-data.",
    "title": "experiment_template.qmd",
    "section": "Preliminaries: source functions, import data.",
    "text": "Preliminaries: source functions, import data.\nWe source our functions, load libraries, and important our (synthetic) data.\n\n\nCode\n# PSYCH 434: Example script for assessment 3 and 5.\n\n# Before running this source code, make sure to update to the current version of R, and to update all existing packages.\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/libs2.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/funs.R\")\n\n# WARNING:  COMMENT THIS OUT. JB DOES THIS FOR WORKING WITHOUT WIFI\nsource(\"/Users/joseph/GIT/templates/functions/experimental_funs.R\")\n############## ############## ############## ############## ############## ############## ############## ########\n#########  ############## ############## IMPORT DATA ##############  ############## ############## ##############\n############## ############## ############## ############## ############## ############## ############## ########\n\n\n#  If you haven't already, you should have created a folder called \"data\", in your Rstudio project. If not, download this file, add it to your the folder called \"data\" in your Rstudio project. # \"https://www.dropbox.com/s/vwqijg4ha17hbs1/nzavs_dat_synth_t10_t12?dl=0\"\n\n# A function we will use for our tables. \ntab_ate_subgroup_rd &lt;- function(x,\n                                new_name,\n                                delta = 1,\n                                sd = 1) {\n  # Check if required packages are installed\n  required_packages &lt;- c(\"EValue\", \"dplyr\")\n  new_packages &lt;-\n    required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  require(EValue)\n  require(dplyr)\n  \n  # check if input data is a dataframe\n  if (!is.data.frame(x))\n    stop(\"Input x must be a dataframe\")\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(x))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \",\n         paste(missing_cols, collapse = \", \"))\n  \n  # Check if lower_ci and upper_ci do not contain NA values\n  if (any(is.na(x$lower_ci), is.na(x$upper_ci)))\n    stop(\"Columns 'lower_ci' and 'upper_ci' should not contain NA values\")\n  \n  x &lt;- x %&gt;%\n    dplyr::mutate(across(where(is.numeric), round, digits = 3)) %&gt;%\n    dplyr::rename(\"E[Y(1)]-E[Y(0)]\" = estimate)\n  \n  x$standard_error &lt;- abs(x$lower_ci - x$upper_ci) / 3.92\n  \n  evalues_list &lt;- lapply(seq_len(nrow(x)), function(i) {\n    row_evalue &lt;- EValue::evalues.OLS(\n      x[i, \"E[Y(1)]-E[Y(0)]\"],\n      se = x[i, \"standard_error\"],\n      sd = sd,\n      delta = delta,\n      true = 0\n    )\n    # If E_value is NA, set it to 1\n    if (is.na(row_evalue[2, \"lower\"])) {\n      row_evalue[2, \"lower\"] &lt;- 1\n    }\n    if (is.na(row_evalue[2, \"upper\"])) {\n      row_evalue[2, \"upper\"] &lt;- 1\n    }\n    data.frame(round(as.data.frame(row_evalue)[2,], 3)) # exclude the NA column\n  })\n  \n  evalues_df &lt;- do.call(rbind, evalues_list)\n  colnames(evalues_df) &lt;- c(\"E_Value\", \"E_Val_bound\")\n  \n  tab_p &lt;- cbind(x, evalues_df)\n  \n  tab &lt;-\n    tab_p |&gt; select(c(\n      \"E[Y(1)]-E[Y(0)]\",\n      \"lower_ci\",\n      \"upper_ci\",\n      \"E_Value\",\n      \"E_Val_bound\"\n    ))\n  \n  return(tab)\n}\n\n\n\n\nCode\n# This will read the synthetic data into Rstudio.  Note that the arrow package allows us to have lower memory demands in the storage and retrieval of data.\nnzavs_synth &lt;-\n  arrow::read_parquet(here::here(\"data\", \"nzavs_dat_synth_t10_t12\"))\n\n\nNext, we will inspect column names.\nMake sure to familiarise your self with the variable names here\nIt’s a good idea to plot the data\n\nData Wrangling.\nNext, we’ll get the data into shape.\nConsider the following causal questions: “Does exercise affect well-being?” “Do such effects, if they exist, differ by ethnicity?”\nThese questions are not precise. What type of excercise? How regularly must one exercise? For how long must one exercise? Which ethnicities shall we compare? Why?\nIt helpst to think like an experimentalist… (say more)\nIn this exampe, we’ll do the following:\n\nCreate a Kessler 6 average score\nCreate a Kessler 6 sum score\nCreate a Kessler 6 binary score (Not Depressed vs. Moderately or Severely Depressed)\nCreate a log Exercise score\nCreate a coarsened Exercise score.\n\nConsider: the NZAVS asks participants the following question. During the past week, list “Hours spent exercising/physical activity.” The question is inherently unclear about what sort of physical activity someone is doing. When participants respond to this question, what do they mean? John considers anything that is not sleep to be physical activity. He returns a high number. Jane only counts aerobic exercise. For Jane, walking an hour to work and back doesn’t count.\nRecall that an assumption of causal inference is consistency. (Say more … then leave to the side.)\nFor now, let’s create the indicators.\n\n\nCode\n# create sum score of kessler 6\ndt_start &lt;- nzavs_synth %&gt;%\n  arrange(id, wave) %&gt;%\n  rowwise() %&gt;%\n  mutate(kessler_6  = mean(\n    sum(\n    # Specify the Kessler scale items\n    c(\n      kessler_depressed,\n      # During the last 30 days, how often did you feel so depressed that nothing could cheer you up?\n      kessler_hopeless,\n      # During the last 30 days, how often did you feel hopeless?\n      kessler_nervous,\n      # During the last 30 days, how often did you feel nervous?\n      kessler_effort,\n      # During the last 30 days, how often did you feel that everything was an effort?\n      kessler_restless,\n      # During the last 30 days, how often did you feel restless or fidgety ?\n      kessler_worthless  # During the last 30 days, how often did you feel worthless?\n    )))) |&gt;\n  mutate(kessler_6_sum = round(\n    sum(c (kessler_depressed,\n                   kessler_hopeless,\n                   kessler_nervous,\n                   kessler_effort,\n                   kessler_restless,\n                   kessler_worthless)),\n    digits = 0\n  )) |&gt;  ungroup() |&gt;\n# Create a categorical variable 'kessler_6_coarsen' based on the sum of Kessler scale items\n  mutate(\n    kessler_6_coarsen = cut(\n      kessler_6_sum,\n      breaks = c(0, 5, 24),\n       labels = c(\n        \"not_depressed\",\n        \"mildly_to_severely_depressed\"),\n      include.lowest = TRUE,\n      include.highest = TRUE,\n      na.rm = TRUE,\n      right = FALSE\n    )\n  ) |&gt;\n  # Transform 'hours_exercise' by applying the log function to compress its scale\n  mutate(hours_exercise_log = log(hours_exercise + 1)) |&gt; # Add 1 to avoid undefined log(0). Hours spent exercising/physical activity\n\n  # Coarsen 'hours_exercise' into categories\n  mutate(\n    hours_exercise_coarsen = cut(\n      hours_exercise,\n      # Hours spent exercising/ physical activity\n      breaks = c(-1, 3, 8, 200),\n      labels = c(\n        \"inactive\",\n        \"active\",\n        \"very_active\"      ),\n      # Define thresholds for categories\n      levels = c(\"(-1,2]\", \"(2,8]\", \"(8,200]\"),\n      ordered = TRUE\n    )\n  ) |&gt;\n\n  # Create a binary 'urban' variable based on the 'rural_gch2018' variable\n  mutate(urban = factor(\n    ifelse(\n      rural_gch2018 == \"medium_urban_accessibility\" |\n        # Define urban condition\n        rural_gch2018 == \"high_urban_accessibility\",\n      \"urban\",\n      # Label 'urban' if condition is met\n      \"rural\"  # Label 'rural' if condition is not met\n    )\n  ))\n\n\nWe next do some data checks. I will leave you to do these in your own time.\n\n\nCode\n# do some checks\nlevels(dt_start$hours_exercise_coarsen)\ntable(dt_start$hours_exercise_coarsen)\nmax( dt_start$hours_exercise)\nmin( dt_start$hours_exercise)\n# checks\ntable(is.na(dt_start$kessler_6_coarsen))\ntable(is.na(dt_start$hours_exercise_coarsen))\n\n# justification for transforming exercise\" has a very long tail\nhist(dt_start$hours_exercise, breaks = 1000)\n# consider only those cases below &lt; or = to 20\nhist(subset(dt_start, hours_exercise &lt;= 20)$hours_exercise)\n\n\n# inspect kessler 6\ntable(dt_start$kessler_6_coarsen)\ntable(dt_start$hours_exercise_coarsen)\n\nhist( as.numeric(dt_start$kessler_6_coarsen) )\nhist( as.numeric(dt_start$hours_exercise_coarsen))\n\n\n\n\nCFA for Kessler 6\nWe have learned how to do confirmatory factor analysis. Let’s put this knowledge to use, and consider whether Kessler 6 is one variable.\nThe code below will:\n\nLoad required packages.\nSelect the Kessler 6 items\nCheck whether there is sufficient correlation among the variables to support factor analysis.\n\n\n\nCode\n# Suppose we have reason to think Kessler 6 isn't one thing.\n# Let's put our factor analysis skills to work\n# Here we will use the paramters and see packages for R (part of the Easystats suite)\n\n# for efa/cfa\nif (!require(psych)) {\n  install.packages(\"psych\")\n  library(\"psych\")\n}\n\n\nLoading required package: psych\n\n\nCode\n# for reporting\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(\"parameters\")\n}\n\n# for graphing\nif (!require(see)) {\n  install.packages(\"see\")\n  library(\"see\")\n}\n\n\nLoading required package: see\n\n\nCode\n# for graphing\nif (!require(lavaan)) {\n  install.packages(\"lavaan\")\n  library(\"lavaan\")\n}\n\n\nLoading required package: lavaan\n\n\nThis is lavaan 0.6-16\nlavaan is FREE software! Please report any bugs.\n\n\nCode\n# for graphing\nif (!require(datawizard)) {\n  install.packages(\"datawizard\")\n  library(\"datawizard\")\n}\n\n\nLoading required package: datawizard\n\n\nCode\n# select the columns we need. \ndt_only_k6 &lt;- dt_start |&gt; select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless)\n\n\n# check factor structure\nperformance::check_factorstructure(dt_only_k6)\n\n\n# Is the data suitable for Factor Analysis?\n\n\n  - Sphericity: Bartlett's test of sphericity suggests that there is sufficient significant correlation in the data for factor analysis (Chisq(15) = 70564.23, p &lt; .001).\n  - KMO: The Kaiser, Meyer, Olkin (KMO) overall measure of sampling adequacy suggests that data seems appropriate for factor analysis (KMO = 0.86). The individual KMO scores are: kessler_depressed (0.83), kessler_effort (0.89), kessler_hopeless (0.85), kessler_worthless (0.85), kessler_nervous (0.88), kessler_restless (0.85).\n\n\nThe code below will allow us to explore the factor structure, on the assumption of n = 3 factors.\n\n\nCode\n# exploratory factor analysis\n# explore a factor structure made of 3 latent variables\nefa &lt;- psych::fa(dt_only_k6, nfactors = 3) %&gt;%\n  model_parameters(sort = TRUE, threshold = \"max\")\n\n\nLoading required namespace: GPArotation\n\n\nCode\nefa\n\n\n# Rotated loadings from Factor Analysis (oblimin-rotation)\n\nVariable          | MR1  | MR2  | MR3  | Complexity | Uniqueness\n----------------------------------------------------------------\nkessler_depressed | 0.85 |      |      |    1.01    |    0.33   \nkessler_worthless | 0.79 |      |      |    1.00    |    0.35   \nkessler_hopeless  | 0.75 |      |      |    1.02    |    0.33   \nkessler_nervous   |      | 1.00 |      |    1.00    |  5.00e-03 \nkessler_restless  |      |      | 0.69 |    1.02    |    0.52   \nkessler_effort    |      |      | 0.48 |    1.66    |    0.50   \n\nThe 3 latent factors (oblimin rotation) accounted for 66.05% of the total variance of the original data (MR1 = 35.14%, MR2 = 17.17%, MR3 = 13.73%).\n\n\nCode\n# This output presents the results of an exploratory factor analysis (EFA), a statistical method used to discover the underlying structure of a relatively large set of variables. It's often used when you don't have a specific hypothesis about what latent factors (unobservable variables) might be influencing the observed variables in your dataset.\n#\n# In this analysis, we've requested three factors (latent variables), and the table presents the loadings of each observed variable on each of these factors. The loadings can be interpreted as the correlations between the observed variables and the latent factors.\n#\n# Here's how to interpret the output:\n#\n#   The variables kessler_depressed, kessler_worthless, and kessler_hopeless load strongly on the first latent factor (MR1), and do not significantly load on the other two. This suggests that these three variables share some common underlying factor.\n#\n# The variable kessler_nervous loads exclusively on the second latent factor (MR2), suggesting it might represent a different latent construct.\n#\n# The variables kessler_restless and kessler_effort load on the third latent factor (MR3), which could represent yet another underlying construct.\n#\n# The \"Complexity\" column indicates the complexity of each item. Complexity 1 indicates that the item is influenced mostly by a single factor.\n#\n# The \"Uniqueness\" column represents the proportion of variance in each variable that is not explained by the factors. For example, the uniqueness of kessler_depressed is 0.33, which means that 33% of the variance in this variable is not accounted for by the three factors.\n#\n# Lastly, the total variance explained by the three latent factors is 66.05%, with MR1 explaining 35.14%, MR2 explaining 17.17%, and MR3 explaining 13.73%. This indicates that about two-thirds of the variance in the six observed variables can be explained by the three latent factors extracted in the analysis.\n# fa -- there is no agreed method!\n# method of agreement:\n\n\n\n\nCode\nn &lt;- n_factors(dt_only_k6)\n\n# # summary\n# as.data.frame(n)\n\n# plot of smmary\nplot(n) + theme_modern()\n\n\n\n\n\n\n\n\n\nCode\n## CFA\n\n\nNext try a CFA\n\n\nCode\n# first partition the data \npart_data &lt;- datawizard::data_partition(dt_only_k6, traing_proportion = .07, seed = 123)\n\ntraining &lt;- part_data$p_0.7\ntest &lt;- part_data$test\n\n\n\n\nCode\n#|label: cfa_all\n# one factor\nstructure_k6_one &lt;- psych::fa(training, nfactors = 1) |&gt;\n  efa_to_cfa()\n\n# two factor model\nstructure_k6_two &lt;- psych::fa(training, nfactors = 2) |&gt;\n  efa_to_cfa()\n\n# three structure model\nstructure_k6_three &lt;- psych::fa(training, nfactors = 3) %&gt;%\n  efa_to_cfa()\n\n# inspect models\nstructure_k6_one\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless + kessler_nervous + kessler_restless + .row_id\n\n\nCode\nstructure_k6_two\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_effort + kessler_nervous + kessler_restless + .row_id\n\n\nCode\nstructure_k6_three\n\n\n# Latent variables\nMR1 =~ kessler_depressed + kessler_effort + kessler_hopeless + kessler_worthless\nMR2 =~ kessler_restless\nMR3 =~ kessler_nervous + .row_id\n\n\n\n\nCode\n# fit and compare models\none_latent &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, data = test))\ntwo_latents &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, data = test))\nthree_latents &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, data = test))\n\ncompare &lt;- performance::compare_performance(one_latent, two_latents, three_latents, verbose = FALSE)\n\n# view as html table\nas.data.frame(compare)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent\nlavaan\n1359.7168\n14\n0\n159746.19\n21\n0\n0.9533955\n0.9067909\n0.9914883\n0.9873622\n0.9915748\n0.1033455\n0.0987385\n0.1080285\n0.0000000\n36.00334\n0.0493327\n0.9872324\n0.6609922\n0.9915752\n0.9915748\n-151483.7\n302995.3\n0\n303094.8\n0\n303050.3\n\n\ntwo_latents\nlavaan\n317.9709\n13\n0\n30915.75\n21\n0\n0.9900793\n0.9786322\n0.9897149\n0.9840541\n0.9901287\n0.0510548\n0.0462789\n0.0559908\n0.3499758\n36.31236\n0.0226983\n0.9833856\n0.6126807\n0.9901313\n0.9901287\n-150962.8\n301955.6\n1\n302062.2\n1\n302014.5\n\n\nthree_latents\nlavaan\n747.8723\n12\n0\n20903.30\n21\n0\n0.9763317\n0.9447739\n0.9642223\n0.9383317\n0.9647609\n0.0825447\n0.0775761\n0.0876237\n0.0000000\n37.13824\n0.0377955\n0.9373890\n0.5509842\n0.9647761\n0.9647609\n-151177.7\n302387.5\n0\n302501.2\n0\n302450.3\n\n\n\n\n\nThis table provides the results of three different Confirmatory Factor Analysis (CFA) models: one that specifies a single latent factor, one that specifies two latent factors, and one that specifies three latent factors. The results include a number of goodness-of-fit statistics, which can be used to assess how well each model fits the data.\n\nOne_latent Model: This model assumes that there is only one underlying latent factor contributing to all variables. This model has a chi-square statistic of 1359.7 with 14 degrees of freedom, which is highly significant (p&lt;0.001), indicating a poor fit of the model to the data. Other goodness-of-fit indices like GFI, AGFI, NFI, NNFI, and CFI are all high (above 0.9), generally indicating good fit, but these indices can be misleading in the presence of large sample sizes. RMSEA is above 0.1 which indicates a poor fit. The SRMR is less than 0.08 which suggests a good fit, but given the high Chi-square and RMSEA values, we can’t solely rely on this index. The Akaike information criterion (AIC), Bayesian information criterion (BIC) and adjusted BIC are used for comparing models, with lower values indicating better fit.\n\n\nTwo_latents Model: This model assumes that there are two underlying latent factors. The chi-square statistic is lower than the one-factor model (317.97 with 13 df), suggesting a better fit. The p-value is still less than 0.05, indicating a statistically significant chi-square, which typically suggests a poor fit. However, all other fit indices (GFI, AGFI, NFI, NNFI, and CFI) are above 0.9 and the RMSEA is 0.051, which generally indicate good fit. The SRMR is also less than 0.08 which suggests a good fit. This model has the lowest AIC and BIC values among the three models, indicating the best fit according to these criteria.\n\n\nThree_latents Model: This model assumes three underlying latent factors. The chi-square statistic is 747.87 with 12 df, higher than the two-factor model, suggesting a worse fit to the data. Other fit indices such as GFI, AGFI, NFI, NNFI, and CFI are below 0.97 and the RMSEA is 0.083, which generally indicate acceptable but not excellent fit. The SRMR is less than 0.08 which suggests a good fit. The AIC and BIC values are higher than the two-factor model but lower than the one-factor model, indicating a fit that is better than the one-factor model but worse than the two-factor model.\nBased on these results, the two-latents model seems to provide the best fit to the data among the three models, according to most of the fit indices and the AIC and BIC. Note, all models have significant chi-square statistics, which suggests some degree of misfit. It’s also important to consider the substantive interpretation of the factors, to make sure the model makes sense theoretically.\n\n\n\nTry with multiple groups\n\n\nCode\n# select the columns we need + ethnicity\ndt_eth_k6_eth &lt;- dt_start |&gt; \n  filter(eth_cat == \"euro\" | eth_cat == \"maori\") |&gt; \n  select(kessler_depressed, kessler_effort,kessler_hopeless,\n                                 kessler_worthless, kessler_nervous,\n                                 kessler_restless, eth_cat)\n\n\n\n# remove ethnicity for traning data \n\n\n\n\n# first partition the data \npart_data_eth &lt;- datawizard::data_partition(dt_eth_k6_eth, traing_proportion = .07, seed = 123, group = \"eth_cat\")\n\ntraining_eth &lt;- part_data_eth$p_0.7\ntest_eth &lt;- part_data_eth$test\n\n\n# fit and compare models for configural equivalence\none_latent_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", data = test_eth))\ntwo_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", data = test_eth))\nthree_latents_eth_configural &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\", data = test_eth))\n\ncompare_eth_configural &lt;- performance::compare_performance(one_latent_eth_configural, two_latents_eth_configural, three_latents_eth_configural, verbose = FALSE)\n\n\n# fit and compare models for metric equivalence\none_latent_eth_metric &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\ntwo_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal = \"loadings\", data = test_eth))\nthree_latents_eth_metric  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal = \"loadings\", data = test_eth))\n\ncompare_eth_metric  &lt;- performance::compare_performance(one_latent_eth_metric, \n                                                        two_latents_eth_metric, \n                                                        three_latents_eth_metric, \n                                                        verbose = FALSE)\n\n\n# fit and compare models for scalar equivalence\none_latent_eth_scalar &lt;- suppressWarnings(lavaan::cfa(structure_k6_one, group = \"eth_cat\", group.equal = c(\"loadings\",\"intercepts\"), data = test_eth))\ntwo_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_two, group = \"eth_cat\", group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\nthree_latents_eth_scalar  &lt;- suppressWarnings(lavaan::cfa(structure_k6_three, group = \"eth_cat\",group.equal =  c(\"loadings\",\"intercepts\"), data = test_eth))\n\ncompare_eth_scalar  &lt;- performance::compare_performance(one_latent_eth_scalar, \n                                                        two_latents_eth_scalar, \n                                                        three_latents_eth_scalar, \n                                                        verbose = FALSE)\n\n\nRecall, in the context of measurement and factor analysis, the concepts of configural, metric, and scalar invariance relate to the comparability of a measurement instrument, such as a survey or test, across different groups.\nWe saw in part 1 of this course that these invariance concepts are frequently tested in the context of cross-cultural, multi-group, or longitudinal studies.\nLet’s first define these concepts, and then apply them to the context of the Kessler 6 (K6) Distress Scale used among Maori and New Zealand Europeans.\n\nConfigural invariance refers to the most basic level of measurement invariance, and it is established when the same pattern of factor loadings and structure is observed across groups. This means that the underlying constructs (factors) are defined the same way for different groups. This doesn’t mean the strength of relationship between items and factors (loadings) or the item means (intercepts) are the same, just that the items relate to the same factors in all groups.\n\nIn the context of the K6 Distress Scale, configural invariance would suggest that the same six items are measuring the construct of psychological distress in the same way for both Māori and New Zealand Europeans, even though the strength of the relationship between the items and the construct (distress), or the average scores, might differ.\n\nMetric invariance (also known as “weak invariance”) refers to the assumption that factor loadings are equivalent across groups, meaning that the relationship or association between the measured items and their underlying factor is the same in all groups. This is important when comparing the strength of relationships with other variables across groups.\n\nIf metric invariance holds for the K6 Distress Scale, this would mean that a unit change in the latent distress factor would correspond to the same change in each item score (e.g., feeling nervous, hopeless, restless, etc.) for both Māori and New Zealand Europeans.\n\nScalar invariance (also known as “strong invariance”) involves equivalence of both factor loadings and intercepts (item means) across groups. This means that not only are the relationships between the items and the factors the same across groups (as with metric invariance), but also the zero-points or origins of the scales are the same. Scalar invariance is necessary when one wants to compare latent mean scores across groups.\n\nIn the context of the K6 Distress Scale, if scalar invariance holds, it would mean that a specific score on the scale would correspond to the same level of the underlying distress factor for both Māori and New Zealand Europeans. It would mean that the groups do not differ systematically in how they interpret and respond to the items. If this holds, one can make meaningful comparisons of distress level between Maori and New Zealand Europeans based on the scale scores.\nNote: each of these levels of invariance is a progressively stricter test of the equivalence of the measurement instrument across groups. Demonstrating scalar invariance, for example, also demonstrates configural and metric invariance. On the other hand, failure to demonstrate metric invariance means that scalar invariance also does not hold. These tests are therefore usually conducted in sequence. The results of these tests should be considered when comparing group means or examining the relationship between a scale and other variables across groups.\n\n\nConfigural invariance:\n\n\nCode\nas.data.frame(compare_eth_configural)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_configural\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_configural\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_configural\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table represents the comparison of three multi-group confirmatory factor analysis (CFA) models conducted to test for configural invariance across different ethnic categories (eth_cat). Configural invariance refers to whether the pattern of factor loadings is the same across groups. It’s the most basic form of measurement invariance.\nLooking at the results, we can draw the following conclusions:\n\nChi2 (Chi-square): A lower value suggests a better model fit. In this case, the two_latents_eth_configural model exhibits the lowest Chi2 value, suggesting it has the best fit according to this metric.\nGFI (Goodness of Fit Index) and AGFI (Adjusted Goodness of Fit Index): These values range from 0 to 1, with values closer to 1 suggesting a better fit. The two_latents_eth_configural model has the highest GFI and AGFI values, indicating it is the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, also called TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting it is the best fit according to these metrics.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 considered good and up to 0.08 considered acceptable. In this table, the two_latents_eth_configural model has an RMSEA of 0.05, which falls within the acceptable range.\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models exhibit acceptable RMR and SRMR values, with the two_latents_eth_configural model having the lowest.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 suggesting a better fit. The one_latent_eth_configural model has the highest values, suggesting the best fit according to these measures.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_configural model has the lowest AIC and BIC, suggesting it is the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_configural model is non-significant, suggesting a good fit.\n\nOverall, the two_latents_eth_configural model appears to provide the best fit across multiple indices, suggesting configural invariance (i.e., the same general factor structure) across ethnic categories with a two-factor solution. As with the previous assessment, theoretical soundness and other substantive considerations should also be taken into account when deciding on the final model.\n\n\nMetric Equivalence\n\n\nCode\nas.data.frame(compare_eth_metric)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_metric\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_metric\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_metric\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThis table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test metric equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_metric, two_latents_eth_metric, three_latents_eth_metric) were run with a constraint of equal factor loadings across groups, which is a requirement for metric invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_metric model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_metric model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. For these indices, the one_latent_eth_metric model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_metric model has an RMSEA within the acceptable range (0.051).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_metric model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_metric model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_metric model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_metric model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_metric model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the metric equivalence (equal factor loadings) assumption is supported across ethnic categories. However, one must also take into consideration the theoretical soundness of the model and other substantive considerations when deciding on the final model.\n\n\nScalar Equivalence\n\n\nCode\n# view as html table\nas.data.frame(compare_eth_scalar)|&gt;\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nChi2\nChi2_df\np_Chi2\nBaseline\nBaseline_df\np_Baseline\nGFI\nAGFI\nNFI\nNNFI\nCFI\nRMSEA\nRMSEA_CI_low\nRMSEA_CI_high\np_RMSEA\nRMR\nSRMR\nRFI\nPNFI\nIFI\nRNI\nLoglikelihood\nAIC\nAIC_wt\nBIC\nBIC_wt\nBIC_adjusted\n\n\n\n\none_latent_eth_scalar\nlavaan\n1162.4746\n14\n0\n341229.17\n21\n0\n0.9831752\n0.9579381\n0.9965933\n0.9949511\n0.9966341\n0.1027048\n0.0977499\n0.1077479\n0.0000000\n38.29915\n0.0439380\n0.9948899\n0.6643955\n0.9966342\n0.9966341\n-129452.1\n258946.1\n0\n259092.3\n0\n259025.5\n\n\ntwo_latents_eth_scalar\nlavaan\n276.7703\n13\n0\n42034.04\n21\n0\n0.9961916\n0.9897467\n0.9934156\n0.9898581\n0.9937217\n0.0510782\n0.0459383\n0.0564019\n0.3560216\n34.17489\n0.0201464\n0.9893636\n0.6149715\n0.9937229\n0.9937217\n-129009.2\n258062.4\n1\n258215.5\n1\n258145.6\n\n\nthree_latents_eth_scalar\nlavaan\n701.6287\n12\n0\n27397.50\n21\n0\n0.9906044\n0.9725962\n0.9743908\n0.9559166\n0.9748095\n0.0859629\n0.0806184\n0.0914299\n0.0000000\n76.26852\n0.0358868\n0.9551839\n0.5567947\n0.9748177\n0.9748095\n-129221.6\n258489.3\n0\n258649.3\n0\n258576.2\n\n\n\n\n\nThe table presents the results of a multi-group confirmatory factor analysis (CFA) conducted to test scalar equivalence (also known as measurement invariance) across different ethnic categories (eth_cat). The models (one_latent_eth_scalar, two_latents_eth_scalar, three_latents_eth_scalar) were run with constraints on both factor loadings and intercepts to be equal across groups, a requirement for scalar invariance.\nHere’s the interpretation of the fit indices:\n\nChi2 (Chi-square): Lower values indicate better model fit. The two_latents_eth_scalar model has the lowest Chi2 value, suggesting the best fit according to this measure.\nGFI (Goodness of Fit Index), AGFI (Adjusted Goodness of Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The two_latents_eth_scalar model has the highest GFI and AGFI values, suggesting the best fit according to these indices.\nNFI (Normed Fit Index), NNFI (Non-Normed Fit Index, or TLI), CFI (Comparative Fit Index): These range from 0 to 1, with values closer to 1 indicating a better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these measures.\nRMSEA (Root Mean Square Error of Approximation): Lower values are better, with values below 0.05 generally considered good, and values up to 0.08 considered acceptable. Only the two_latents_eth_scalar model has an RMSEA within the acceptable range (0.05).\nRMR (Root Mean Square Residual) and SRMR (Standardized Root Mean Square Residual): Lower values are better, typically less than 0.08 is considered a good fit. All models have acceptable RMR and SRMR values, with the two_latents_eth_scalar model having the lowest, indicating the best fit.\nRFI (Relative Fit Index), PNFI (Parsimonious Normed Fit Index), IFI (Incremental Fit Index), RNI (Relative Noncentrality Index): These range from 0 to 1, with values closer to 1 indicating better fit. The one_latent_eth_scalar model has the highest values, suggesting the best fit according to these indices.\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion): Lower values indicate a better fit when comparing models. The two_latents_eth_scalar model has the lowest AIC and BIC, indicating the best fit according to these criteria.\np_Chi2 and p_RMSEA: These are the significance levels for the Chi-square test and the RMSEA, respectively. Non-significant values (p &gt; 0.05) suggest a good fit. Only the RMSEA for the two_latents_eth_scalar model is non-significant, suggesting a good fit.\n\nIn summary, the two_latents_eth_scalar model appears to provide the best fit overall, indicating that a two-factor solution might be appropriate and that the scalar equivalence (equal factor loadings and intercepts) assumption is supported across ethnic categories. However, one must also consider the theoretical soundness of the model and other substantive considerations when deciding on the final model.\nOverall it seems that we have good evidence for the two-factor model of Kessler-6.\nLet’s next get the data into shape for analysis. Here we create a variable for the two factors:\n\n\nCode\n# get two factors from data\ndt_start2 &lt;- dt_start |&gt;\n  arrange(id, wave) |&gt;\n  rowwise() |&gt;\n  mutate(\n    kessler_latent_depression = mean(c(kessler_depressed, kessler_hopeless, kessler_effort), na.rm = TRUE),\n    kessler_latent_anxiety  = mean(c(kessler_effort, kessler_nervous, kessler_restless), na.rm = TRUE)\n  ) |&gt; ungroup()\n\n\nIt is useful toinspect histograms\n\n\nCode\nhist(dt_start2$kessler_latent_anxiety)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(dt_start2$kessler_latent_depression)\n\n\n\n\n\n\n\n\n\n\n\nAssess change in the exposure\nNot this is just a description of the the summary scores. We do not assess change within indivuals\n\n\nCode\n#  select only the baseline year and the exposure year.  That will give us change in the exposure. ()\ndt_exposure &lt;- dt_start2 |&gt;\n\n  # select baseline year and exposure year\n  filter(wave == \"2018\" | wave == \"2019\") |&gt;\n\n  # select variables of interest\n  select(id, wave, hours_exercise_coarsen,  eth_cat) |&gt;\n\n  # the categorical variable needs to be numeric for us to use msm package to investigate change\n  mutate(hours_exercise_coarsen_n = as.numeric(hours_exercise_coarsen)) |&gt;\n  droplevels()\n\n\n# check\ndt_exposure |&gt;\n  tabyl(hours_exercise_coarsen_n, eth_cat,  wave )\n\n\n$`2018`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 3238   319      78   170\n                        2 3790   341      81   130\n                        3 1613   161      31    48\n\n$`2019`\n hours_exercise_coarsen_n euro māori pacific asian\n                        1 2880   307      79   143\n                        2 3927   354      82   141\n                        3 1834   160      29    64\n\n\nI’ve written a function called transition_table that will help us assess change in the exposure at the individual level.\n\n\nCode\n#   consider people going from active to vary active\nout &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure)\n\n\n# for a function I wrote to create state tables\nstate_names &lt;- c(\"Inactive\", \"Somewhat Active\", \"Active\", \"Extremely Active\")\n\n# transition table\n\ntransition_table(out, state_names)\n\n\n$explanation\n[1] \"This transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\"\n\n$table\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   2186   |      1324       |  295   |\n| Somewhat Active |   1019   |      2512       |  811   |\n|     Active      |   204    |       668       |  981   |\n\n\nNext consider Māori only\n\n\nCode\n# Maori only\n\ndt_exposure_maori &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"māori\")\n\nout_m &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_maori)\n\n# with this little support we might consider parametric models\nt_tab_m&lt;- transition_table( out_m, state_names)\n\n#interpretation\ncat(t_tab_m$explanation)\n\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n\nCode\nprint(t_tab_m$table)\n\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   187    |       108       |   24   |\n| Somewhat Active |    92    |       188       |   61   |\n|     Active      |    28    |       58        |   75   |\n\n\n\n\nCode\n# filter euro\ndt_exposure_euro &lt;- dt_exposure |&gt;\n  filter(eth_cat == \"euro\")\n\n# model change\nout_e &lt;- msm::statetable.msm(round(hours_exercise_coarsen_n, 0), id, data = dt_exposure_euro)\n\n\n# creat transition table.\nt_tab_e &lt;- transition_table( out_e, state_names)\n\n#interpretation\ncat(t_tab_e$explanation)\n\n\nThis transition matrix describes the shifts from one state to another between the baseline wave and the following wave. The numbers in the cells represent the number of individuals who transitioned from one state (rows) to another (columns). For example, the cell in the first row and second column shows the number of individuals who transitioned from the first state (indicated by the left-most cell in the row) to the second state. The top left cell shows the number of individuals who remained in the first state.\n\n\nCode\n# table\nprint(t_tab_e$table)\n\n\n\n\n|      From       | Inactive | Somewhat Active | Active |\n|:---------------:|:--------:|:---------------:|:------:|\n|    Inactive     |   1843   |      1136       |  259   |\n| Somewhat Active |   870    |      2208       |  712   |\n|     Active      |   167    |       583       |  863   |\n\n\nOverall we find evidence for change in the exposure variable. This suggest that we are ready to proceed with the next step of causal estimation.\n\n\nCreate wide data frame for analysis\n\n\nCode\n############## ############## ############## ############## ############## ############## ############## ########\n####  ####  ####  CREATE DATA FRAME FOR ANALYSIS ####  ####  ################## ############## ######## #########\n############## ############## ############## ############## ############## ############## ############# #########\n\n# To find out more about our dataset go here:\n# https://github.com/go-bayes/psych-434-2023/blob/main/data/readme.qmd\n\n\n# I have created a function that will put the data into the correct shape. Here are the steps.\n\n# Step 1: choose baseline variables (confounders).  here we select standard demographic variablees plus personality variables.\n\n# note that the function will automatically include the baseline exposure and basline outcome in the baseline variable confounder set so you don't need to include these. \n\n\n# here are some plausible baseline confounders\nbaseline_vars = c(\n  \"edu\",\n  \"male\",\n  \"eth_cat\",\n  \"employed\",\n  \"gen_cohort\",\n  \"nz_dep2018\", # nz dep\n  \"nzsei13\", # occupational prestige\n  \"partner\",\n  \"parent\",\n  \"pol_orient\",\n # \"rural_gch2018\",\n   \"urban\", # use the two level urban varaible. \n  \"agreeableness\",\n  \"conscientiousness\",\n  \"extraversion\",\n  \"honesty_humility\",\n  \"openness\",\n  \"neuroticism\",\n  \"modesty\",\n  \"religion_identification_level\"\n)\n\n\n## Step 2, select the exposure variable.  This is the \"cause\"\nexposure_var = c(\"hours_exercise_coarsen\")\n\n\n## step 3. select the outcome variable.  These are the outcomes.\noutcome_vars_reflective = c(\"kessler_latent_anxiety\",\n                            \"kessler_latent_depression\")\n\n\n\n# the function \"create_wide_data\" should be in your environment.\n# If not, make sure to run the first line of code in this script once more.  You may ignore the warnings. or uncomment and run the code below\n# source(\"https://raw.githubusercontent.com/go-bayes/templates/main/functions/funs.R\")\ndt_prepare &lt;-\n  create_wide_data(\n    dat_long = dt_start2,\n    baseline_vars = baseline_vars,\n    exposure_var = exposure_var,\n    outcome_vars = outcome_vars_reflective\n  )\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exclude_vars)\n\n  # Now:\n  data %&gt;% select(all_of(exclude_vars))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(t0_column_order)\n\n  # Now:\n  data %&gt;% select(all_of(t0_column_order))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;."
  },
  {
    "objectID": "scripts/experiment_template.html#descriptive-table",
    "href": "scripts/experiment_template.html#descriptive-table",
    "title": "experiment_template.qmd",
    "section": "Descriptive table",
    "text": "Descriptive table\nI created a simple function\n\n\nCode\n# I have created a function that will allow you to take a data frame and\n# create a table\n# REDO\n\n\nHowever, if would like like a nicer table, try this:\n\n\nCode\n# get data into shape\ndt_new &lt;- dt_prepare %&gt;%\n  select(starts_with(\"t0\")) %&gt;%\n  rename_all(~ stringr::str_replace(., \"^t0_\", \"\")) %&gt;%\n  mutate(wave = factor(rep(\"baseline\", nrow(dt_prepare)))) |&gt;\n  janitor::clean_names(case = \"screaming_snake\")\n\n\n# create a formula string\n\nbaseline_vars_names &lt;- dt_new %&gt;%\n  select(-WAVE) %&gt;%\n  colnames()\n\ntable_baseline_vars &lt;-\n  paste(baseline_vars_names, collapse = \"+\")\n\nformula_string_table_baseline &lt;-\n  paste(\"~\", table_baseline_vars, \"|WAVE\")\n\ntable1::table1(as.formula(formula_string_table_baseline),\n               data = dt_new,\n               overall = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nbaseline\n(N=10000)\n\n\n\n\nEDU\n\n\n\nMean (SD)\n5.85 (2.59)\n\n\nMedian [Min, Max]\n6.96 [-0.128, 10.1]\n\n\nMALE\n\n\n\nMale\n3905 (39.1%)\n\n\nNot_male\n6095 (61.0%)\n\n\nETH_CAT\n\n\n\neuro\n8641 (86.4%)\n\n\nmāori\n821 (8.2%)\n\n\npacific\n190 (1.9%)\n\n\nasian\n348 (3.5%)\n\n\nEMPLOYED\n\n\n\nMean (SD)\n0.836 (0.370)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nGEN_COHORT\n\n\n\nGen_Silent: born&lt; 1946\n166 (1.7%)\n\n\nGen Boomers: born &gt;= 1946 & b.&lt; 1965\n4257 (42.6%)\n\n\nGenX: born &gt;=1961 & b.&lt; 1981\n3493 (34.9%)\n\n\nGenY: born &gt;=1981 & b.&lt; 1996\n1883 (18.8%)\n\n\nGenZ: born &gt;= 1996\n201 (2.0%)\n\n\nNZ_DEP2018\n\n\n\nMean (SD)\n4.46 (2.65)\n\n\nMedian [Min, Max]\n4.01 [0.835, 10.1]\n\n\nNZSEI13\n\n\n\nMean (SD)\n57.0 (16.1)\n\n\nMedian [Min, Max]\n61.0 [9.91, 90.1]\n\n\nPARTNER\n\n\n\nMean (SD)\n0.795 (0.404)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPARENT\n\n\n\nMean (SD)\n0.706 (0.456)\n\n\nMedian [Min, Max]\n1.00 [0, 1.00]\n\n\nPOL_ORIENT\n\n\n\nMean (SD)\n3.47 (1.40)\n\n\nMedian [Min, Max]\n3.09 [0.862, 7.14]\n\n\nURBAN\n\n\n\nrural\n1738 (17.4%)\n\n\nurban\n8262 (82.6%)\n\n\nAGREEABLENESS\n\n\n\nMean (SD)\n5.36 (0.986)\n\n\nMedian [Min, Max]\n5.48 [0.977, 7.13]\n\n\nCONSCIENTIOUSNESS\n\n\n\nMean (SD)\n5.19 (1.03)\n\n\nMedian [Min, Max]\n5.28 [0.938, 7.16]\n\n\nEXTRAVERSION\n\n\n\nMean (SD)\n3.85 (1.21)\n\n\nMedian [Min, Max]\n3.80 [0.861, 7.07]\n\n\nHONESTY_HUMILITY\n\n\n\nMean (SD)\n5.52 (1.12)\n\n\nMedian [Min, Max]\n5.71 [1.14, 7.15]\n\n\nOPENNESS\n\n\n\nMean (SD)\n5.06 (1.10)\n\n\nMedian [Min, Max]\n5.12 [0.899, 7.15]\n\n\nNEUROTICISM\n\n\n\nMean (SD)\n3.41 (1.17)\n\n\nMedian [Min, Max]\n3.31 [0.860, 7.08]\n\n\nMODESTY\n\n\n\nMean (SD)\n6.07 (0.860)\n\n\nMedian [Min, Max]\n6.24 [2.17, 7.17]\n\n\nRELIGION_IDENTIFICATION_LEVEL\n\n\n\nMean (SD)\n2.19 (2.07)\n\n\nMedian [Min, Max]\n1.00 [1.00, 7.00]\n\n\nHOURS_EXERCISE_COARSEN\n\n\n\ninactive\n3805 (38.1%)\n\n\nactive\n4342 (43.4%)\n\n\nvery_active\n1853 (18.5%)\n\n\nKESSLER_LATENT_ANXIETY\n\n\n\nMean (SD)\n1.16 (0.719)\n\n\nMedian [Min, Max]\n1.03 [-0.0800, 4.03]\n\n\nKESSLER_LATENT_DEPRESSION\n\n\n\nMean (SD)\n0.744 (0.686)\n\n\nMedian [Min, Max]\n0.646 [-0.0871, 4.02]\n\n\n\n\n\n\n\nCode\n# another method for making a table\n# x &lt;- table1::table1(as.formula(formula_string_table_baseline),\n#                     data = dt_new,\n#                     overall = FALSE)\n\n# # some options, see: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html\n# table1::t1kable(x, format = \"html\", booktabs = TRUE) |&gt;\n#   kable_material(c(\"striped\", \"hover\"))\n\n\nSome more data wrangling.\n\nmutate(id = factor(1:nrow(dt_prepare))): This creates a new column called id that has unique identification factors for each row in the dataset. It ranges from 1 to the number of rows in the dataset.\nThe next mutate operation is used to convert the t0_eth_cat, t0_urban, and t0_gen_cohort variables to factor type, if they are not already.\nThe filter command is used to subset the dataset to only include rows where the t0_eth_cat is either “euro” or “māori”. The original dataset includes data with four different ethnic categories. This command filters out any row not related to these two groups.\nungroup() ensures that there’s no grouping in the dataframe.\nThe mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) step standardizes all numeric columns in the dataset by subtracting the mean and dividing by the standard deviation (a z-score transformation). The resulting columns are renamed to include “_z” at the end of their original names.\nThe select function is used to keep only specific columns: the id column, any columns that are factors, and any columns that end in “_z”.\nThe relocate functions re-order columns. The first relocate places the id column at the beginning. The next three relocate functions order the rest of the columns based on their names: those starting with “t0_” are placed before “t1_” columns, and those starting with “t2_” are placed after “t1_” columns.\ndroplevels() removes unused factor levels in the dataframe.\nFinally, skimr::skim(dt) will print out a summary of the data in the dt object using the skimr package. This provides a useful overview of the data, including data types and summary statistics.\n\nThis function seems to be part of a data preparation pipeline in a longitudinal or panel analysis, where observations are ordered over time (indicated by t0_, t1_, t2_, etc.).\n\n\nCode\n### ### ### ### ### ### SUBGROUP DATA ANALYSIS: DATA WRANGLING  ### ### ### ###\n\ndt &lt;- dt_prepare|&gt;\n  mutate(id = factor(1:nrow(dt_prepare))) |&gt;\n  mutate(\n  t0_eth_cat = as.factor(t0_eth_cat),\n  t0_urban = as.factor(t0_urban),\n  t0_gen_cohort = as.factor(t0_gen_cohort)\n) |&gt;\n  dplyr::filter(t0_eth_cat == \"euro\" |\n                t0_eth_cat == \"māori\") |&gt; # Too few asian and pacific\n  ungroup() |&gt;\n  # transform numeric variables into z scores (improves estimation)\n  dplyr::mutate(across(where(is.numeric), ~ scale(.x), .names = \"{col}_z\")) %&gt;%\n  # select only factors and numeric values that are z-scores\n  select(id, # category is too sparse\n         where(is.factor),\n         ends_with(\"_z\"), ) |&gt;\n  # tidy data frame so that the columns are ordered by time (useful for more complex models)\n  relocate(id, .before = starts_with(\"t1_\"))   |&gt;\n  relocate(starts_with(\"t0_\"), .before = starts_with(\"t1_\"))  |&gt;\n  relocate(starts_with(\"t2_\"), .after = starts_with(\"t1_\")) |&gt;\n  droplevels()\n\n# view object\nskimr::skim(dt)\n\n\n\n\nData summary\n\n\n\n\nName\n\n\ndt\n\n\n\n\nNumber of rows\n\n\n9462\n\n\n\n\nNumber of columns\n\n\n26\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n7\n\n\n\n\nnumeric\n\n\n19\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\nVariable type: factor\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nid\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n9462\n\n\n1: 1, 2: 1, 3: 1, 4: 1\n\n\n\n\nt0_male\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nNot: 5767, Mal: 3695\n\n\n\n\nt0_eth_cat\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\neur: 8641, māo: 821\n\n\n\n\nt0_gen_cohort\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n5\n\n\nGen: 4107, Gen: 3311, Gen: 1716, Gen: 164\n\n\n\n\nt0_urban\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n2\n\n\nurb: 7762, rur: 1700\n\n\n\n\nt0_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4131, ina: 3557, ver: 1774\n\n\n\n\nt1_hours_exercise_coarsen\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n3\n\n\nact: 4281, ina: 3187, ver: 1994\n\n\n\n\nVariable type: numeric\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nt0_edu_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.29\n\n\n-1.05\n\n\n0.44\n\n\n0.82\n\n\n1.66\n\n\n▂▃▃▇▂\n\n\n\n\nt0_employed_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.26\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n0.44\n\n\n▂▁▁▁▇\n\n\n\n\nt0_nz_dep2018_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.36\n\n\n-0.92\n\n\n-0.16\n\n\n0.63\n\n\n2.17\n\n\n▇▆▆▅▂\n\n\n\n\nt0_nzsei13_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.94\n\n\n-0.75\n\n\n0.25\n\n\n0.81\n\n\n2.07\n\n\n▁▃▅▇▁\n\n\n\n\nt0_partner_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.99\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n0.50\n\n\n▂▁▁▁▇\n\n\n\n\nt0_parent_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.58\n\n\n-1.58\n\n\n0.63\n\n\n0.63\n\n\n0.63\n\n\n▃▁▁▁▇\n\n\n\n\nt0_pol_orient_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.87\n\n\n-1.02\n\n\n-0.28\n\n\n0.44\n\n\n2.62\n\n\n▇▆▇▅▂\n\n\n\n\nt0_agreeableness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.46\n\n\n-0.62\n\n\n0.12\n\n\n0.68\n\n\n1.79\n\n\n▁▁▃▇▆\n\n\n\n\nt0_conscientiousness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.13\n\n\n-0.65\n\n\n0.08\n\n\n0.76\n\n\n1.91\n\n\n▁▁▅▇▅\n\n\n\n\nt0_extraversion_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.48\n\n\n-0.71\n\n\n-0.04\n\n\n0.72\n\n\n2.67\n\n\n▂▆▇▅▁\n\n\n\n\nt0_honesty_humility_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.95\n\n\n-0.69\n\n\n0.17\n\n\n0.84\n\n\n1.45\n\n\n▁▁▃▆▇\n\n\n\n\nt0_openness_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-3.76\n\n\n-0.71\n\n\n0.05\n\n\n0.81\n\n\n1.90\n\n\n▁▂▆▇▅\n\n\n\n\nt0_neuroticism_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-2.18\n\n\n-0.76\n\n\n-0.09\n\n\n0.71\n\n\n3.14\n\n\n▃▇▇▃▁\n\n\n\n\nt0_modesty_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-4.67\n\n\n-0.66\n\n\n0.19\n\n\n0.83\n\n\n1.26\n\n\n▁▁▂▅▇\n\n\n\n\nt0_religion_identification_level_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-0.56\n\n\n-0.56\n\n\n-0.56\n\n\n-0.08\n\n\n2.37\n\n\n▇▁▁▁▂\n\n\n\n\nt0_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.72\n\n\n-0.69\n\n\n-0.19\n\n\n0.70\n\n\n4.01\n\n\n▇▇▆▁▁\n\n\n\n\nt0_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.21\n\n\n-0.63\n\n\n-0.13\n\n\n0.42\n\n\n4.83\n\n\n▇▂▂▁▁\n\n\n\n\nt2_kessler_latent_depression_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.23\n\n\n-0.65\n\n\n-0.16\n\n\n0.39\n\n\n4.75\n\n\n▇▃▂▁▁\n\n\n\n\nt2_kessler_latent_anxiety_z\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n-1.74\n\n\n-0.70\n\n\n-0.20\n\n\n0.68\n\n\n3.97\n\n\n▇▇▆▁▁\n\n\n\n\n\n\n\n\nCode\n# quick cross table\n#table( dt$t1_hours_exercise_coarsen, dt$t0_eth_cat )\n\n# checks\nhist(dt$t2_kessler_latent_depression_z)\nhist(dt$t2_kessler_latent_anxiety_z)\n\ndt |&gt;\n  tabyl(t0_eth_cat, t1_hours_exercise_coarsen ) |&gt;\n  kbl(format = \"markdown\")\n\n# Visualise missingness\nnaniar::vis_miss(dt)\n\n# save your dataframe for future use\n\n# make dataframe\ndt = as.data.frame(dt)\n\n# save data\nsaveRDS(dt, here::here(\"data\", \"dt\"))"
  },
  {
    "objectID": "scripts/experiment_template.html#propensity-scores",
    "href": "scripts/experiment_template.html#propensity-scores",
    "title": "experiment_template.qmd",
    "section": "Propensity scores",
    "text": "Propensity scores\nNext we generate propensity scores. Instead of modelling the outcome (t2_y) we will model the exposure (t1_x) as predicted by baseline indicators (t0_c) that we assume may be associated with the outcome and the exposure.\nThe first step is to obtain the baseline variables. note that we must remove “t0_eth_cat” because we are performing separate weighting for each stratum within this variable.\n\n\nCode\n# read -- you may start here if you need to repeat the analysis\n\ndt &lt;- readRDS(here::here(\"data\", \"dt\"))\n\n# get column names\nbaseline_vars_reflective_propensity &lt;- dt|&gt;\n  dplyr::select(starts_with(\"t0\"), -t0_eth_cat) |&gt; colnames()\n\n# define our exposure\nX &lt;- \"t1_hours_exercise_coarsen\"\n\n# define subclasses\nS &lt;- \"t0_eth_cat\"\n\n# Make sure data is in a data frame format\ndt &lt;- data.frame(dt)\n\n\n# next we use our trick for creating a formula string, which will reduce our work\nformula_str_prop &lt;-\n  paste(X,\n        \"~\",\n        paste(baseline_vars_reflective_propensity, collapse = \"+\"))\n\n# this shows the exposure variable as predicted by the baseline confounders.\nformula_str_prop\n\n\n[1] \"t1_hours_exercise_coarsen ~ t0_male+t0_gen_cohort+t0_urban+t0_hours_exercise_coarsen+t0_edu_z+t0_employed_z+t0_nz_dep2018_z+t0_nzsei13_z+t0_partner_z+t0_parent_z+t0_pol_orient_z+t0_agreeableness_z+t0_conscientiousness_z+t0_extraversion_z+t0_honesty_humility_z+t0_openness_z+t0_neuroticism_z+t0_modesty_z+t0_religion_identification_level_z+t0_kessler_latent_anxiety_z+t0_kessler_latent_depression_z\"\n\n\nFor propensity score analysis, we will try several different approaches. We will want to select the method that produces the best balance.\nI typically use “ps” (classical propensity scores), ebal and energy. The latter two in my experience yeild good balance. Also energy will work with continuous exposures.\nFor more information, see https://ngreifer.github.io/WeightIt/\n\n\nCode\n# traditional propensity scores-- note we select the ATT and we have a subgroup \ndt_match_ps &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ps\"\n)\n\nsaveRDS(dt_match_ps, here::here(\"data\", \"dt_match_ps\"))\n\n\n# ebalance\ndt_match_ebal &lt;- match_mi_general(\n  data = dt,\n  X = X,\n  baseline_vars = baseline_vars_reflective_propensity,\n  subgroup = \"t0_eth_cat\",\n  estimand = \"ATE\",\n  method = \"ebal\"\n)\n\n# save output\nsaveRDS(dt_match_ebal, here::here(\"data\", \"dt_match_ebal\"))\n\n\n\n## energy balance method\n# dt_match_energy &lt;- match_mi_general(\n#   data = dt,\n#   X = X,\n#   baseline_vars = baseline_vars_reflective_propensity,\n#   subgroup = \"t0_eth_cat\",\n#   estimand = \"ATE\",\n#   #focal = \"high\", # for use with ATT\n#   method = \"energy\"\n# )\n# saveRDS(dt_match_energy, here::here(\"data\", \"dt_match_energy\"))\n\n\nResults, first for Europeans\n\n\nCode\ndt_match_energy &lt;- readRDS(here::here(\"data\", \"dt_match_energy\"))\ndt_match_ebal &lt;- readRDS(here::here(\"data\", \"dt_match_ebal\"))\ndt_match_ps &lt;- readRDS(here::here(\"data\", \"dt_match_ps\"))\n\n# next we inspect balance. \"Max.Diff.Adj\" should ideally be less than .05, but less than .1 is ok. This is the standardised mean difference. The variance ratio should be less than 2. \n# note that if the variables are unlikely to influence the outcome we can be less strict. \n\n#See: Hainmueller, J. 2012. “Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies.” Political Analysis 20 (1): 25–46. https://doi.org/10.1093/pan/mpr025.\n\n# Cole SR, Hernan MA. Constructing inverse probability weights for marginal structural models. American Journal of\n# Epidemiology 2008; 168(6):656–664.\n\n# Moving towards best practice when using inverse probability of treatment weighting (IPTW) using the propensity score to estimate causal treatment effects in observational studies\n# Peter C. Austin, Elizabeth A. Stuart\n# https://onlinelibrary.wiley.com/doi/10.1002/sim.6607\n\nbal.tab(dt_match_energy$euro)   #  good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0008\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0007\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0006\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0012\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0001\nt0_urban_urban                                      Binary       0.0002\nt0_hours_exercise_coarsen_inactive                  Binary       0.0049\nt0_hours_exercise_coarsen_active                    Binary       0.0005\nt0_hours_exercise_coarsen_very_active               Binary       0.0045\nt0_edu_z                                           Contin.       0.0021\nt0_employed_z                                      Contin.       0.0016\nt0_nz_dep2018_z                                    Contin.       0.0039\nt0_nzsei13_z                                       Contin.       0.0024\nt0_partner_z                                       Contin.       0.0003\nt0_parent_z                                        Contin.       0.0010\nt0_pol_orient_z                                    Contin.       0.0033\nt0_agreeableness_z                                 Contin.       0.0019\nt0_conscientiousness_z                             Contin.       0.0015\nt0_extraversion_z                                  Contin.       0.0026\nt0_honesty_humility_z                              Contin.       0.0015\nt0_openness_z                                      Contin.       0.0003\nt0_neuroticism_z                                   Contin.       0.0031\nt0_modesty_z                                       Contin.       0.0011\nt0_religion_identification_level_z                 Contin.       0.0029\nt0_kessler_latent_anxiety_z                        Contin.       0.0009\nt0_kessler_latent_depression_z                     Contin.       0.0010\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1646.63 3164.13      946.78\n\n\nCode\nbal.tab(dt_match_ebal$euro)   #  best\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0001\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0001\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0001\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0001\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0001\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0001\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0003\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0001\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0000\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0001\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0001\nt0_modesty_z                                       Contin.       0.0001\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0001\nt0_kessler_latent_depression_z                     Contin.       0.0000\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1855.89 3659.59     1052.01\n\n\nCode\nbal.tab(dt_match_ps$euro)   #  not as good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0664\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0038\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0305\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0236\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0107\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0025\nt0_urban_urban                                      Binary       0.0447\nt0_hours_exercise_coarsen_inactive                  Binary       0.0832\nt0_hours_exercise_coarsen_active                    Binary       0.1000\nt0_hours_exercise_coarsen_very_active               Binary       0.0728\nt0_edu_z                                           Contin.       0.1782\nt0_employed_z                                      Contin.       0.0636\nt0_nz_dep2018_z                                    Contin.       0.1321\nt0_nzsei13_z                                       Contin.       0.1387\nt0_partner_z                                       Contin.       0.0698\nt0_parent_z                                        Contin.       0.0186\nt0_pol_orient_z                                    Contin.       0.0644\nt0_agreeableness_z                                 Contin.       0.1456\nt0_conscientiousness_z                             Contin.       0.0769\nt0_extraversion_z                                  Contin.       0.0607\nt0_honesty_humility_z                              Contin.       0.0181\nt0_openness_z                                      Contin.       0.0488\nt0_neuroticism_z                                   Contin.       0.0338\nt0_modesty_z                                       Contin.       0.0137\nt0_religion_identification_level_z                 Contin.       0.0757\nt0_kessler_latent_anxiety_z                        Contin.       0.0327\nt0_kessler_latent_depression_z                     Contin.       0.0768\n\nEffective sample sizes\n           inactive  active very_active\nUnadjusted  2880.   3927.       1834.  \nAdjusted    1585.48 3760.58      949.56\n\n\nResults for Maori\n\n\nCode\nbal.tab(dt_match_energy$māori)   #  good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0090\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0020\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0161\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0038\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0077\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0067\nt0_urban_urban                                      Binary       0.0058\nt0_hours_exercise_coarsen_inactive                  Binary       0.0408\nt0_hours_exercise_coarsen_active                    Binary       0.0152\nt0_hours_exercise_coarsen_very_active               Binary       0.0256\nt0_edu_z                                           Contin.       0.0205\nt0_employed_z                                      Contin.       0.0432\nt0_nz_dep2018_z                                    Contin.       0.0387\nt0_nzsei13_z                                       Contin.       0.0147\nt0_partner_z                                       Contin.       0.0179\nt0_parent_z                                        Contin.       0.0182\nt0_pol_orient_z                                    Contin.       0.0262\nt0_agreeableness_z                                 Contin.       0.0057\nt0_conscientiousness_z                             Contin.       0.0447\nt0_extraversion_z                                  Contin.       0.0117\nt0_honesty_humility_z                              Contin.       0.0073\nt0_openness_z                                      Contin.       0.0159\nt0_neuroticism_z                                   Contin.       0.0134\nt0_modesty_z                                       Contin.       0.0053\nt0_religion_identification_level_z                 Contin.       0.0167\nt0_kessler_latent_anxiety_z                        Contin.       0.0202\nt0_kessler_latent_depression_z                     Contin.       0.0125\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted    307.  354.        160.  \nAdjusted      212.2 289.68       97.34\n\n\nCode\nbal.tab(dt_match_ebal$māori)   #  best\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0000\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0000\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0000\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0000\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0000\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0000\nt0_urban_urban                                      Binary       0.0000\nt0_hours_exercise_coarsen_inactive                  Binary       0.0000\nt0_hours_exercise_coarsen_active                    Binary       0.0000\nt0_hours_exercise_coarsen_very_active               Binary       0.0000\nt0_edu_z                                           Contin.       0.0000\nt0_employed_z                                      Contin.       0.0001\nt0_nz_dep2018_z                                    Contin.       0.0000\nt0_nzsei13_z                                       Contin.       0.0000\nt0_partner_z                                       Contin.       0.0002\nt0_parent_z                                        Contin.       0.0001\nt0_pol_orient_z                                    Contin.       0.0000\nt0_agreeableness_z                                 Contin.       0.0001\nt0_conscientiousness_z                             Contin.       0.0000\nt0_extraversion_z                                  Contin.       0.0000\nt0_honesty_humility_z                              Contin.       0.0000\nt0_openness_z                                      Contin.       0.0000\nt0_neuroticism_z                                   Contin.       0.0000\nt0_modesty_z                                       Contin.       0.0000\nt0_religion_identification_level_z                 Contin.       0.0001\nt0_kessler_latent_anxiety_z                        Contin.       0.0000\nt0_kessler_latent_depression_z                     Contin.       0.0001\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     220.54 321.09       76.39\n\n\nCode\nbal.tab(dt_match_ps$māori)   #  not good\n\n\nBalance summary across all treatment pairs\n                                                      Type Max.Diff.Adj\nt0_male_Not_male                                    Binary       0.0839\nt0_gen_cohort_Gen_Silent: born&lt; 1946                Binary       0.0048\nt0_gen_cohort_Gen Boomers: born &gt;= 1946 & b.&lt; 1965  Binary       0.0692\nt0_gen_cohort_GenX: born &gt;=1961 & b.&lt; 1981          Binary       0.0203\nt0_gen_cohort_GenY: born &gt;=1981 & b.&lt; 1996          Binary       0.0548\nt0_gen_cohort_GenZ: born &gt;= 1996                    Binary       0.0131\nt0_urban_urban                                      Binary       0.0790\nt0_hours_exercise_coarsen_inactive                  Binary       0.0810\nt0_hours_exercise_coarsen_active                    Binary       0.1137\nt0_hours_exercise_coarsen_very_active               Binary       0.1082\nt0_edu_z                                           Contin.       0.1130\nt0_employed_z                                      Contin.       0.0998\nt0_nz_dep2018_z                                    Contin.       0.1613\nt0_nzsei13_z                                       Contin.       0.1672\nt0_partner_z                                       Contin.       0.1720\nt0_parent_z                                        Contin.       0.1720\nt0_pol_orient_z                                    Contin.       0.1194\nt0_agreeableness_z                                 Contin.       0.1196\nt0_conscientiousness_z                             Contin.       0.0692\nt0_extraversion_z                                  Contin.       0.1158\nt0_honesty_humility_z                              Contin.       0.0994\nt0_openness_z                                      Contin.       0.2098\nt0_neuroticism_z                                   Contin.       0.0706\nt0_modesty_z                                       Contin.       0.1336\nt0_religion_identification_level_z                 Contin.       0.1557\nt0_kessler_latent_anxiety_z                        Contin.       0.1641\nt0_kessler_latent_depression_z                     Contin.       0.1164\n\nEffective sample sizes\n           inactive active very_active\nUnadjusted   307.   354.        160.  \nAdjusted     181.71 334.58       79.73\n\n\n\n\nCode\n# code for summar\nsum_e &lt;- summary(dt_match_ebal$euro)\nsum_m &lt;- summary(dt_match_ebal$māori)\n\n\nlove_plot_e &lt;- love.plot(dt_match_ebal$euro,\n          binary = \"std\",\n          thresholds = c(m = .1))+ labs(title = \"NZ Euro PS: ebalance\")\n\nlove_plot_m &lt;- love.plot(dt_match_ebal$māori,\n          binary = \"std\",\n          thresholds = c(m = .1)) + labs(title = \"Māori PS: ebalance\")\n\n\nlibrary(patchwork)\n\n\nlove_plot_e / love_plot_m\n\n\n\n\n\n\n\n\n\nMore data wrangling\n\n\nCode\n# prepare data post-weighting ---------------------------------------------\n\n\n# prepare data\ndt_ref_e &lt;- subset(dt, t0_eth_cat == \"euro\") # original data subset only nz europeans\n\ndt_ref_e$weights &lt;- dt_match_ebal$euro$weights # get weights from the ps matching model,add to data\n\n# prepare data\ndt_ref_m &lt;- subset(dt, t0_eth_cat == \"māori\")# original data subset only maori\ndt_ref_m$weights &lt;- dt_match_ebal$māori$weights # get weights from the ps matching model, add to data\n\n# combine data into one data frame\ndt_ref_all &lt;- rbind(dt_ref_e, dt_ref_m) # combine the data into one dataframe. \n\n\nLet’s consider a pseudo experiment where someome moves from inactive to active.\n\n\nCode\n# we do not evaluate to save time\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_anxiety_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# formula_str. # inspect on our own time \n\n\n\n# fit model\nfit_all_all  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_all, n = nsims, vcov = \"HC0\")\n\n# simulate effect as modified in europeans\nsim_estimand_all_e &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n#rm(sim_estimand_all_e)\n# note contrast of interest\nsim_estimand_all_e &lt;-\n  transform(sim_estimand_all_e, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n#rm(sim_estimand_all_m)\n\n# simulate effect as modified in māori\nsim_estimand_all_m &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\n#m(sim_estimand_all_m)\n\nsim_estimand_all_m &lt;-\n  transform(sim_estimand_all_m, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n# rearrange\nnames(sim_estimand_all_e) &lt;-\n  paste(names(sim_estimand_all_e), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m) &lt;-\n  paste(names(sim_estimand_all_m), \"m\", sep = \"_\")\n\nest_all_anxiety &lt;- cbind(sim_estimand_all_m, sim_estimand_all_e)\nest_all_anxiety &lt;- transform(est_all_anxiety, `RD_m - RD_e` = RD_m - RD_e)\n\nsaveRDS(est_all_anxiety, here::here(\"data\",\"est_all_anxiety\"))\n# view summary\n\n\nCalculate E-values\n\n\nCode\n# read data\nest_all_anxiety &lt;- readRDS( here::here(\"data\",\"est_all_anxiety\"))\n\n# make dataframe\ndf_anxiety_all &lt;- data.frame( summary(est_all_anxiety) )\n\ntable_estimates_anxiety &lt;- df_anxiety_all |&gt; \n    filter(row.names(df_anxiety_all) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\nWarning: There was 1 warning in `dplyr::mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nCode\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntest_tab &lt;- tab_ate_subgroup_rd(table_estimates_anxiety, delta = 1, sd = 1)\n\n\nConfidence interval crosses the true value, so its E-value is 1.\n\n\nCode\ntest_tab |&gt; kbl(format = \"markdown\")\n\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n0.027\n-0.114\n0.188\n1.185\n1\n\n\nRD_e\n-0.077\n-0.131\n-0.022\n1.352\n1"
  },
  {
    "objectID": "scripts/experiment_template.html#differences-by-subgroups",
    "href": "scripts/experiment_template.html#differences-by-subgroups",
    "title": "experiment_template.qmd",
    "section": "Differences by subgroups",
    "text": "Differences by subgroups\n\n\nCode\ndf_anxiety_all_plot &lt;- df_anxiety_all |&gt; \n # filter(row.names(df_anxiety_all) %in% c(\"RD_m - RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3)) \n\ndf_anxiety_all_plot\n\n\n                    estimate lower_ci upper_ci\nE[Y(inactive)]_m       0.108    0.010    0.201\nE[Y(active)]_m         0.138    0.056    0.214\nE[Y(very_active)]_m    0.134    0.014    0.264\nRD_m                   0.027   -0.114    0.188\nE[Y(inactive)]_e       0.034    0.002    0.066\nE[Y(active)]_e        -0.021   -0.046    0.002\nE[Y(very_active)]_e   -0.043   -0.085    0.004\nRD_e                  -0.077   -0.131   -0.022\nRD_m - RD_e            0.104   -0.042    0.279\n\n\nCode\nplot_sub_forest(df_anxiety_all_plot)\n\n\n\n\n\n\n\n\n\nCode\ndf_anxiety_all_plot|&gt; \n  kbl(format = \"html\")\n\n\n\n\n\n\nestimate\nlower_ci\nupper_ci\n\n\n\n\nE[Y(inactive)]_m\n0.108\n0.010\n0.201\n\n\nE[Y(active)]_m\n0.138\n0.056\n0.214\n\n\nE[Y(very_active)]_m\n0.134\n0.014\n0.264\n\n\nRD_m\n0.027\n-0.114\n0.188\n\n\nE[Y(inactive)]_e\n0.034\n0.002\n0.066\n\n\nE[Y(active)]_e\n-0.021\n-0.046\n0.002\n\n\nE[Y(very_active)]_e\n-0.043\n-0.085\n0.004\n\n\nRD_e\n-0.077\n-0.131\n-0.022\n\n\nRD_m - RD_e\n0.104\n-0.042\n0.279"
  },
  {
    "objectID": "scripts/experiment_template.html#depression",
    "href": "scripts/experiment_template.html#depression",
    "title": "experiment_template.qmd",
    "section": "Depression",
    "text": "Depression\n\n\nCode\n### SUBGROUP analysis\ndf &lt;-  dt_ref_all\nY &lt;-  \"t2_kessler_latent_depression_z\"\nX &lt;- \"t1_hours_exercise_coarsen\" # already defined above\nbaseline_vars = baseline_vars_reflective_propensity\ntreat_0 = \"inactive\"\ntreat_1 = \"very_active\"\nestimand = \"ATE\"\nscale = \"RD\"\nnsims = 1000\nfamily = \"gaussian\"\ncontinuous_X = FALSE\nsplines = FALSE\ncores = parallel::detectCores()\nS = \"t0_eth_cat\"\n\n# not we interact the subclass X treatment X covariates\n\nformula_str &lt;-\n  paste(\n    Y,\n    \"~\",\n    S,\n    \"*\",\n    \"(\",\n    X ,\n    \"*\",\n    \"(\",\n    paste(baseline_vars_reflective_propensity, collapse = \"+\"),\n    \")\",\n    \")\"\n  )\n\n# fit model\nfit_all_dep  &lt;- glm(\n  as.formula(formula_str),\n  weights = weights,\n  # weights = if (!is.null(weight_var)) weight_var else NULL,\n  family = family,\n  data = df\n)\n\n\n# coefs &lt;- coef(fit_all_dep)\n# table(is.na(coefs))#   \n# insight::get_varcov(fit_all_all)\n\n# simulate coefficients\nconflicts_prefer(clarify::sim)\nsim_model_all &lt;- sim(fit_all_dep, n = nsims, vcov = \"HC1\")\n\n\n# simulate effect as modified in europeans\nsim_estimand_all_e_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"euro\",\n  verbose = FALSE\n)\n\n\n# note contrast of interest\nsim_estimand_all_e_d &lt;-\n  transform(sim_estimand_all_e_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# simulate effect as modified in māori\nsim_estimand_all_m_d &lt;- sim_ame(\n  sim_model_all,\n  var = X,\n  cl = cores,\n  subset = t0_eth_cat == \"māori\",\n  verbose = FALSE\n)\n\n# combine\nsim_estimand_all_m_d &lt;-\n  transform(sim_estimand_all_m_d, RD = `E[Y(inactive)]` - `E[Y(very_active)]`)\n\n\n# summary\n#summary(sim_estimand_all_e_d)\n#summary(sim_estimand_all_m_d)\n\n# rearrange\nnames(sim_estimand_all_e_d) &lt;-\n  paste(names(sim_estimand_all_e_d), \"e\", sep = \"_\")\n\nnames(sim_estimand_all_m_d) &lt;-\n  paste(names(sim_estimand_all_m_d), \"m\", sep = \"_\")\n\n\nest_all_d &lt;- cbind(sim_estimand_all_m_d, sim_estimand_all_e_d)\nest_all_d &lt;- transform(est_all_d, `RD_m - RD_e` = RD_m - RD_e)\nsaveRDS(est_all_d, here::here(\"data\", \"est_all_d\"))\n\n\n\n\nCode\nest_all_d &lt;- readRDS( here::here(\"data\", \"est_all_d\"))\n\n        \n# make dataframe\ndf_dep &lt;- data.frame( summary(est_all_d) )\n\ntable_estimates_depression &lt;- df_dep |&gt; \n    filter(row.names(df_dep) %in% c(\"RD_m\", \"RD_e\")) |&gt; \n    rename(lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) |&gt; \n # dplyr::mutate(standard_error = abs(`2.5 %` - `97.5 %`) / 3.92) |&gt; \n  dplyr::mutate(across(where(is.numeric), round, digits = 3))\n\n\n# note that I made a function to calculate the Evalue, load this with \"experimental functions\"\ntable_depression &lt;- tab_ate_subgroup_rd(table_estimates_depression, delta = 1, sd = 1)\n\n\nConfidence interval crosses the true value, so its E-value is 1.\nConfidence interval crosses the true value, so its E-value is 1.\n\n\nCode\ntable_depression |&gt; kbl(format = \"markdown\")\n\n\n\n\n\n\nE[Y(1)]-E[Y(0)]\nlower_ci\nupper_ci\nE_Value\nE_Val_bound\n\n\n\n\nRD_m\n0.028\n-0.120\n0.185\n1.189\n1\n\n\nRD_e\n-0.039\n-0.103\n0.021\n1.230\n1\n\n\n\n\n\nSummary\n\n\nCode\n# view summary\ndf_dep |&gt; \n  kbl(format = \"html\")\n\n\n\n\n\n\nEstimate\nX2.5..\nX97.5..\n\n\n\n\nE[Y(inactive)]_m\n0.1510358\n0.0537454\n0.2442848\n\n\nE[Y(active)]_m\n0.1877594\n0.1031401\n0.2866124\n\n\nE[Y(very_active)]_m\n0.1793980\n0.0718405\n0.2990333\n\n\nRD_m\n0.0283622\n-0.1197312\n0.1854479\n\n\nE[Y(inactive)]_e\n0.0035095\n-0.0324475\n0.0411906\n\n\nE[Y(active)]_e\n-0.0168208\n-0.0404293\n0.0071173\n\n\nE[Y(very_active)]_e\n-0.0358775\n-0.0825212\n0.0120629\n\n\nRD_e\n-0.0393869\n-0.1027416\n0.0212851\n\n\nRD_m - RD_e\n0.0677491\n-0.0902557\n0.2292381\n\n\n\n\n\n\n\nCode\n# This table provides estimated levels of depression, in standard deviation units, for different levels of activity for two groups: Māori (indicated by \"_m\") and NZ Europeans (indicated by \"_e\").\n#\n# The expectations are named as `E[Y(&lt;level of activity&gt;)]_group`, where the level of activity can be `inactive`, `active`, or `very_active`.\n#\n# Here is a breakdown of the results.\n#\n#   1. For the Māori group (`_m`):\n#\n#   - `E[Y(inactive)]_m`: When inactive, the expected level of depression is 0.23 standard deviations, with a 95% confidence interval from 0.116 to 0.356.\n# - `E[Y(active)]_m`: When active, the expected level of depression decreases to 0.193 standard deviations, with a 95% confidence interval from 0.108 to 0.282.\n# - `E[Y(very_active)]_m`: When very active, the expected level of depression further decreases to 0.133 standard deviations, with a 95% confidence interval from 0.009 to 0.262.\n# - `RD_m`: The risk difference (RD) between inactive and very active Māori individuals is 0.097 standard deviations, with a 95% confidence interval from -0.068 to 0.274. This indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# 2. For the NZ European group (`_e`):\n#\n#   - `E[Y(inactive)]_e`: When inactive, the expected level of depression is 0.034 standard deviations, with a 95% confidence interval from -0.012 to 0.078.\n# - `E[Y(active)]_e`: When active, the expected level of depression slightly decreases to -0.006 standard deviations, with a 95% confidence interval from -0.03 to 0.016.\n# - `E[Y(very_active)]_e`: When very active, the expected level of depression further decreases to -0.046 standard deviations, with a 95% confidence interval from -0.086 to -0.007.\n# - `RD_e`: The risk difference (RD) between inactive and very active NZ European individuals is 0.081 standard deviations, with a 95% confidence interval from 0.02 to 0.138. Similar to the Māori group, this indicates a decrease in depression when individuals move from an inactive to a very active lifestyle.\n#\n# The last row, `RD_m - RD_e`, represents the difference in risk differences between Māori and NZ Europeans. It's 0.017 standard deviations with a 95% confidence interval from -0.152 to 0.204. This is not statistically significant (the confidence interval contains 0), suggesting that the difference in depression reduction from being inactive to very active is not significantly different between the two groups.\n#\n# These are estimates and subject to statistical uncertainty. While they suggest a trend, the wide confidence intervals indicate that these estimates come with a degree of uncertainty.\n\n\n\n\nCode\ndf_dep_plot_data &lt;- df_dep |&gt; \n rename( lower_ci = `X2.5..`,\n         upper_ci = `X97.5..`,\n         estimate = Estimate) \n\n\ndf_dep\n\n\n                        Estimate      X2.5..     X97.5..\nE[Y(inactive)]_m     0.151035788  0.05374538 0.244284755\nE[Y(active)]_m       0.187759424  0.10314013 0.286612442\nE[Y(very_active)]_m  0.179397991  0.07184045 0.299033259\nRD_m                 0.028362203 -0.11973124 0.185447917\nE[Y(inactive)]_e     0.003509459 -0.03244753 0.041190620\nE[Y(active)]_e      -0.016820762 -0.04042933 0.007117345\nE[Y(very_active)]_e -0.035877479 -0.08252119 0.012062852\nRD_e                -0.039386939 -0.10274163 0.021285115\nRD_m - RD_e          0.067749141 -0.09025568 0.229238107\n\n\nCode\nplot_sub_forest &lt;- function(df) {\n  require(ggplot2)\n\n  # Check if required packages are installed\n  required_packages &lt;- c(\"ggplot2\")\n  new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[, \"Package\"])]\n  if (length(new_packages))\n    stop(\"Missing packages: \", paste(new_packages, collapse = \", \"))\n  \n  # Check if required columns are in the dataframe\n  required_cols &lt;- c(\"estimate\", \"lower_ci\", \"upper_ci\")\n  missing_cols &lt;- required_cols[!(required_cols %in% colnames(df))]\n  if (length(missing_cols) &gt; 0)\n    stop(\"Missing columns in dataframe: \", paste(missing_cols, collapse = \", \"))\n  \n  # Order the factor levels by the estimate column in decreasing order\n  \n  ggplot(df, aes(x=estimate, y=factor(row.names(df)))) +\n    geom_point() +\n    geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height=0.3) +\n    geom_vline(xintercept = 0, linetype=\"dashed\", color = \"red\") +\n    theme_bw() +\n    xlab(\"Estimate\") +\n    ylab(\"\")\n}\nplot_sub_forest(df_dep_plot_data)"
  },
  {
    "objectID": "scripts/test-graphs.html",
    "href": "scripts/test-graphs.html",
    "title": "TEST GRAPHS",
    "section": "",
    "text": "(a) Reflective model: assume univariate latent variable η giving rise to indicators X1…X3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Reflective Model: causal assumptions\n\n\n\n\n\n\n\n\nFigure 1: Reflective model & causal assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Formative model:: assume univariate latent variable from which the indicators X1…X3 give rise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Formative model: causal assumptions\n\n\n\n\n\n\n\n\nFigure 2: Formative model & causal assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Formative model is compatible with indicators causing outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Reflective model is compatible with indicators causing the outcome\n\n\n\n\n\n\n\n\nFigure 3: Formative and Reflective Models are compatible with indicators having causal effects"
  },
  {
    "objectID": "scripts/test-graphs.html#measurement-graphs",
    "href": "scripts/test-graphs.html#measurement-graphs",
    "title": "TEST GRAPHS",
    "section": "",
    "text": "(a) Reflective model: assume univariate latent variable η giving rise to indicators X1…X3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Reflective Model: causal assumptions\n\n\n\n\n\n\n\n\nFigure 1: Reflective model & causal assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Formative model:: assume univariate latent variable from which the indicators X1…X3 give rise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Formative model: causal assumptions\n\n\n\n\n\n\n\n\nFigure 2: Formative model & causal assumptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Formative model is compatible with indicators causing outcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Reflective model is compatible with indicators causing the outcome\n\n\n\n\n\n\n\n\nFigure 3: Formative and Reflective Models are compatible with indicators having causal effects"
  },
  {
    "objectID": "scripts/test-graphs.html#formative-measurement-model",
    "href": "scripts/test-graphs.html#formative-measurement-model",
    "title": "TEST GRAPHS",
    "section": "",
    "text": "(b) Formative model: causal assumptions"
  },
  {
    "objectID": "scripts/test-graphs.html#more-complex-models-is-this-a-causal-graph",
    "href": "scripts/test-graphs.html#more-complex-models-is-this-a-causal-graph",
    "title": "TEST GRAPHS",
    "section": "More complex models (is this a causal graph?)",
    "text": "More complex models (is this a causal graph?)\nVanderWeele writes this:\n\n\n\n\n\n\n\n\nFigure 4: Multivariate reality gives rise to the indicators, from which we draw our measure.r\n\n\n\n\n\nRather, I would prefer to say A$ = f(X_1, X_n)$, and to make the point we write:\n\n\n\n\n\n\n\n\nFigure 5: Multivariate reality gives rise to the indicators, from which we draw our measure.r"
  },
  {
    "objectID": "scripts/test-graphs.html#vanderweeles-model-of-reality-is-this-a-causal-diagramme",
    "href": "scripts/test-graphs.html#vanderweeles-model-of-reality-is-this-a-causal-diagramme",
    "title": "TEST GRAPHS",
    "section": "VanderWeele’s model of reality (is this a causal diagramme?)",
    "text": "VanderWeele’s model of reality (is this a causal diagramme?)\n\n\n\n\n\n\n\n\nFigure 6: Multivariate reality gives rise to the latent variables,…\n\n\n\n\n\nHowever this is not a causal graph. Can we do better?"
  },
  {
    "objectID": "scripts/test-graphs.html#perhaps-this-is-better",
    "href": "scripts/test-graphs.html#perhaps-this-is-better",
    "title": "TEST GRAPHS",
    "section": "Perhaps this is better",
    "text": "Perhaps this is better\n\n\n\n\n\n\n\n\nFigure 7: Multivariate reality gives rise to the latent variables,if reality continues to strongly effect η at t0 …tn, there is an open backdoor path to Y through our concepts and constructs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PSYC 434 Conducting Research Across Cultures: Trimester 1, 2024",
    "section": "",
    "text": "ReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "content/01-content.html#lecture-introduction-to-the-course",
    "href": "content/01-content.html#lecture-introduction-to-the-course",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Lecture: Introduction to the Course",
    "text": "Lecture: Introduction to the Course"
  },
  {
    "objectID": "content/01-content.html#lab-introduction-to-r.",
    "href": "content/01-content.html#lab-introduction-to-r.",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Lab: Introduction to R.",
    "text": "Lab: Introduction to R."
  },
  {
    "objectID": "content/01-content.html#introduction",
    "href": "content/01-content.html#introduction",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Introduction",
    "text": "Introduction\nThis session is designed to introduce you to R and RStudio."
  },
  {
    "objectID": "content/01-content.html#installing-r",
    "href": "content/01-content.html#installing-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Installing R",
    "text": "Installing R\n\nVisit the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/.\nSelect the version of R suitable for your operating system (Windows, Mac, or Linux).\nDownload and install it by following the on-screen instructions."
  },
  {
    "objectID": "content/01-content.html#installing-rstudio",
    "href": "content/01-content.html#installing-rstudio",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nAfter downloading R…\nSee Johannes Karl’s Video\n\nStep 1: Install R-Studio\nIf you have already downloaded and installed R from the Comprehensive R Archive Network (CRAN):\n\nGo to the RStudio download page at https://www.rstudio.com/products/rstudio/download/\nChoose the free version of RStudio Desktop, and download it for your operating system.\nDownload and install RStudio Desktop.\nOpen RStudio to begin setting up your project environment.\n\n\n\nStep 2: Create a new project\n\nIn RStudio, go to File &gt; New Project.\nChoose New Directory for a new project or Existing Directory if you have a folder where you want to initialise an RStudio project.\nFor a new project, select New Project, then provide a directory name. This name will also be the name of your project.\nSpecify the location where the project folder will be created.\nClick Create Project.\n\n\n\n\n\n\n\nOrder your R-studio/R workflow\n\n\n\n\nClear folder structure\nIf you are using GitHub (or similar) create a location on your machine (i.d. not dropbox)\nIf you are not using GitHub choose the cloud (Dropbox or similar).\nWhen creating new files and scripts, use clear labels that anyone could understand.\nThat “anyone” will be your future self, trying to make sense.\n\n\n\n\n\nStep 3: Give project structure\n\nOrganising Files and Folders:\n\nWithin your project, create folders to organise your scripts and data.\nCommon folder names include R/ for R scripts, data/ for datasets, and doc/ for documentation.\nYou can create these folders using RStudio’s Files pane or through your operating system’s file explorer.\n\nCreating and Managing R Scripts:\n\nTo create a new R script, go to File &gt; New File &gt; R Script.\nSave the script in your project directory’s R/ folder to keep your work organised. Use meaningful file names that describe the script’s purpose.\n\nVersion Control:\n\nIf you are familiar with version control, you can initialise a Git repository within your project by selecting the Version Control option when creating a new project.\nThis allows for better tracking of changes and collaboration if working with others.\nIf you are not familiar with version control (or have not installed git on your machine), do not worry about initialising a Git repository.\n\n\n\n\nStep 4: Working with R-scripts\n\nWriting and executing Code:\n\nWrite your R code in the script editor.\nExecute code by selecting lines and pressing Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac).\n\nCommenting and documentation:\n\nUse comments (preceded by #) to document your code for clarity and future reference.\n\nSaving and organising scripts:\n\nRegularly save your scripts (Ctrl + S or Cmd + S).\nOrganise scripts into folders within your project for different analyses or data processing tasks.\n\n\n\n\nStep 5: When you exit R-studio\n\nBefore concluding your work, save your workspace or clear it to start fresh in the next session (Session &gt; Restart R).\n\n\n\n\n\n\n\nOrder your R-studio/R workflow\n\n\n\n\nAgain, use clearly defined script names\nAnnotate your code\nSave your scripts often (Ctrl + S or Cmd + S).\n\n\n\n\n\n\n\n\n\nExercise 1: Install the tidyverse package\n\n\n\nFollow these instructions to install the tidyverse package in RStudio:\n\nOpen RStudio: launch RStudio on your computer.\nAccess package installation:\n\nNavigate to the menu at the top of RStudio and click on Tools &gt; Install Packages.... This opens the Install Packages dialogue box.\n\nInstall tidyverse:\n\nIn the Install Packages dialogue box, you will see a field labelled “Packages (separate multiple with space or comma):”. Click in this field and type tidyverse.\nBelow the packages field, ensure the checkbox for Install dependencies is checked. This ensures all packages that tidyverse depends on are also installed.\n\nBegin installation:\n\nClick on the Install button to start the installation process.\n\n\nThe installation might take a few minutes. Monitor the progress in the “Console” pane. Once the installation is complete, you will see a message in the console indicating that the process has finished.\n\nLoad tidyverse: After successful installation, you can load the tidyverse package into your R session by typing library(tidyverse) in the console and pressing Enter."
  },
  {
    "objectID": "content/01-content.html#familiarizing-yourself-with-rstudio",
    "href": "content/01-content.html#familiarizing-yourself-with-rstudio",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Familiarizing Yourself with RStudio",
    "text": "Familiarizing Yourself with RStudio\n\nConsole: Executes R code line by line.\nSource Editor: Allows you to write and execute scripts (series of commands).\nEnvironment: Displays variables and data you’ve loaded.\nFiles/Plots/Packages/Help: Allows you to navigate your files, view plots, manage packages, and access R documentation."
  },
  {
    "objectID": "content/01-content.html#basic-r-commands",
    "href": "content/01-content.html#basic-r-commands",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Basic R Commands",
    "text": "Basic R Commands\n\n\n\n\n\n\nHow to copy the code on this page\n\n\n\n\nR Script &gt; New File &gt; R\nName your new R script and save it in a folder.\nHover your cursor over the top right of the code panel, and click the copy tab.\nCopy the text into your script.\nSave: Ctrl + S or Cmd + S.\n\n\n\n\nAssignment (&lt;-)\nAssignment in R is done using the ‘&lt;-’ operator, which on my machine renders &lt;-. This operator assigns values to variables:\n\nx &lt;- 10 # assigns the value 10 to x\ny &lt;- 5 # assigns the value 5 to y\n\n# this does the same\nx &lt;- 10\ny &lt;- 5\n\n# note what happens when we do this\n# 10 = 5 # not run\n\n# but we can do this\n# 10 == 5 # considered below\n\n\n\n\n\n\n\nRStudio Assignment Operator Shortcut\n\n\n\n\nFor macOS: Option + - (minus key) inserts &lt;-.\nFor Windows and Linux: Alt + - (minus key) inserts &lt;-.\n\nConsult the latest RStudio documentation or access the Keyboard Shortcuts Help (Tools -&gt; Keyboard Shortcuts Help) for up-to-date shortcuts.\n\n\n\n\nConcatenation (c())\nThe c() function combines multiple elements into a vector.\n\nnumbers &lt;- c(1, 2, 3, 4, 5) # a vector of numbers\nprint(numbers)\n\n[1] 1 2 3 4 5\n\n\n\n\nOperations (+, -)\nBasic arithmetic operations include addition (+) and subtraction (-).\n\n# this does the same\nx &lt;- 10\ny &lt;- 5\n\nsum &lt;- x + y # adds x and y\n\nprint(sum)\n\n[1] 15\n\ndifference &lt;- x - y # subtracts y from x\n\n# note we did not need to use the `print()` function\ndifference\n\n[1] 5\n\n\n\n\n\n\n\n\nExecuting code\n\n\n\n\nCtrl + Enter (Windows/Linux) or Cmd + Enter (Mac).\n\n\n\nIn addition to assignment, multiplication and division are fundamental arithmetic operations in R that allow you to manipulate numeric data. Here is how you can incorporate these operations into your basic R commands documentation:\n\n\nMultiplication (*) and Division (/)\nMultiplication and division in R are performed using the * and / operators, respectively. These operators allow for element-wise operations on vectors, as well as operations on individual numeric values.\n\n# multiplication\nproduct &lt;- x * y # multiplies x by y\nproduct\n\n[1] 50\n\n# division\nquotient &lt;- x / y # divides x by y\nquotient\n\n[1] 2\n\n# element-wise multiplication on vectors\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n# multiplies each element of vector1 by the corresponding element of vector2\nvector_product &lt;- vector1 * vector2\nvector_product\n\n[1]  4 10 18\n\n# element-wise division on vectors\n# divides each element of vector1 by the corresponding element of vector2\nvector_division &lt;- vector1 / vector2\nvector_division\n\n[1] 0.25 0.40 0.50\n\n\n\nMultiplication and division can be used for scalar (single values) and vector (multiple values) operations. When applied to vectors, these operations are performed element-wise.\nBe mindful of division by zero, as this will result in Inf (infinity) or NaN (not a number) depending on the context.\n\n\n# example of division by zero\nresult &lt;- 10 / 0 # results in Inf\nzero_division &lt;- 0 / 0 # results in NaN\n\n\nR also supports integer division using the %/% operator and modulo operation using %% to find the remainder.\n\n\n# integer division\ninteger_division &lt;- 10 %/% 3 # results in 3\n\n# modulo operation\nremainder &lt;- 10 %% 3 # results in 1\n\n\n\nrm() Remove Object\n\n# `rm()` remove object ----------------------------------------------------\ndevil_number &lt;- 666 # results in 1\n\n# view\ndevil_number\n\n[1] 666\n\n# remove the devil number\nrm(devil_number)\n\n# check\n# devil_number\n\n\n\nLogic (!, !=, ==)\nLogical operations include NOT (!), NOT EQUAL (!=), and EQUAL (==).\n\nx_not_y &lt;- x != y # checks if x is not equal to y\nx_not_y\n\n[1] TRUE\n\nx_equal_10 &lt;- x == 10 # checks if x is equal to 10\nx_equal_10\n\n[1] TRUE\n\n\nLogical operations are fundamental in R for controlling the flow of execution and making decisions based on conditions. In addition to NOT (!), NOT EQUAL (!=), and EQUAL (==), there are several other logical operators you should know:\n\n\nOR (| and ||)\n\nThe | operator performs element-wise logical OR operation. It evaluates each pair of elements in two logical vectors to see if at least one is TRUE.\nThe || operator performs a logical OR operation but only evaluates the first element of each vector – mainly used in if statements and not for vectorised operations.\n\n\n# element-wise OR\nvector_or &lt;- c(TRUE, FALSE) | c(FALSE, TRUE) # returns c(TRUE, TRUE)\nvector_or\n\n[1] TRUE TRUE\n\n# single OR (only looks at first element)\nsingle_or &lt;- TRUE || FALSE # returns TRUE\nsingle_or\n\n[1] TRUE\n\n\n\n\nAND (& and &&)\n\nThe & operator performs element-wise logical AND operations. It checks if both elements in the corresponding positions of two logical vectors are TRUE.\nThe && operator performs a logical AND operation but only evaluates the first element of each vector. Like ||, used in conditions that do not require vectorised operations.\n\n\n# element-wise AND\nvector_and &lt;- c(TRUE, FALSE) & c(FALSE, TRUE) # returns c(FALSE, FALSE)\n\n# single AND (only looks at first element)\nsingle_and &lt;- TRUE && FALSE # returns FALSE\n\n\n\n\n\n\n\nRStudio Workflow Shortcuts\n\n\n\nShortcuts bring order and boost creativity\n\nExecute Code Line: Cmd + Return (Mac) or Ctrl + Enter (Windows/Linux)\nInsert Section Heading: Cmd + Shift + R (Mac) or Ctrl + Shift + R (Windows/Linux)\nAlign Code: Cmd + Shift + A (Mac) or Ctrl + Shift + A (Windows/Linux)\nComment/Uncomment: Cmd/Ctrl + Shift + C\nSave All: Cmd/Ctrl + Shift + S\nFind/Replace: Cmd/Ctrl + F, Cmd/Ctrl + Shift + F\nNew File: Cmd/Ctrl + Shift + N\nAuto-complete: Tab\n\nFor more commands, explore the Command Palette available under Tools -&gt; Command Palette or Shift + Cmd + P (Mac) or Shift + Ctrl + P (Windows/Linux)."
  },
  {
    "objectID": "content/01-content.html#simulating-data-in-r",
    "href": "content/01-content.html#simulating-data-in-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Simulating Data in R",
    "text": "Simulating Data in R\nSimulating data is a powerful method to understand statistical concepts and data manipulation. Let’s simulate a simple dataset representing scores from two cultural groups.\n\nStep 1: Setting Up Your R Environment\nBefore simulating data, ensure R or RStudio is installed and open. RStudio provides a user-friendly interface for R, which can simplify the process of writing and executing R scripts.\n\n\nStep 2: Setting a Seed for Reproducibility\nTo ensure that your simulated data can be reproduced exactly, it’s good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\nset.seed(123) # use any number to set the seed\n\n\nStep 3: Simulating Continuous Data\nTo simulate continuous data, you can use functions like rnorm() for normal distributions, runif() for uniform distributions, etc. Here’s how to simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\n\nn &lt;- 100 # number of observations\nmean &lt;- 50\nsd &lt;- 10\ndata_continuous &lt;- rnorm(n, mean, sd)\n\n\n\nStep 4: Simulating Categorical Data\nCategorical data can be simulated using the sample() function. For example, to simulate a binary variable (e.g., gender) with two levels for 100 observations:\n\nlevels &lt;- c(\"Male\", \"Female\")\ndata_categorical &lt;- sample(levels, n, replace = TRUE)\n\n\n\nStep 5: Simulating Data Frames\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\n\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame &lt;- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\n\nNote that you can sample probabilistically for your groups\n\nn &lt;- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))\n\n\n\nStep 6: Add Complexity\nTo simulate more complex datasets, you can introduce relationships between variables. For instance, simulating age and income with a correlation:\n\n# set the number of observations\nn &lt;- 100\n\n# simulate the 'Age' variable\nmean_age &lt;- 30\nsd_age &lt;- 5\nAge &lt;- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept &lt;- 20000   # Intercept for the income equation\nbeta_age &lt;- 1500     # Coefficient for the effect of age on income\nerror_sd &lt;- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome &lt;- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex &lt;- data.frame(Age, Income)\n\n\n\nStep 7: Visualising Simulated Data\nVisualising your simulated data can help understand its distribution and relationships. Use the ggplot2 package for this:\n\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")"
  },
  {
    "objectID": "content/01-content.html#visualizing-simulated-data",
    "href": "content/01-content.html#visualizing-simulated-data",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Visualizing Simulated Data",
    "text": "Visualizing Simulated Data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n\nlibrary(ggplot2)\nggplot(scores_df, aes(x = Group, y = Scores, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n\n\n\n\nScore Distribution by Group"
  },
  {
    "objectID": "content/01-content.html#exercises",
    "href": "content/01-content.html#exercises",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Exercises",
    "text": "Exercises\n\nModify the simulation parameters to change the mean and standard deviation for each group. Observe how these changes affect the distribution.\nCreate a new variable in scores_df that categorizes scores into ‘High’ and ‘Low’ based on a threshold you define. Use the ifelse function.\nGenerate a histogram for each group to visualize the frequency of scores. Experiment with different bin widths."
  },
  {
    "objectID": "content/01-content.html#conclusion",
    "href": "content/01-content.html#conclusion",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conclusion",
    "text": "Conclusion\nThis session introduced you to R and RStudio, covered basic R commands, and demonstrated how to simulate and visualize data. These skills will serve as the foundation for more advanced data analysis techniques in cross-cultural psychology."
  },
  {
    "objectID": "content/01-content.html#recommended-reading",
    "href": "content/01-content.html#recommended-reading",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Recommended Reading",
    "text": "Recommended Reading\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/ For now, only lecture 1 would be relevant. It’s worth noting that this lecture covers working with GitHub. GitHub is a useful tool, but it’s not a requirement for the current course."
  },
  {
    "objectID": "content/01-content.html#visualising-simulated-data",
    "href": "content/01-content.html#visualising-simulated-data",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Visualising simulated data",
    "text": "Visualising simulated data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# plot your data\nggplot(scores_df, aes(x = Group, y = Scores, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n\n\n\n\nScore Distribution by Group"
  },
  {
    "objectID": "content/01-content.html#histogram",
    "href": "content/01-content.html#histogram",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Histogram",
    "text": "Histogram\n\nlibrary(ggplot2)\n\n# Histograms for both groups\nggplot(scores_df, aes(x = Scores, fill = Group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~Group, ncol = 1)"
  },
  {
    "objectID": "content/01-content.html#simulating-data-for-familar-statistical-tests",
    "href": "content/01-content.html#simulating-data-for-familar-statistical-tests",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Simulating data for familar statistical tests",
    "text": "Simulating data for familar statistical tests\n\n# simulate some data\ndata &lt;- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nt.test(data, mu = 4)\n\n\n    One Sample t-test\n\ndata:  data\nt = 11.796, df = 99, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.931989 5.308942\nsample estimates:\nmean of x \n 5.120465 \n\n\n\n# simulate data for two groups\ngroup1 &lt;- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 &lt;- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nt.test(group1, group2)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -2.0293, df = 97.95, p-value = 0.04514\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.837548023 -0.009343886\nsample estimates:\nmean of x mean of y \n 5.002054  5.425500 \n\n\n\n# simulate pre-test and post-test scores\npre_test &lt;- rnorm(30, mean = 80, sd = 10)\npost_test &lt;- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nt.test(pre_test, post_test, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pre_test and post_test\nt = -4.7761, df = 29, p-value = 4.725e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -6.785042 -2.716352\nsample estimates:\nmean difference \n      -4.750697"
  },
  {
    "objectID": "content/01-content.html#hands-on-exercise-answer-at-the-bottom-of-this-page",
    "href": "content/01-content.html#hands-on-exercise-answer-at-the-bottom-of-this-page",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Hands on Exercise (Answer at the bottom of this page)",
    "text": "Hands on Exercise (Answer at the bottom of this page)\n\nModify the simulation parameters to change the mean and standard deviation for each group. Observe how these changes affect the distribution.\nGenerate a histogram for each group to visualize the frequency of scores. Experiment with different bin widths."
  },
  {
    "objectID": "content/01-content.html#sec-histogram",
    "href": "content/01-content.html#sec-histogram",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Histogram",
    "text": "Histogram\n\nlibrary(ggplot2)\n\n# H=histograms for both groups\nggplot(scores_df, aes(x = Scores, fill = Group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~Group, ncol = 1)"
  },
  {
    "objectID": "content/01-content.html#equivalence-of-anova-and-regression",
    "href": "content/01-content.html#equivalence-of-anova-and-regression",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Equivalence of ANOVA and Regression",
    "text": "Equivalence of ANOVA and Regression\nWe will simulate data in R to show that a one-way ANOVA is a special case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings)."
  },
  {
    "objectID": "content/01-content.html#method",
    "href": "content/01-content.html#method",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Method",
    "text": "Method\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a “dependant”) variable. This setup allows us to apply both ANOVA and linear regression for comparison.\n\n# nice tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n\nset.seed(321) # reproducibility\nn &lt;- 90 # total number of observations\nk &lt;- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup &lt;- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n\n# simulate outcome variable\nmeans &lt;- c(100, 100, 220) # Mean for each group\nsd &lt;- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny &lt;- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 &lt;- cbind.data.frame(y, group)\n\nanova_model &lt;- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova &lt;- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\n\nThe ANOVA (formula: y ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 87) =\n689.11, p &lt; .001; Eta2 = 0.94, 95% CI [0.92, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\n\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit &lt;- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit &lt;- parameters::model_parameters(fit)\n\n# print table\ntable_fit\n\nParameter   | Coefficient |   SE |           95% CI | t(87) |      p\n--------------------------------------------------------------------\n(Intercept) |      101.22 | 2.60 | [ 96.06, 106.39] | 38.98 | &lt; .001\ngroup [2]   |       -0.80 | 3.67 | [ -8.10,   6.50] | -0.22 | 0.827 \ngroup [3]   |      117.67 | 3.67 | [110.37, 124.97] | 32.04 | &lt; .001\n\n\n\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit &lt;- report_parameters(fit)\n\n#print\nreport_fit\n\n  - The intercept is statistically significant and positive (beta = 101.22, 95% CI [96.06, 106.39], t(87) = 38.98, p &lt; .001; Std. beta = -0.68, 95% CI [-0.76, -0.59])\n  - The effect of group [2] is statistically non-significant and negative (beta = -0.80, 95% CI [-8.10, 6.50], t(87) = -0.22, p = 0.827; Std. beta = -0.01, 95% CI [-0.14, 0.11])\n  - The effect of group [3] is statistically significant and positive (beta = 117.67, 95% CI [110.37, 124.97], t(87) = 32.04, p &lt; .001; Std. beta = 2.04, 95% CI [1.91, 2.17])"
  },
  {
    "objectID": "content/01-content.html#upshot",
    "href": "content/01-content.html#upshot",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Upshot",
    "text": "Upshot\nANOVA partitions variance into between-group and within-group components, while regression models the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when we are comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain understanding. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, Along the way, you’ll learn more about data visualisation, modelling, and reporting.\n\n# graph the output of the parameters table\n# visualisation\nplot(table_fit)\n\n\n\n\n\n\n\n\n\nExercise 3\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\nSimulate Data:\n\nGenerate two continuous variables, Y and A, with n = 100 observations each.\nThe variable A should have a mean of 50 and a standard deviation (sd) of 10.\n\nDefine the Relationship:\n\nSimulate the variable Y such that it is linearly related to A with a specified effect size. The effect size of A on Y must be explicitly defined as 2.\n\nIncorporate an Error Term:\n\nWhen simulating Y, include an error term with a standard deviation (sd) of 20 to introduce variability.\n\nRegression Analysis:\n\nUse the lm() function in R to regress Y on A.\nEnsure the regression model captures the specified effect of A on Y.\n\nReport the Results:\n\nOutput the regression model summary to examine the coefficients, including the effect of A on Y, and assess the model’s overall fit and significance.\n\n\nHere’s a template to get you started\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn &lt;-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A &lt;-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)"
  },
  {
    "objectID": "content/01-content.html#for-more-information-about-packages",
    "href": "content/01-content.html#for-more-information-about-packages",
    "title": "Asking questions in cross-cultural psychology",
    "section": "For more information about packages",
    "text": "For more information about packages\nggplot2 Parameters package Report package"
  },
  {
    "objectID": "content/01-content.html#further-information-on-packages",
    "href": "content/01-content.html#further-information-on-packages",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Further Information on Packages",
    "text": "Further Information on Packages\nFor comprehensive details and resources on specific packages, visit the following links:\n\nggplot2: A system for declaratively creating graphics, based on The Grammar of Graphics.\nParameters package: Provides utilities for processing model parameters and their metrics.\nReport package: Facilitates the automated generation of reports from statistical models."
  },
  {
    "objectID": "content/01-content.html#getting-help",
    "href": "content/01-content.html#getting-help",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Getting Help",
    "text": "Getting Help\nAs sure as night follows day, you will need help coding. Key resources\n\nLarge Language Models (LLMs): OpenAI’s premium LLM (GPT-4) outperforms the free version (GPT-3.5) for complex queries.\nStack Exchange: a valuable resource for coding advice and solutions.\nDeveloper Websites and GitHub Pages: Directly engage with package developers and the community for insights and support.Parameters package discussion page offers insights and support directly from its developers and user community.\nYour tutors and lecturer. We care. We’re here to help you!"
  },
  {
    "objectID": "content/01-content.html#getting-help-1",
    "href": "content/01-content.html#getting-help-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Getting Help",
    "text": "Getting Help\nAs sure as night follows day, you will need help coding. Key resources\n\nLarge Language Models (LLMs): OpenAI’s premium LLM (GPT-4) outperforms the free version (GPT-3.5) for complex queries.\nStack Exchange: a valuable resource for coding advice and solutions.\nDeveloper Websites and GitHub Pages: Directly engage with package developers and the community for insights and support.\nYour tutors and lecturer. We care.We’re here to help you."
  },
  {
    "objectID": "content/01-content.html#hands-on-exercises",
    "href": "content/01-content.html#hands-on-exercises",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Hands on Exercises",
    "text": "Hands on Exercises\n\nModify the simulation parameters to change the mean and standard deviation for each group. Observe how these changes affect the distribution.\nInstall the tidyverse package on your machine doing the following steps:\n\n\nGo to the Rstudio help tab\nSearch install packages\nType in tidyverse\nClick “install” (make sure the intall dependencies box is ticked)\n\n\nGo to the histogram. Generate a histogram for each group to visualise the frequency of scores. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/01-content.html#packages",
    "href": "content/01-content.html#packages",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Packages",
    "text": "Packages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Xie Y (2023). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.49, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/01-content.html#what-youve-learned",
    "href": "content/01-content.html#what-youve-learned",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What you’ve learned",
    "text": "What you’ve learned\n\nInstallation and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nNavigating RStudio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practiced using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nData simulation:\n\nYou’ve learned to simulate datasets in R, a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations!\n\nData visualisation:\n\nYou’ve begun visualising data through boxplots and histograms and coefficient plots, which is crucial for analysing and communicating statistical findings.\n\nStatistical tests: You’ve conducted basic statistical tests, including t-tests and ANOVA, gaining insights into comparing means across groups.\nUnderstanding ANOVA and fegression:\n\nYou’ve explored the equivalence of ANOVA and regression analysis, learning how these methods can be applied to analyse and interpret data effectively."
  },
  {
    "objectID": "content/01-content.html#practice",
    "href": "content/01-content.html#practice",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Practice",
    "text": "Practice\n\nset.seed(123) #  reproducibility\ngroupA_scores &lt;- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores &lt;- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\nscores_df &lt;- data.frame(Group = rep(c(\"A\", \"B\"), each = 100), Scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\nstr(scores_df)\n\n'data.frame':   200 obs. of  2 variables:\n $ Group : chr  \"A\" \"A\" \"A\" \"A\" ...\n $ Scores: num  91.6 96.5 123.4 101.1 101.9 ...\n\n# summary of columns\nsummary(scores_df)\n\n    Group               Scores      \n Length:200         Min.   : 65.36  \n Class :character   1st Qu.: 92.59  \n Mode  :character   Median :101.38  \n                    Mean   :102.37  \n                    3rd Qu.:111.59  \n                    Max.   :153.62  \n\n# top rows\nhead(scores_df)\n\n  Group    Scores\n1     A  91.59287\n2     A  96.54734\n3     A 123.38062\n4     A 101.05763\n5     A 101.93932\n6     A 125.72597\n\n# bottom rows\ntail(scores_df)\n\n    Group    Scores\n195     B  85.33798\n196     B 134.95820\n197     B 114.01063\n198     B  86.23093\n199     B  95.83251\n200     B  87.21780"
  },
  {
    "objectID": "content/01-content.html#excercise-1",
    "href": "content/01-content.html#excercise-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Excercise 1",
    "text": "Excercise 1\n\nModify the simulation parameters to change each group’s mean and standard deviation. Observe how these changes affect the distribution.\nGo to the histogram. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/01-content.html#here-is-what-you-have-learned",
    "href": "content/01-content.html#here-is-what-you-have-learned",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Here is what you have learned",
    "text": "Here is what you have learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use RStudio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations!\n\nData visualisation:\n\nYou’ve begun data visualising data through boxplots and histograms and coefficient plots, which is crucial for analysing and communicating statistical findings.\n\nStatistical tests: You’ve conducted basic statistical tests, including t-tests and ANOVA, gaining insights into comparing means across groups.\nUnderstanding ANOVA and regression:\n\nYou’ve explored the equivalence of ANOVA and regression analysis, learning how these methods can be applied to analyse and interpret data effectively."
  },
  {
    "objectID": "content/01-content.html#appendix-a",
    "href": "content/01-content.html#appendix-a",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix A: Solutions to Problem Sets",
    "text": "Appendix A: Solutions to Problem Sets\n\nSolution Problem Set 3: simulate data and regression reporting\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed(12345)\n\n# number of observations\nn &lt;- 100\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, mean = 50, sd = 10)\n\n# define the specified effect size of A on Y\nbeta_A &lt;- 2\n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A =  rnorm(n, mean = 50, sd = 10),\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20)\n)\n\n# view\nhead(df_3)\n\n         A         Y\n1 52.23925  87.98766\n2 38.43777 106.60413\n3 54.22419 107.68437\n4 36.75245 117.09730\n5 51.41084 133.74473\n6 44.63952  70.74512\n\nstr(df_3)\n\n'data.frame':   100 obs. of  2 variables:\n $ A: num  52.2 38.4 54.2 36.8 51.4 ...\n $ Y: num  88 107 108 117 134 ...\n\n# Perform linear regression of Y on A\n\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n# Report the results of the regression\n# summary(model)\n\n# report\nparameters::model_parameters(fit_3)\n\nParameter   | Coefficient |    SE |          95% CI | t(98) |      p\n--------------------------------------------------------------------\n(Intercept) |      109.17 | 14.62 | [80.17, 138.18] |  7.47 | &lt; .001\nA           |   -3.80e-03 |  0.28 | [-0.57,   0.56] | -0.01 | 0.989 \n\nreport(fit_3)\n\nWe fitted a linear model (estimated using OLS) to predict Y with A (formula: Y\n~ A). The model explains a statistically not significant and very weak\nproportion of variance (R2 = 1.83e-06, F(1, 98) = 1.79e-04, p = 0.989, adj. R2\n= -0.01). The model's intercept, corresponding to A = 0, is at 109.17 (95% CI\n[80.17, 138.18], t(98) = 7.47, p &lt; .001). Within this model:\n\n  - The effect of A is statistically non-significant and negative (beta =\n-3.80e-03, 95% CI [-0.57, 0.56], t(98) = -0.01, p = 0.989; Std. beta =\n-1.35e-03, 95% CI [-0.20, 0.20])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/01-content.html#appendix-b-i-lied",
    "href": "content/01-content.html#appendix-b-i-lied",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix B: I lied",
    "text": "Appendix B: I lied\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/01-content.html#simulating-data-for-familiar-statistical-tests",
    "href": "content/01-content.html#simulating-data-for-familiar-statistical-tests",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Simulating data for familiar statistical tests",
    "text": "Simulating data for familiar statistical tests\n\n# simulate some data\ndata &lt;- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nt.test(data, mu = 4)\n\n\n    One Sample t-test\n\ndata:  data\nt = 11.796, df = 99, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 4\n95 percent confidence interval:\n 4.931989 5.308942\nsample estimates:\nmean of x \n 5.120465 \n\n\n\n# simulate data for two groups\ngroup1 &lt;- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 &lt;- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nt.test(group1, group2)\n\n\n    Welch Two Sample t-test\n\ndata:  group1 and group2\nt = -2.0293, df = 97.95, p-value = 0.04514\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.837548023 -0.009343886\nsample estimates:\nmean of x mean of y \n 5.002054  5.425500 \n\n\n\n# simulate pre-test and post-test scores\npre_test &lt;- rnorm(30, mean = 80, sd = 10)\npost_test &lt;- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nt.test(pre_test, post_test, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pre_test and post_test\nt = -4.7761, df = 29, p-value = 4.725e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -6.785042 -2.716352\nsample estimates:\nmean difference \n      -4.750697"
  },
  {
    "objectID": "content/01-content.html#for-more-information-about-the-packages-used-here",
    "href": "content/01-content.html#for-more-information-about-the-packages-used-here",
    "title": "Asking questions in cross-cultural psychology",
    "section": "For more information about the packages used here:",
    "text": "For more information about the packages used here:\n\nggplot2: A system for declaratively creating graphics, based on The Grammar of Graphics.\nParameters package: Provides utilities for processing model parameters and their metrics.\nReport package: Facilitates the automated generation of reports from statistical models."
  },
  {
    "objectID": "content/01-content.html#what-you-have-learned",
    "href": "content/01-content.html#what-you-have-learned",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What You Have Learned",
    "text": "What You Have Learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use RStudio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nBasic R Data Structures:"
  },
  {
    "objectID": "slides/01-slides.html#age-might-cause-both-marriage-and-happiness",
    "href": "slides/01-slides.html#age-might-cause-both-marriage-and-happiness",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Age might cause both Marriage and Happiness",
    "text": "Age might cause both Marriage and Happiness\n\nThat is Age might be a confounder by common cause"
  },
  {
    "objectID": "content/01-content.html#appendix-a-other-data-types-you-may-encounter",
    "href": "content/01-content.html#appendix-a-other-data-types-you-may-encounter",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix A: Other Data Types You May Encounter",
    "text": "Appendix A: Other Data Types You May Encounter\n\nArrays and Matrices\nArrays are multi-dimensional data structures, while matrices are two-dimensional.\n\nmatrix_1 &lt;- matrix(1:9, nrow = 3) # creates a 3x3 matrix\narray_1 &lt;- array(1:12, dim = c(2, 3, 2)) # creates a 2x3x2 array\n\n\n\nData Frames\nA data.frame is used for storing tabular data.\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Age = c(24, 27))\n\n#check\nstr(df)\n\n'data.frame':   2 obs. of  2 variables:\n $ Name: chr  \"Alice\" \"Bob\"\n $ Age : num  24 27\n\n# change matrix to array:\ndf_matrix_1 &lt;- data.frame( matrix_1 )\n\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ X1: int  1 2 3\n $ X2: int  4 5 6\n $ X3: int  7 8 9\n\nhead(df_matrix_1)\n\n  X1 X2 X3\n1  1  4  7\n2  2  5  8\n3  3  6  9\n\n# change colnames\nnew_colnames &lt;- c(\"col_1\", \"col_2\", \"col_3\")\n\ncolnames(df_matrix_1)&lt;- new_colnames\n\n# check\nstr(df_matrix_1)  \n\n'data.frame':   3 obs. of  3 variables:\n $ col_1: int  1 2 3\n $ col_2: int  4 5 6\n $ col_3: int  7 8 9\n\nhead(df_matrix_1)\n\n  col_1 col_2 col_3\n1     1     4     7\n2     2     5     8\n3     3     6     9\n\n\n\n\nWorking with Lists in R\n\nCreating Lists\nTo create a list, you use the list() function. Here’s an example:\n\n# Creating a simple list\nmy_list &lt;- list(name = \"John Doe\", age = 30, scores = c(90, 80, 70))\n\n# A list containing various types of elements, including another list\ncomplex_list &lt;- list(id = 1, name = \"Jane Doe\", preferences = list(color = \"blue\", hobby = \"reading\"))\n\n\n\nAccessing List Elements\nList elements can be accessed using the [[ ]] notation for single elements, or the $ notation if you’re accessing named elements:\n\n# Accessing elements\nname &lt;- my_list$name  # or my_list[[\"name\"]]\npreference_color &lt;- complex_list$preferences$color\n\n\n\nModifying Lists\nLists can be modified by adding new elements, changing existing elements, or removing elements:\n\n# Adding a new element\nmy_list$gender &lt;- \"Male\"\n\n# Changing an existing element\nmy_list$age &lt;- 31\n\n# Removing an element\nmy_list$scores &lt;- NULL\n\n\n\nLists in Functions\nLists are often used as return values for functions that need to provide multiple pieces of data:\n\n# Function returning a list\ncalculate_stats &lt;- function(numbers) {\n  mean_val &lt;- mean(numbers)\n  sum_val &lt;- sum(numbers)\n  return(list(mean = mean_val, sum = sum_val))\n}\n\n# Using the function\nresults &lt;- calculate_stats(c(1, 2, 3, 4, 5))\n\n\n\nWhy Lists are Important\n\nFlexibility: lists can contain different types of elements, including other lists, data frames, and functions.\nFunction outputs: many R functions return lists because they can hold multiple types of outputs.\n\n\n\nSolutions\n\n\n\nExercise 1: Basic Operations and Data Structure Manipulation\n\n# Create vectors\nvector_a &lt;- c(2, 4, 6, 8)\nvector_b &lt;- c(1, 3, 5, 7)\n\n# Operations\nsum_vector &lt;- vector_a + vector_b\ndiff_vector &lt;- vector_a - vector_b\ndouble_vector_a &lt;- vector_a * 2\nhalf_vector_b &lt;- vector_b / 2\n\n# Mean and Standard Deviation\nmean_a &lt;- mean(vector_a)\nsd_a &lt;- sd(vector_a)\nmean_b &lt;- mean(vector_b)\nsd_b &lt;- sd(vector_b)\n\n\n\nExercise 2: Working with Data Frames\n\n# Create data frame\nstudent_data &lt;- data.frame(\n  ID = 1:4,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n  Score = c(88, 92, 85, 95),\n  stringsAsFactors = FALSE\n)\n\n# Add Passed column\nstudent_data$Passed &lt;- student_data$Score &gt;= 90\n\n# Subset students who passed\npassed_students &lt;- student_data[student_data$Passed == TRUE, ]\n\n# Explore data frame\nsummary(student_data)\n\n       ID           Name               Score         Passed       \n Min.   :1.00   Length:4           Min.   :85.00   Mode :logical  \n 1st Qu.:1.75   Class :character   1st Qu.:87.25   FALSE:2        \n Median :2.50   Mode  :character   Median :90.00   TRUE :2        \n Mean   :2.50                      Mean   :90.00                  \n 3rd Qu.:3.25                      3rd Qu.:92.75                  \n Max.   :4.00                      Max.   :95.00                  \n\nhead(student_data)\n\n  ID    Name Score Passed\n1  1   Alice    88  FALSE\n2  2     Bob    92   TRUE\n3  3 Charlie    85  FALSE\n4  4   Diana    95   TRUE\n\nstr(student_data)\n\n'data.frame':   4 obs. of  4 variables:\n $ ID    : int  1 2 3 4\n $ Name  : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\"\n $ Score : num  88 92 85 95\n $ Passed: logi  FALSE TRUE FALSE TRUE\n\n\n\n\nExercise 3: Logical Operations and Subsetting\n\n# Subset data based on score\nmean_score &lt;- mean(student_data$Score)\nstudents_above_mean &lt;- student_data[student_data$Score &gt; mean_score, ]\n\n# Add attendance and subset\nattendance &lt;- c(\"Present\", \"Absent\", \"Present\", \"Present\")\nstudent_data$Attendance &lt;- attendance\npresent_students &lt;- student_data[student_data$Attendance == \"Present\", ]\n\n\n\nExercise 4: Cross-Tabulation and Analysis\n\n# Create factor variables\nFruit &lt;- factor(c(\"Apple\", \"Banana\", \"Apple\", \"Orange\", \"Banana\"))\nColor &lt;- factor(c(\"Red\", \"Yellow\", \"Green\", \"Orange\", \"Green\"))\n\n# Create data frame\nfruit_data &lt;- data.frame(Fruit, Color)\n\n# Cross-tabulation\nfruit_color_table &lt;- table(fruit_data$Fruit, fruit_data$Color)\nprint(fruit_color_table)\n\n        \n         Green Orange Red Yellow\n  Apple      1      0   1      0\n  Banana     1      0   0      1\n  Orange     0      1   0      0\n\n# Interpretation: Apple has the most color variety with 2 colors (Red, Green).\n\n\n\nExercise 5: Visualization with ggplot2\n\n# Install and load ggplot2\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Create bar plot\nggplot(student_data, aes(x = Name, y = Score, fill = Passed)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n  labs(title = \"Student Scores\", x = \"Name\", y = \"Score\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "",
    "text": "Readings\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html"
  },
  {
    "objectID": "content/02-content.html#overview",
    "href": "content/02-content.html#overview",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Overview",
    "text": "Overview\n\nUnderstand basic features of causal diagrams: definitions and applications\nIntroduction to the five elementary causal structures\nLab: Simulation and Regression"
  },
  {
    "objectID": "content/02-content.html#lab-regression-in-r",
    "href": "content/02-content.html#lab-regression-in-r",
    "title": "Causal diagrams: Five elementary structures",
    "section": "Lab – Regression in R",
    "text": "Lab – Regression in R"
  },
  {
    "objectID": "content/02-content.html#simulating-data-in-r-outcome-treatment",
    "href": "content/02-content.html#simulating-data-in-r-outcome-treatment",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating Data in R: Outcome ~ Treatment",
    "text": "Simulating Data in R: Outcome ~ Treatment\n\nStep 1: Set Up Your R Environment\nEnsure R or RStudio is installed and open.\n\n\nStep 2: Set a Seed for Reproducibility\nTo ensure that your simulated data can be reproduced exactly. Again, it is good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\n\nset.seed(123) # use any number to set the seed\n\n\n\nStep 3: Simulate Continuous Data: One Variable\nTo simulate continuous data, you can use functions like rnorm() for normal distributions, runif() for uniform distributions, etc. Here we simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\n\nn &lt;- 100 # number of observations\nmean &lt;- 50\nsd &lt;- 10\ndata_continuous &lt;- rnorm(n, mean, sd)\n\n# view\nhead(data_continuous)\n\n[1] 44.39524 47.69823 65.58708 50.70508 51.29288 67.15065\n\n# view using base R histogram\nhist(data_continuous)\n\n\n\n\n\n\n\n\n\n\nStep 4: Simulate Categorical Data\nCategorical data can be simulated using the sample() function. Here, we simulate a binary variable (gender) with two levels for 100 observations. There is equal probability of assignment.\n\nlevels &lt;- c(\"Male\", \"Female\")\ndata_categorical &lt;- sample(levels, n, replace = TRUE)\n\n# view\nhead(data_categorical)\n\n[1] \"Female\" \"Female\" \"Female\" \"Male\"   \"Female\" \"Female\"\n\n# check\ntable(data_categorical)\n\ndata_categorical\nFemale   Male \n    49     51 \n\n\nTo generate categories with unequal probabilities, you can use the sample() function by specifying the prob parameter, which defines the probability of selecting each level. This allows for simulating categorical data where the distribution between categories is not uniform.\nBelow is an example that modifies your initial code to create a categorical variable with unequal probabilities for “Male” and “Female”. Here is an example with unequal probabilities:\n\n# define levels and number of observations\nlevels &lt;- c(\"Male\", \"Female\")\nn &lt;- 100 # total number of observations\n\n# generate categorical data with unequal probabilities\ndata_categorical_unequal &lt;- sample(levels, n, replace = TRUE, prob = c(0.3, 0.7))\n\n# view the first few elements\n# head(data_categorical_unequal)\n\n# check\ntable(data_categorical_unequal)\n\ndata_categorical_unequal\nFemale   Male \n    69     31"
  },
  {
    "objectID": "content/02-content.html#visualising-simulated-data",
    "href": "content/02-content.html#visualising-simulated-data",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Visualising simulated data",
    "text": "Visualising simulated data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# plot your data\nggplot(scores_df, aes(x = Group, y = Scores, fill = Group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n\n\n\n\nScore Distribution by Group"
  },
  {
    "objectID": "content/02-content.html#sec-histogram",
    "href": "content/02-content.html#sec-histogram",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Histogram",
    "text": "Histogram\n\n#fig-cap: \"Histogram of simulated scores, by group, using facet_wrap()\"\nlibrary(ggplot2)\n\n# H=histograms for both groups\nggplot(df_scores_1, aes(x = scores, fill = group)) +\n  geom_histogram(binwidth = 5, color = \"black\") +\n  labs(title = \"Distribution of Scores by Group\",\n       x = \"Scores\",\n       y = \"Frequency\") +\n  facet_wrap(~group, ncol = 1) +   theme_minimal()"
  },
  {
    "objectID": "content/02-content.html#excercise-3",
    "href": "content/02-content.html#excercise-3",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Excercise 3",
    "text": "Excercise 3\n\nModify the simulation parameters to change each group’s mean and standard deviation. Observe how these changes affect the distribution.\nGo to the histogram. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/02-content.html#simulating-data-for-familiar-statistical-tests",
    "href": "content/02-content.html#simulating-data-for-familiar-statistical-tests",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating data for familiar statistical tests",
    "text": "Simulating data for familiar statistical tests\n\n# tbl-caption: \"results of a one-sample t-test.\"\n# simulate some data\nset.seed(123)\ndata &lt;- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nmod_first_test&lt;- t.test(data, mu = 4)\n\n# table\nparameters::parameters(mod_first_test)\n\nOne Sample t-test\n\nParameter | Mean |   mu | Difference |       95% CI | t(99) |      p\n--------------------------------------------------------------------\ndata      | 5.09 | 4.00 |       1.09 | [4.91, 5.27] | 11.95 | &lt; .001\n\nAlternative hypothesis: true mean is not equal to 4\n\n\nWe can automatically report these results using the report package:\n\n# report results automatically\nreport::report(mod_first_test)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe One Sample t-test testing the difference between data (mean = 5.09) and mu\n= 4 suggests that the effect is positive, statistically significant, and large\n(difference = 1.09, 95% CI [4.91, 5.27], t(99) = 11.95, p &lt; .001; Cohen's d =\n1.19, 95% CI [0.94, 1.45])\n\n\n\n# simulate data for two groups\nset.seed(123)\n\ngroup1 &lt;- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 &lt;- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nmod_t_test_result &lt;- t.test(group1, group2)\n\nreport::report(mod_t_test_result)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference between group1 and group2\n(mean of x = 5.03, mean of y = 5.65) suggests that the effect is negative,\nstatistically significant, and medium (difference = -0.61, 95% CI [-0.98,\n-0.25], t(97.95) = -3.34, p = 0.001; Cohen's d = -0.67, 95% CI [-1.07, -0.26])\n\nparameters::parameters(mod_t_test_result)\n\nWelch Two Sample t-test\n\nParameter1 | Parameter2 | Mean_Parameter1 | Mean_Parameter2 | Difference |         95% CI | t(97.95) |     p\n------------------------------------------------------------------------------------------------------------\ngroup1     |     group2 |            5.03 |            5.65 |      -0.61 | [-0.98, -0.25] |    -3.34 | 0.001\n\nAlternative hypothesis: true difference in means is not equal to 0\n\n\n\n# simulate pre-test and post-test scores\nset.seed(123)\n\npre_test &lt;- rnorm(30, mean = 80, sd = 10)\npost_test &lt;- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nmod_pre_post &lt;- t.test(pre_test, post_test, paired = TRUE)\n\nreport::report(mod_pre_post)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between pre_test and post_test (mean\ndifference = -5.89) suggests that the effect is negative, statistically\nsignificant, and large (difference = -5.89, 95% CI [-7.45, -4.33], t(29) =\n-7.73, p &lt; .001; Cohen's d = -1.41, 95% CI [-1.91, -0.90])\n\nparameters::parameters(mod_pre_post)\n\nPaired t-test\n\nParameter |     Group | Difference | t(29) |      p |         95% CI\n--------------------------------------------------------------------\npre_test  | post_test |      -5.89 | -7.73 | &lt; .001 | [-7.45, -4.33]\n\nAlternative hypothesis: true mean difference is not equal to 0"
  },
  {
    "objectID": "content/02-content.html#equivalence-of-anova-and-regression",
    "href": "content/02-content.html#equivalence-of-anova-and-regression",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Equivalence of ANOVA and Regression",
    "text": "Equivalence of ANOVA and Regression\nWe will simulate data in R to show that a one-way ANOVA is a particular case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings)."
  },
  {
    "objectID": "content/02-content.html#method",
    "href": "content/02-content.html#method",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Method",
    "text": "Method\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a “dependant”) variable. This setup allows us to apply both ANOVA and linear regression for comparison.\n\n# nice tables\nlibrary(parameters)\n\nset.seed(123) # reproducibility\nn &lt;- 90 # total number of observations\nk &lt;- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup &lt;- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n\n Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n\n# simulate outcome variable\nmeans &lt;- c(100, 100, 220) # Mean for each group\nsd &lt;- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny &lt;- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 &lt;- cbind.data.frame(y, group)\n\nanova_model &lt;- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova &lt;- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\n\nThe ANOVA (formula: y ~ group) suggests that:\n\n  - The main effect of group is statistically significant and large (F(2, 87) =\n786.88, p &lt; .001; Eta2 = 0.95, 95% CI [0.93, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\n\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit_regression &lt;- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit &lt;- parameters::model_parameters(fit_regression)\n\n# print table\ntable_fit\n\nParameter   | Coefficient |   SE |           95% CI | t(87) |      p\n--------------------------------------------------------------------\n(Intercept) |       99.29 | 2.46 | [ 94.41, 104.18] | 40.40 | &lt; .001\ngroup [2]   |        3.38 | 3.48 | [ -3.53,  10.29] |  0.97 | 0.333 \ngroup [3]   |      121.07 | 3.48 | [114.16, 127.98] | 34.83 | &lt; .001\n\n\n\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit &lt;- report_parameters(fit_regression)\n\n#print\nreport_fit\n\n  - The intercept is statistically significant and positive (beta = 99.29, 95% CI [94.41, 104.18], t(87) = 40.40, p &lt; .001; Std. beta = -0.71, 95% CI [-0.80, -0.63])\n  - The effect of group [2] is statistically non-significant and positive (beta = 3.38, 95% CI [-3.53, 10.29], t(87) = 0.97, p = 0.333; Std. beta = 0.06, 95% CI [-0.06, 0.18])\n  - The effect of group [3] is statistically significant and positive (beta = 121.07, 95% CI [114.16, 127.98], t(87) = 34.83, p &lt; .001; Std. beta = 2.08, 95% CI [1.96, 2.20])"
  },
  {
    "objectID": "content/02-content.html#upshot",
    "href": "content/02-content.html#upshot",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Upshot",
    "text": "Upshot\nANOVA partitions variance into between-group and within-group components. Regression analysis estimates the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain comparisons. Note: comparisons do not establish causality. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, along the way, you’ll learn more about data visualisation, modelling, and reporting. These skills are essential for your final report. They are also skills that you will serve you beyond this course."
  },
  {
    "objectID": "content/02-content.html#more-complexity",
    "href": "content/02-content.html#more-complexity",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "More complexity",
    "text": "More complexity\n\n# set the number of observations\nn &lt;- 100\n\n# simulate the 'Age' variable\nmean_age &lt;- 30\nsd_age &lt;- 5\nAge &lt;- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept &lt;- 20000   # Intercept for the income equation\nbeta_age &lt;- 1500     # Coefficient for the effect of age on income\nerror_sd &lt;- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome &lt;- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex &lt;- data.frame(Age, Income)\n\n\nStep 7: Visualising Simulated Data\nVisualising your simulated data can help understand its distribution and relationships. Use the ggplot2 package for this:\n\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")"
  },
  {
    "objectID": "content/02-content.html#practice",
    "href": "content/02-content.html#practice",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Practice",
    "text": "Practice\nSimulating data is a powerful method to understand statistical concepts and data manipulation. Let’s simulate a simple dataset representing scores from two cultural groups.\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations!"
  },
  {
    "objectID": "content/02-content.html#appendix-a",
    "href": "content/02-content.html#appendix-a",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix A: Solutions",
    "text": "Appendix A: Solutions\n\nSolution Excercise 2: simulate data and regression reporting\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed(12345)\n\n# number of observations\nn &lt;- 100\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, mean = 50, sd = 10)\n\n# define the specified effect size of A on Y\nbeta_A &lt;- 2\n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A =  rnorm(n, mean = 50, sd = 10),\n  Y = 5 +  # intercept (optional)\n    beta_A * A + rnorm(n, mean = 0, sd = 20)\n)\n\n# view\nhead(df_3)\n\n         A         Y\n1 52.23925  87.98766\n2 38.43777 106.60413\n3 54.22419 107.68437\n4 36.75245 117.09730\n5 51.41084 133.74473\n6 44.63952  70.74512\n\nstr(df_3)\n\n'data.frame':   100 obs. of  2 variables:\n $ A: num  52.2 38.4 54.2 36.8 51.4 ...\n $ Y: num  88 107 108 117 134 ...\n\n# linear regression of Y on A\n\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n# results \n# summary(model)\n\n# nicer report\nparameters::model_parameters(fit_3)\n\nParameter   | Coefficient |    SE |          95% CI | t(98) |      p\n--------------------------------------------------------------------\n(Intercept) |      109.17 | 14.62 | [80.17, 138.18] |  7.47 | &lt; .001\nA           |   -3.80e-03 |  0.28 | [-0.57,   0.56] | -0.01 | 0.989 \n\nreport(fit_3)\n\nWe fitted a linear model (estimated using OLS) to predict Y with A (formula: Y\n~ A). The model explains a statistically not significant and very weak\nproportion of variance (R2 = 1.83e-06, F(1, 98) = 1.79e-04, p = 0.989, adj. R2\n= -0.01). The model's intercept, corresponding to A = 0, is at 109.17 (95% CI\n[80.17, 138.18], t(98) = 7.47, p &lt; .001). Within this model:\n\n  - The effect of A is statistically non-significant and negative (beta =\n-3.80e-03, 95% CI [-0.57, 0.56], t(98) = -0.01, p = 0.989; Std. beta =\n-1.35e-03, 95% CI [-0.20, 0.20])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation."
  },
  {
    "objectID": "content/02-content.html#what-you-have-learned",
    "href": "content/02-content.html#what-you-have-learned",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "What You Have Learned",
    "text": "What You Have Learned\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a important skill for exploring statistical concepts and causal inference. Congratulations!\n\nData visualisation:\n\nYou’ve expanded your capacity for data visualisation.\n\nStatistical tests: You’ve conducted basic statistical tests, including t-tests and ANOVA, and regression.\nUnderstanding ANOVA and regression:\n\nYou’ve examined the equivalence of ANOVA and regression analysis"
  },
  {
    "objectID": "content/01-content.html#background-readings-none-today",
    "href": "content/01-content.html#background-readings-none-today",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Background Readings: None today!",
    "text": "Background Readings: None today!\n\nIf you want to get started, download R and Rstudio as below\n\n\n\n\n\n\n\nKey Concepts For the Test(s):\n\n\n\nToday we introduce the following topics relevant to the Test(s)\n\nConfounding (week 2,3)\nInternal Validity (week 2,3,4)\nExternal Validity (week 3,4)\n\nHowever, we do no more than raise these topics. They will be covered in technical detail in the upcoming weeks."
  },
  {
    "objectID": "content/01-content.html#lab-1-installing-r-and-rstudio",
    "href": "content/01-content.html#lab-1-installing-r-and-rstudio",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Lab 1: Installing R and RStudio",
    "text": "Lab 1: Installing R and RStudio\nThis session is designed to introduce you to R and RStudio."
  },
  {
    "objectID": "content/01-content.html#excercises",
    "href": "content/01-content.html#excercises",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Excercises",
    "text": "Excercises\n\n\n\n\n\n\nExercise 4\n\n\n\nExercise 4: Basic Operations and Data Structure Manipulation\nObjective: Practice creating vectors and performing basic arithmetic operations.\n\nCreate two numeric vectors, vector_a and vector_b, with the following values:\n\nvector_a: 2, 4, 6, 8\nvector_b: 1, 3, 5, 7\n\nPerform the following operations and store the results in new variables:\n\nAdd vector_a and vector_b.\nSubtract vector_b from vector_a.\nMultiply vector_a by 2.\nDivide vector_b by 2.\n\nCalculate the mean and standard deviation of both vector_a and vector_b.\n\n\n\n\n\n\n\n\n\nExercise 5\n\n\n\nExercise 5: Working with Data Frames\nObjective: Gain familiarity with data frame creation, manipulation, and basic data exploration functions.\n\nCreate a data frame student_data with the following columns:\n\nID: 1, 2, 3, 4\nName: Alice, Bob, Charlie, Diana\nScore: 88, 92, 85, 95\nEnsure you set stringsAsFactors = FALSE.\n\nAdd a new column Passed to student_data indicating whether the student passed. Assume a pass mark of 90.\nExtract the Name and Score of students who passed into a new data frame.\nUse summary(), head(), and str() functions to explore student_data.\n\n\n\n\n\n\n\n\n\nExercise 6\n\n\n\nExercise 6: Logical Operations and Subsetting\nObjective: Practice using logical operations to subset data frames.\n\nUsing the student_data data frame from Exercise 2, subset the data to find students who scored above the mean score of the class.\nCreate a vector attendance with values (Present, Absent, Present, Present) corresponding to each student’s attendance.\nAdd attendance as a new column to student_data and then subset the data frame to select only the rows where students were present.\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\nExercise 4: Cross-Tabulation and Analysis\nObjective: Understand the use of table() function for cross-tabulation and analysis.\n\nCreate two factor variables:\n\nFruit: Apple, Banana, Apple, Orange, Banana\nColor: Red, Yellow, Green, Orange, Green\n\nConvert Fruit and Color into factors and then into a data frame named fruit_data.\nUse the table() function to perform a cross-tabulation of Fruit by Color.\nInterpret the results. Which fruit has the most color variety?\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\nExercise 8: Visualization with ggplot2\nObjective: (If ggplot2 was introduced) Create a simple plot to visualize the data.\n\nInstall and load the ggplot2 package if not already done.\nUsing student_data, create a bar plot showing the scores of students. Use Name for the x-axis and Score for the y-axis.\nEnhance the plot by adding a title, x and y-axis labels, and use different colors for passed and failed students.\n\nThese exercises are designed to be progressively challenging, ensuring that students apply what they’ve learned about basic operations, data frame manipulation, logical operations, and simple data analysis and visualization in R.\n\n\n\nConclusion\nData frames are indispensable in R for handling real-world data, offering flexibility in data storage, manipulation, and analysis. Their ability to accommodate columns of different data types, combined with R’s functions for data manipulation and analysis, makes them a powerful tool for data scientists and statisticians."
  },
  {
    "objectID": "content/01-content.html#summary",
    "href": "content/01-content.html#summary",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Summary",
    "text": "Summary\nCongratulations on completing Lab 1! This session has laid the groundwork. We’ve covered a lot, but we’ll have a good deal of practice throughout the course.\n\nWhat We Have Learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use RStudio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nBasic R Data Structures such as:\n\nVectors and Matrices: You have learned to create and manipulate vectors and matrices, the simplest forms of data storage in R, which are crucial for handling numeric, character, and logical data types in a structured manner.\nData Frames: You’ve been introduced to data frames, a key data structure in R for storing tabular data. Data frames accommodate columns of different data types, making them highly versatile for data analysis and manipulation.\nFactors and Ordered Factors: Understanding factors and ordered factors has provided you with the tools to handle categorical data effectively, including the ability to manage and analyse data involving categorical variables with both unordered and ordered levels.\n\nBasics of ggplot2:\n\nYou’ve been equipped with the fundamentals of data visualisation using ggplot2, including how to create basic plots like bar charts, scatter plots, and line graphs. You’ve learned about the importance of aesthetics (aes) and geometries (geom_ functions) in creating visually appealing and informative graphics.\n\nCustomizing Plots:\n\nTechniques for enhancing plots with titles, axis labels, and custom colour schemes have been covered. You’ve practised making your visualisations more informative and engaging by customising plot aesthetics.\n\n\nHow to Build Skills?\n\nPractical Application:\n\nDo the hands-on exercises at home. They’ll help you apply what you have learned here.\n\n\nWhere to Get Help\nAs sure as night follows day, you will need help coding. Good resources:\n\nLarge Language Models (LLMs): OpenAI’s premium LLM (GPT-4) outperforms the free version (GPT-3.5) for complex queries. I don’t think LLM’s are quite ready for science, but they are remarkably helpful for coding, and for learning how to code. So please use them, with caution, but use them.\nStack Overflow: a valuable resource for coding advice and solutions.\nCross-validated is probably the best place to go for stats advice. (LLMs are currently not reliable for statisticis for which there is not much information – which is most of science.)\nDeveloper Websites and GitHub Pages: Directly engage with package developers and the community for insights and support.Parameters package discussion page offers insights and support directly from its developers and user community.\nYour tutors and lecturer. We care. We’re here to help you!\n\n\n\nRecommended Reading\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/ For now, only lecture 1 would be relevant. It’s worth noting that this lecture covers working with GitHub. GitHub is a useful tool, you should learn how to use it; however, it is not a requirement for the current course.\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Xie Y (2023). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.49, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/01-content.html#appendix-a-at-home-exercises",
    "href": "content/01-content.html#appendix-a-at-home-exercises",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix A: At Home Exercises",
    "text": "Appendix A: At Home Exercises\n\n\n\n\n\n\nExercise 1: Install the tidyverse package\n\n\n\nFollow these instructions to install the tidyverse package in RStudio:\n\nOpen RStudio: launch RStudio on your computer.\nAccess package installation:\n\nNavigate to the menu at the top of RStudio and click on Tools &gt; Install Packages.... This opens the Install Packages dialogue box.\n\nInstall tidyverse:\n\nIn the Install Packages dialogue box, you will see a field labelled “Packages (separate multiple with space or comma):”. Click in this field and type tidyverse.\nBelow the packages field, ensure the checkbox for Install dependencies is checked. This ensures all packages that tidyverse depends on are also installed.\n\nBegin installation:\n\nClick on the Install button to start the installation process.\n\n\nThe installation might take a few minutes. Monitor the progress in the “Console” pane. Once the installation is complete, you will see a message in the console indicating that the process has finished.\n\nLoad tidyverse: After successful installation, you can load the tidyverse package into your R session by typing library(tidyverse) in the console and pressing Enter.\n\n\n\n\n\n\n\n\n\nExercise 2: Install the parameters and report packages\n\n\n\nTo install the parameters and report packages in RStudio, follow these instructions:\n\nOpen RStudio: start by launching the RStudio application on your computer.\nAccess Package Installation:\n\nGo to the RStudio menu bar at the top of the screen and click on Tools &gt; Install Packages.... This action opens the Install Packages dialogue box.\n\nInstall parameters and report:\n\nIn the Install Packages dialogue box, locate the field labelled “Packages (separate multiple with space or comma):”. Click in this field and type parameters, report, separating the package names with a comma.\nMake sure the checkbox for Install dependencies is selected. This ensures that any additional packages needed by parameters and report are also installed.\nClick the Install button to initiate the installation of both packages and their dependencies.\n\n\n\n\n\n\n\n\n\n\nExercise 3: Basic Operations and Data Structure Manipulation\n\n\n\nObjective: Practice creating vectors and performing basic arithmetic operations.\n\nCreate two numeric vectors, vector_a and vector_b, with the following values:\n\nvector_a: 2, 4, 6, 8\nvector_b: 1, 3, 5, 7\n\nPerform the following operations and store the results in new variables:\n\nAdd vector_a and vector_b.\nSubtract vector_b from vector_a.\nMultiply vector_a by 2.\nDivide vector_b by 2.\n\nCalculate the mean and standard deviation of both vector_a and vector_b.\n\n\n\n\n\n\n\n\n\nExercise 4: Working with Data Frames\n\n\n\nObjective: Gain familiarity with data frame creation, manipulation, and basic data exploration functions.\n\nCreate a data frame student_data with the following columns:\n\nid: 1, 2, 3, 4\nname: alice, bob, charlie, diana\nscore: 88, 92, 85, 95\nEnsure you set stringsAsFactors = FALSE.\n\nAdd a new column passed to student_data indicating whether the student passed. Assume a pass mark of 90.\nExtract the name and score of students who passed into a new data frame.\nUse summary(), head(), and str() functions to explore student_data.\n\n\n\n\n\n\n\n\n\nExercise 5 Logical Operations and Subsetting\n\n\n\nObjective: Practice using logical operations to subset data frames.\n\nUsing the student_data data frame from Exercise 2, subset the data to find students who scored above the mean score of the class.\nCreate a vector attendance with values (present, absent, present, present) corresponding to each student’s attendance.\nAdd attendance as a new column to student_data and then subset the data frame to select only the rows where students were present.\n\n\n\n\n\n\n\n\n\nExercise 6: Cross-Tabulation and Analysis\n\n\n\nObjective: Understand the use of table() function for cross-tabulation and analysis.\n\nCreate two-factor variables:\n\nfruit: apple, banana, apple, orange, banana\ncolour: red, yellow, green, orange, green\n\nConvert fruit and colour into factors and then into a data frame named fruit_data.\nUse the table() function to perform a cross-tabulation of fruit by colour.\nInterpret the results. Which fruit has the most colour variety?\n\n\n\n\n\n\n\n\n\nExercise 7: Visualization with ggplot2\n\n\n\nObjective: (If ggplot2 was introduced) Create a simple plot to visualise the data.\n\nInstall and load the ggplot2 package if not already done.\nUsing student_data, create a bar plot showing the scores of students. Use name for the x-axis and score for the y-axis.\nEnhance the plot by adding a title, x and y-axis labels, and use different colours for passed and failed students.\n\nThese exercises are designed to be progressively challenging, ensuring that students apply what they’ve learned about basic operations, data frame manipulation, logical operations, and simple data analysis and visualisation in R."
  },
  {
    "objectID": "content/01-content.html#appendix-c-other-data-types-you-may-encounter",
    "href": "content/01-content.html#appendix-c-other-data-types-you-may-encounter",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix C: Other Data Types You May Encounter",
    "text": "Appendix C: Other Data Types You May Encounter\n\nArrays and Matrices\nArrays are multi-dimensional data structures, while matrices are two-dimensional.\n\nmatrix_1 &lt;- matrix(1:9, nrow = 3) # creates a 3x3 matrix\narray_1 &lt;- array(1:12, dim = c(2, 3, 2)) # creates a 2x3x2 array\n\n\n\nConvert Matrix to Data Frame\nA data.frame is used for storing tabular data.\n\n# change matrix to array:\ndf_matrix_1 &lt;- data.frame( matrix_1 )\n\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ X1: int  1 2 3\n $ X2: int  4 5 6\n $ X3: int  7 8 9\n\nhead(df_matrix_1)\n\n  X1 X2 X3\n1  1  4  7\n2  2  5  8\n3  3  6  9\n\n# change colnames\nnew_colnames &lt;- c(\"col_1\", \"col_2\", \"col_3\")\n\ncolnames(df_matrix_1)&lt;- new_colnames\n\n# check\nstr(df_matrix_1)  \n\n'data.frame':   3 obs. of  3 variables:\n $ col_1: int  1 2 3\n $ col_2: int  4 5 6\n $ col_3: int  7 8 9\n\nhead(df_matrix_1)\n\n  col_1 col_2 col_3\n1     1     4     7\n2     2     5     8\n3     3     6     9\n\n\n\n\nWorking with Lists in R\n\nCreating lists\nTo create a list, you use the list() function.\n\n# Creating a simple list\nmy_list &lt;- list(name = \"John Doe\", age = 30, scores = c(90, 80, 70))\n\n# A list containing various types of elements, including another list\ncomplex_list &lt;- list(id = 1, name = \"Jane Doe\", preferences = list(color = \"blue\", hobby = \"reading\"))\n\n\n\nAccessing list elements\nList elements can be accessed using the [[ ]] notation for single elements, or the $ notation if you’re accessing named elements:\n\n# Accessing elements\nname &lt;- my_list$name  # or my_list[[\"name\"]]\n\npreference_color &lt;- complex_list$preferences$color\n\n\n\nModifying lists\nLists can be modified by adding new elements, changing existing elements, or removing elements:\n\n# Adding a new element\nmy_list$gender &lt;- \"Male\"\n\n# Changing an existing element\nmy_list$age &lt;- 31\n\n# Removing an element\nmy_list$scores &lt;- NULL\n\n\n\nLists in Functions\nLists are often used as return values for functions that need to provide multiple pieces of data:\n\n# Function returning a list\ncalculate_stats &lt;- function(numbers) {\n  mean_val &lt;- mean(numbers)\n  sum_val &lt;- sum(numbers)\n  return(list(mean = mean_val, sum = sum_val))\n}\n\n# Using the function\nresults &lt;- calculate_stats(c(1, 2, 3, 4, 5))\n\n\n\nWhy Lists are Important\n\nFlexibility: lists can contain different types of elements, including other lists, data frames, and functions.\nFunction outputs: many R functions return lists because they can hold multiple types of outputs."
  },
  {
    "objectID": "content/01-content.html#appendix-c-else",
    "href": "content/01-content.html#appendix-c-else",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix C: Else",
    "text": "Appendix C: Else\n\nUsing Logical Operators in Conditions\nThese operators are often used in conditional statements and loops:\n\nif (x &gt; 0 && y &gt; 0) {\n  print(\"both x and y are positive\")\n}\n\n[1] \"both x and y are positive\"\n\nz &lt;- c(TRUE, FALSE, TRUE)\nw &lt;- c(FALSE, TRUE, TRUE)\ncombined_logic &lt;- z | w # element-wise OR\n\ncombined_logic\n\n[1] TRUE TRUE TRUE"
  },
  {
    "objectID": "content/01-content.html#working-with-data-frames",
    "href": "content/01-content.html#working-with-data-frames",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Working with Data Frames",
    "text": "Working with Data Frames\n\nCreating Data Frames\nData frames can be created using the data.frame() function, specifying each column and its values. Here’s a simple example:\n\ndf &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 35),\n  Gender = c(\"Female\", \"Male\", \"Male\")\n)\n\n# check structure\nhead(df)\n\n     Name Age Gender\n1   Alice  25 Female\n2     Bob  30   Male\n3 Charlie  35   Male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ Name  : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ Age   : num  25 30 35\n $ Gender: chr  \"Female\" \"Male\" \"Male\"\n\nsummary(df)\n\n     Name                Age          Gender         \n Length:3           Min.   :25.0   Length:3          \n Class :character   1st Qu.:27.5   Class :character  \n Mode  :character   Median :30.0   Mode  :character  \n                    Mean   :30.0                     \n                    3rd Qu.:32.5                     \n                    Max.   :35.0                     \n\n\nIn this example, df is a data frame with three columns (Name, Age, Gender) and three rows, each representing a different individual.\n\n\nAccessing Data Frame Elements\nThere are often several ways to do the same thing in R. You can access the elements of a data frame in several ways:\n\nBy Column Name: Use the $ operator followed by the column name.\n\n\nnames &lt;- df$Name  # extracts the `Name` column\n\n\nBy Row and Column: Use the [row, column] indexing. Rows or columns can be specified by number or name.\n\n\nsecond_person &lt;- df[2, ]  # extracts the second row\nage_column &lt;- df[, \"Age\"]  # extracts the `Age` column\n\n# show\nsecond_person\n\n  Name Age Gender\n2  Bob  30   Male\n\nage_column\n\n[1] 25 30 35\n\n\n\nUsing subset() Function: To extract subsets of the data frame based on conditions.\n\n\nadults &lt;- subset(df, Age &gt; 18)  # extracts rows where `Age` is greater than 18\n\nsummary(adults$Age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   25.0    27.5    30.0    30.0    32.5    35.0 \n\nmean(adults$Age)\n\n[1] 30\n\nmin(adults$Age)\n\n[1] 25\n\n\n\n\nManipulating Data Frames\nData frames are flexible; they can be manipulated in various ways, such as:\n\nAdding Columns: You can add new columns using the $ operator.\n\n\ndf$Employed &lt;- c(TRUE, TRUE, FALSE)  # adds a new column \"Employed\"\n\n# show\nhead(df)\n\n     Name Age Gender Employed\n1   Alice  25 Female     TRUE\n2     Bob  30   Male     TRUE\n3 Charlie  35   Male    FALSE\n\n\n\nAdding Rows: Use the rbind() function to add new rows.\n\n\nnew_person &lt;- data.frame(Name = \"Diana\", Age = 28, Gender = \"Female\", Employed = TRUE)\ndf &lt;- rbind(df, new_person)\n\n# show\nhead(df)\n\n     Name Age Gender Employed\n1   Alice  25 Female     TRUE\n2     Bob  30   Male     TRUE\n3 Charlie  35   Male    FALSE\n4   Diana  28 Female     TRUE\n\n\n\nModifying Values: Access the element or column and assign it a new value.\n\n\n# note double brackets\n\ndf[4, \"Age\"] &lt;- 26  # changes Alice's age to 26\n\n# view row\ndf[4, ]\n\n   Name Age Gender Employed\n4 Diana  26 Female     TRUE\n\n\n\nRemoving Columns or Rows: Set columns to NULL to remove them, or use - with row or column indices.\n\n\nhead(df)\n\n     Name Age Gender Employed\n1   Alice  25 Female     TRUE\n2     Bob  30   Male     TRUE\n3 Charlie  35   Male    FALSE\n4   Diana  26 Female     TRUE\n\n# remove employed column\ndf$Employed &lt;- NULL  # removes the Employed column\n\nhead(df)\n\n     Name Age Gender\n1   Alice  25 Female\n2     Bob  30   Male\n3 Charlie  35   Male\n4   Diana  26 Female\n\n# remove fourth row (Diana)\ndf &lt;- df[-4, ]  # removes the fourth row\n\n#show  \ndf\n\n     Name Age Gender\n1   Alice  25 Female\n2     Bob  30   Male\n3 Charlie  35   Male\n\n\n\n\nViewing Data Structure (summary(), str(), head(), tail())\n\nsummary(): Provides a summary of an object’s structure.\nstr(): Displays the structure of an object.\nhead(): Shows the first few rows of a data frame or the first elements of a vector.\ntail(): Shows the last few rows of a data frame or the last elements of a vector.\n\n\n# iris is a preloaded dataset\nstr(iris) # displays structure of scores_df\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris) # summary statistics\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nhead(iris) # first few rows\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris) # last few rows\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\n\n\nmean()\n\nCalculates the arithmetic mean of a numerical object.\n\n\nset.seed(12345)\n\n# we will cover R’s powerful simulation functions like `rnorm()`next week\nvector &lt;- rnorm(n = 40, mean = 0, sd = 1)\nmean(vector)  # note the sampling error here\n\n[1] 0.2401853\n\n\n\n\nsd()\n\nComputes the standard deviation, which measures the amount of variation or dispersion of a set of values.\n\n\nsd(vector)  # replace 'vector' with your numerical vector\n\n[1] 1.038425\n\n\n\n\nmin() and max()\n\nThese functions return a numerical object’s minimum and maximum values, respectively.\n\n\nmin(vector)  # minimum value\n\n[1] -1.817956\n\nmax(vector)  # maximum value\n\n[1] 2.196834\n\n\n\n\ntable()\n\nGenerates a frequency table of an object, useful for categorical data. It counts the number of occurrences of each unique element.\n\n\n#  seed for reproducibility\nset.seed(12345)\n\n#  two categorical variables\nGender &lt;- sample(c(\"Male\", \"Female\"), \n                 size = 100, \n                 replace = TRUE, \n                 prob = c(0.5, 0.5))\nEducation_Level &lt;- sample(c(\"High School\", \"Bachelor\", \"Master\"),\n                          size = 100, \n                          replace = TRUE, \n                          prob = c(0.4, 0.4, 0.2))\n# create a data frame\ndf_table_example &lt;- data.frame(Gender, Education_Level)\n\n# show\nhead(df_table_example)\n\n  Gender Education_Level\n1   Male        Bachelor\n2   Male     High School\n3   Male          Master\n4   Male     High School\n5 Female     High School\n6 Female          Master\n\n\n\n\nCross-Tabulation with table()\n\ntable() can also be used for cross-tabulation, providing a way to analyse the relationship between two or more factors.\n\n\ntable(df_table_example$Gender, df_table_example$Education_Level)  # crosstab\n\n        \n         Bachelor High School Master\n  Female       14          19     15\n  Male         21          18     13\n\n\nThis produces a contingency table showing the counts at each combination of factor1 and factor2 levels.\n\n\nExplore Data Frames\n\nData Exploration: Functions like head(), tail(), and str() help you explore the first few rows, last few rows, and the structure of the data frame, respectively.\n\n\n# show\nhead(df)  # First six rows\n\n     Name Age Gender\n1   Alice  25 Female\n2     Bob  30   Male\n3 Charlie  35   Male\n\ntail(df)  # Last six rows\n\n     Name Age Gender\n1   Alice  25 Female\n2     Bob  30   Male\n3 Charlie  35   Male\n\nstr(df)   # Structure of the data frame\n\n'data.frame':   3 obs. of  3 variables:\n $ Name  : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ Age   : num  25 30 35\n $ Gender: chr  \"Female\" \"Male\" \"Male\"\n\n\n\n\nSummary Statistics\nUse summary() to get a summary of each column.\n\n# show\nsummary(df)\n\n     Name                Age          Gender         \n Length:3           Min.   :25.0   Length:3          \n Class :character   1st Qu.:27.5   Class :character  \n Mode  :character   Median :30.0   Mode  :character  \n                    Mean   :30.0                     \n                    3rd Qu.:32.5                     \n                    Max.   :35.0"
  },
  {
    "objectID": "content/01-content.html#first-data-visualisation-with-ggplot2",
    "href": "content/01-content.html#first-data-visualisation-with-ggplot2",
    "title": "Asking questions in cross-cultural psychology",
    "section": "First Data Visualisation with ggplot2",
    "text": "First Data Visualisation with ggplot2\nggplot2 is a powerful and flexible R package for creating elegant data visualisations. It is based on the Grammar of Graphics, allowing users to build plots layer by layer, making it versatile for creating a wide range of plots.\n\nInstalling and Loading ggplot2\nBefore using ggplot2, ensure the package is installed and loaded into your R session:\n\n# load ggplot2\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# seed for reproducibility\nset.seed(12345)\n\n# simulate student data (more on simulation next week)\nstudent_data &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n    score = sample(80:100, 8, replace = TRUE), # random scores between 80 and 100\n    stringsasfactors = FALSE\n)\n\n# determine pass/fail based on score\n# we will cover the ifelse() operator in detail in upcoming weeks\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\n\n# convert 'passed' to factor for colour coding in ggplot2\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\n\n# view the first few rows of the data frame\nhead(student_data)\n\n     name score stringsasfactors passed\n1   alice    93            FALSE passed\n2     bob    98            FALSE passed\n3 charlie    95            FALSE passed\n4   diana    90            FALSE passed\n5   ethan    81            FALSE failed\n6   fiona    90            FALSE passed\n\n# simulate study hours\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)\n\n\n\nBasic Components of a ggplot2 Plot\n\nData: the dataset you want to visualise.\nAesthetics (aes): defines how data are mapped to colour, size, shape, and other visual properties.\nGeometries (geom_ functions): the type of plot or layer you want to add (e.g., points, lines, bars).\n\n\nCreate a Basic Plot\nStart by creating a simple bar plot:\n\nggplot(student_data, aes(x = name, y = score)) +\n    geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nThis code plots score for each name in the student_data dataframe. The stat = \"identity\" argument tells ggplot2 to use the score values directly to determine the height of the bars.\n\n\nCustomising the plot\nTo enhance your plot, you can add titles, change axis labels, and modify colours:\n\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"true\" = \"blue\", \"FALSE\" = \"red\")) +\n    labs(title = \"student scores\", x = \"student name\", y = \"score\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\naes(fill = passed): maps the passed variable to the colour fill of the bars, allowing for colour differentiation based on whether students passed or failed.\nscale_fill_manual(): customizes the colours used for the true and FALSE values of the passed variable.\nlabs(): adds a main title and axis labels.\ntheme_minimal(): applies a minimalistic theme to the plot for a cleaner appearance.\n\n\n\n\nScatter Plot with ggplot2\nA scatter plot is useful for examining the relationship between two continuous variables.\nWe next simulate a scenario where we compare student scores against study hours.\n\n# create scatter plot\nggplot(student_data, aes(x = study_hours, y = score, color = passed)) +\n    geom_point(size = 4) +\n    labs(title = \"student scores vs. study hours\", x = \"study hours\", y = \"score\") +\n    theme_minimal() +\n    scale_color_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nBox Plot with ggplot2\nBox plots are excellent for visualising the distribution of scores by pass/fail status, showing medians, quartiles, and potential outliers.\n\n# create box plot\nggplot(student_data, aes(x = passed, y = score, fill = passed)) +\n    geom_boxplot() +\n    labs(title = \"score distribution by pass/fail status\", x = \"status\", y = \"score\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nHistogram with ggplot2\nHistograms are helpful for understanding the distribution of a single continuous variable, such as scores.\n\n# create a histogram\nggplot(student_data, aes(x = score, fill = passed)) +\n    geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n    labs(title = \"histogram of scores\", x = \"score\", y = \"count\") +\n    theme_minimal() +\n    scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\nLine Plot with ggplot2 (Time Series)\nFor demonstrating a line plot, we simulate monthly study hours over a semester for a student.\n\n# simulate monthly study hours\nmonths &lt;- factor(month.abb[1:8], levels = month.abb[1:8])\nstudy_hours &lt;- c(0, 3, 15, 30, 35, 120, 18, 15)\n\n# make data frame\nstudy_data &lt;- data.frame(month = months, study_hours = study_hours)\n\n# create a line plot\nggplot(study_data, aes(x = month, y = study_hours, group = 1)) +\n    geom_line(linewidth = 1, color = \"blue\") +\n    geom_point(color = \"red\", size = 1) +\n    labs(title = \"monthly study hours\", x = \"month\", y = \"study hours\") +\n    theme_minimal()"
  },
  {
    "objectID": "content/01-content.html#working-with-strings",
    "href": "content/01-content.html#working-with-strings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Working with Strings",
    "text": "Working with Strings\nStrings are sequences of characters.\n\nname &lt;- \"world\" # assigns a string to name\ngreeting &lt;- paste(\"Hello,\", name) # concatenates strings\n\n#show\ngreeting\n\n[1] \"Hello, world\""
  },
  {
    "objectID": "content/01-content.html#factors",
    "href": "content/01-content.html#factors",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Factors",
    "text": "Factors\nFactors are used to represent categorical data.\n\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\n\nhead(gender)\n\n[1] Male   Female Female Male  \nLevels: Female Male\n\n\n\nOrdered Factors\nTo create an ordered factor, you use the factor() function with the ordered argument set to TRUE, or you can directly use the ordered() function. You can also specify the levels in their natural order using the levels argument if the default order does not match the natural sequence.\n\n# using factor() function with ordered = TRUE\neducation_levels &lt;- c(\"High School\", \"Bachelor\", \"Master\", \"Ph.D.\")\neducation_factor &lt;- factor(education_levels, ordered = TRUE)\n\n# alternatively, using ordered() function\neducation_ordered &lt;- ordered(education_levels)\n\n# specifying levels explicitly\neducation_ordered_explicit &lt;- factor(education_levels, levels = c(\"High School\", \"Bachelor\", \"Master\", \"Ph.D.\"), ordered = TRUE)\n\n\n\nOperations with Ordered Factors\nOrdered factors allow for comparison operations that make sense in their context, such as less than (&lt;), greater than (&gt;), etc., based on their order.\n\n# Comparing educational levels\nedu1 &lt;- ordered(\"Bachelor\", levels = c(\"High School\", \"Bachelor\", \"Master\", \"Ph.D.\"))\nedu2 &lt;- ordered(\"Master\", levels = c(\"High School\", \"Bachelor\", \"Master\", \"Ph.D.\"))\n\n# Check if edu2 is a higher degree than edu1\nedu2 &gt; edu1 # Returns TRUE\n\n[1] TRUE\n\n\nOrdered factors can be useful in statistical models where the order impacts the model, such as ordinal logistic regression. The order of the factor levels can influence the model’s interpretation and coefficients. We will return to ordered factors and ordered/logistic regression later in the course.\n\n\nChecking Data with Ordered Factors\nYou can view the structure and summary of ordered factors just as with regular factors, but the output will indicate the order.\n\n# view the structure\nstr(education_ordered)\n\n Ord.factor w/ 4 levels \"Bachelor\"&lt;\"High School\"&lt;..: 2 1 3 4\n\n# summary to see the distribution\nsummary(education_ordered)\n\n   Bachelor High School      Master       Ph.D. \n          1           1           1           1 \n\n\n\n\nModifying Ordered Factors\nIf you need to change the order of levels or add new levels, you can re-factor the variable using factor() or ordered() and specify the new levels.\n\n# adding a new level and reordering\nnew_levels &lt;- c(\"Primary School\", \"High School\", \"Bachelor\", \"Master\", \"Ph.D.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)"
  },
  {
    "objectID": "content/01-content.html#working-with-vectors",
    "href": "content/01-content.html#working-with-vectors",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Working with Vectors",
    "text": "Working with Vectors\nVectors are one of R’s most fundamental data structures, essential for storing and manipulating a sequence of data elements.\nVectors are homogenous, meaning all elements in a vector must be of the same type (e.g., all numeric, all character, etc.).\nVectors in R can be created using the c() function, which stands for concatenate or combine:\n\n# numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\nManipulating Vectors\nR allows you to perform operations on vectors in a very intuitive way. Operations are vectorised, meaning they are applied element-wise:\n\n# arithmetic operations\nvector_sum &lt;- numeric_vector + 10  # Adds 10 to each element\n#show\nvector_sum\n\n[1] 11 12 13 14 15\n\n# vector mutliplication\nvector_multiplication &lt;- numeric_vector * 2  # Multiplies each element by 2\n#show\nvector_multiplication\n\n[1]  2  4  6  8 10\n\n# logical operations\nvector_greater_than_three &lt;- numeric_vector &gt; 3  # Returns a logical vector\n\n#show\nvector_greater_than_three\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nYou can access elements of a vector by using square brackets [ ] with an index or a vector of indices:\n\n#  the first element of numeric_vector\nfirst_element &lt;- numeric_vector[1]\n\n#show\nfirst_element\n\n[1] 1\n\n# multiple elements\nsome_elements &lt;- numeric_vector[c(2, 4)]  # Gets the 2nd and 4th elements\n\n#show\nfirst_element\n\n[1] 1\n\n\n\n\nFunctions with vectors\nR provides a rich set of functions for statistical computations and manipulations that work with vectors:\n\n# statistical summary\nvector_mean &lt;- mean(numeric_vector)\nvector_mean\n\n[1] 3\n\nvector_sum &lt;- sum(numeric_vector)\nvector_sum\n\n[1] 15\n\n# sorting\nsorted_vector &lt;- sort(numeric_vector)\nsorted_vector\n\n[1] 1 2 3 4 5\n\n# unique values\nunique_vector &lt;- unique(character_vector)\nunique_vector\n\n[1] \"apple\"  \"banana\" \"cherry\""
  },
  {
    "objectID": "content/01-content.html#order-your-r-studior-workflow",
    "href": "content/01-content.html#order-your-r-studior-workflow",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Order your R-studio/R workflow",
    "text": "Order your R-studio/R workflow\n\nClear folder structure\nClearly defined script names\nAnnotate your code"
  },
  {
    "objectID": "index.html#welcome-to-the-psych-434-lecture-website.",
    "href": "index.html#welcome-to-the-psych-434-lecture-website.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Accessing Lectures and Readings\n\n\n\n\nTo locate the assigned readings and lecture materials, please navigate to the Content tab located in the upper right-hand corner of the course platform.\n\n\n\n\n\n\n\n\n\nInstructor Availability\n\n\n\n\nOffice hour reserved post-lab for meetings.\nContact to schedule: joseph.bulbulia@vuw.ac.nz."
  },
  {
    "objectID": "content/01-content.html#why-learn-r",
    "href": "content/01-content.html#why-learn-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Why learn R?",
    "text": "Why learn R?\n\nYou’ll need it for your final report.\nSupports your psychology coursework.\nEnhances your coding skills. Presently all buzz is about AI. Amateur coders can greatly accelerate their abilities – but you still need to have some coding. If you have no coding skills, you will lose out to those that do.\nCoding can become as enjoyable as music or languages, really!"
  },
  {
    "objectID": "content/01-content.html#appendix-c-sundry",
    "href": "content/01-content.html#appendix-c-sundry",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix C: Sundry",
    "text": "Appendix C: Sundry\n\nUsing Logical Operators in Conditions\nThese operators are often used in conditional statements and loops:\n\nif (x &gt; 0 && y &gt; 0) {\n  print(\"both x and y are positive\")\n}\n\n[1] \"both x and y are positive\"\n\nz &lt;- c(TRUE, FALSE, TRUE)\nw &lt;- c(FALSE, TRUE, TRUE)\ncombined_logic &lt;- z | w # element-wise OR\n\ncombined_logic\n\n[1] TRUE TRUE TRUE"
  },
  {
    "objectID": "scripts/course-outline.html",
    "href": "scripts/course-outline.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\n\n\n\n(J. A. Bulbulia 2022) link\n\n\n\n\n\n\n\n\n\nUnderstanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data\n\n\n\n\n\n\n\n\nConfounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: simulation and ggdag\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6 link\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(J. A. Bulbulia 2023) link\n(Neal 2020) Chapter 3 link\n\n\n\n\n\n\n\n\n\nKey concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6-9 link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link\n\n\n\n\n\n\n\n\n\nAssessment covering key terms and concepts taught so far\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R for ATE estimation\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on ATE\n\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 4-5 link\n\n\n\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nKey concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents\n\n\n\n\n\n\n\n\nWorkflow for causal question formulation, population statement, and causal diagram creation\nEstimation techniques for ATE and CATE using R and documentation with LaTeX\n\n\n\n\n\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(J. A. Bulbulia 2024) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nPreparing your report analysis and manuscript: descriptive analysis and reporting\n\n\n\n\n\n\n\n\nSecond assessment covering advanced topics in causal inference\nTopics include ATE, CATE, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your report analysis and manuscript: causal analysis and reporting\n\n\n\n\n\n\n\n\nFactor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\nR exercises focusing on measurement theory applications and graphing\n\n\n\n\n\n\n\n\nEnhancing measurement and external validity in causal inference studies\nGuidance on writing introduction and method sections for research papers\n\n\n\n\n\n(Tyler J. VanderWeele 2022) link\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models\n\n\n\n\n\n\n\n\nAddressing missing data issues in research\nCreating and managing Quarto documents for research workflows\n\n\n\n\n\nNon-parametric causal estimation; inverse probability of censoring weights.\nHands on working with your manuscripts\n\n\n\n\n\n(Joseph A. Bulbulia et al. 2023) link\n\n\n\n\n\n\nYou assess a questions: do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "scripts/course-outline.html#week-1-feb-26---course-introduction---introduction-to-r",
    "href": "scripts/course-outline.html#week-1-feb-26---course-introduction---introduction-to-r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\n\n\n\n(J. A. Bulbulia 2022) link"
  },
  {
    "objectID": "scripts/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-structures-4-rules---r",
    "href": "scripts/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-structures-4-rules---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Understanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data"
  },
  {
    "objectID": "scripts/course-outline.html#week-3-march-11---causal-diagrams-confounding-bias---r",
    "href": "scripts/course-outline.html#week-3-march-11---causal-diagrams-confounding-bias---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nSuggested: Bulbulia (2023) link\n\n\n\n\n\nRegression and simulation exercises in R"
  },
  {
    "objectID": "scripts/course-outline.html#week-4-march-18---causal-diagrams-interaction-measurement-bias-selection-bias---r",
    "href": "scripts/course-outline.html#week-4-march-18---causal-diagrams-interaction-measurement-bias-selection-bias---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R"
  },
  {
    "objectID": "scripts/course-outline.html#week-5-march-25---quiz---r",
    "href": "scripts/course-outline.html#week-5-march-25---quiz---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far\nTerms include node, edge, arrow, conditional independence, d-separation, d-connection, confounding, over-adjustment bias, collider bias, the five elemental structural relationships, effect-modification, interaction, measurement error bias, selection bias."
  },
  {
    "objectID": "scripts/course-outline.html#week-6-april-15---causal-inference-ate",
    "href": "scripts/course-outline.html#week-6-april-15---causal-inference-ate",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R for ATE estimation\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on ATE"
  },
  {
    "objectID": "scripts/course-outline.html#week-7-april-22---causal-inference-effect-modification",
    "href": "scripts/course-outline.html#week-7-april-22---causal-inference-effect-modification",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents"
  },
  {
    "objectID": "scripts/course-outline.html#week-8-april-29---causal-inference-estimation-of-ate-and-cate",
    "href": "scripts/course-outline.html#week-8-april-29---causal-inference-estimation-of-ate-and-cate",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Workflow for causal question formulation, population statement, and causal diagram creation\nEstimation techniques for ATE and CATE using R and documentation with LaTeX\n\n\n\n\n\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(J. A. Bulbulia 2024) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nPreparing your report analysis and manuscript: descriptive analysis and reporting"
  },
  {
    "objectID": "scripts/course-outline.html#week-9-may-6---quiz---r",
    "href": "scripts/course-outline.html#week-9-may-6---quiz---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Second assessment covering advanced topics in causal inference\nTopics include ATE, CATE, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your report analysis and manuscript: causal analysis and reporting"
  },
  {
    "objectID": "scripts/course-outline.html#week-10-may-13---measurement-matters",
    "href": "scripts/course-outline.html#week-10-may-13---measurement-matters",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Factor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\nR exercises focusing on measurement theory applications and graphing"
  },
  {
    "objectID": "scripts/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference---r",
    "href": "scripts/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Enhancing measurement and external validity in causal inference studies\nGuidance on writing introduction and method sections for research papers\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models"
  },
  {
    "objectID": "scripts/course-outline.html#week-12-may-27---missing-data-workflows.",
    "href": "scripts/course-outline.html#week-12-may-27---missing-data-workflows.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Addressing missing data issues in research\nCreating and managing Quarto documents for research workflows\n\n\n\n\n\nNon-parametric causal estimation; inverse probability of censoring weights.\nHands on working with your manuscripts\n\n\n\n\n\n(Joseph A. Bulbulia et al. 2023) link"
  },
  {
    "objectID": "scripts/course-outline.html#assessment",
    "href": "scripts/course-outline.html#assessment",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "June 3: Comparative Report Due (40%)\n\nOption to focus on descriptive analysis or a specific causal question"
  },
  {
    "objectID": "scripts/course-outline.html#week-7-april-22---causal-inference-and-cate",
    "href": "scripts/course-outline.html#week-7-april-22---causal-inference-and-cate",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "(Effect Modification Across Groups)\n\n\n\nKey concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents"
  },
  {
    "objectID": "scripts/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-causal-structures",
    "href": "scripts/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-causal-structures",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Understanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data"
  },
  {
    "objectID": "scripts/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "href": "scripts/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: simulation and ggdag\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6 link\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(J. A. Bulbulia 2023) link\n(Neal 2020) Chapter 3 link"
  },
  {
    "objectID": "scripts/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias---r",
    "href": "scripts/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias---r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R"
  },
  {
    "objectID": "scripts/course-outline.html#week-5-march-25---quiztest-25",
    "href": "scripts/course-outline.html#week-5-march-25---quiztest-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far"
  },
  {
    "objectID": "scripts/course-outline.html#week-9-may-6---testquiz-25",
    "href": "scripts/course-outline.html#week-9-may-6---testquiz-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Second assessment covering advanced topics in causal inference\nTopics include ATE, CATE, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your report analysis and manuscript: causal analysis and reporting"
  },
  {
    "objectID": "scripts/course-outline.html#june-3-comparative-report-due-40",
    "href": "scripts/course-outline.html#june-3-comparative-report-due-40",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "scripts/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "href": "scripts/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "You assess a questions: do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "scripts/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "href": "scripts/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6-9 link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link"
  },
  {
    "objectID": "scripts/course-outline.html#week-7-april-22---causal-inference-and-cate-effect-modification",
    "href": "scripts/course-outline.html#week-7-april-22---causal-inference-and-cate-effect-modification",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "(Hernan and Robins 2024) Chapters 4-5 link\n\n\n\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nKey concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents"
  },
  {
    "objectID": "scripts/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference",
    "href": "scripts/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Enhancing measurement and external validity in causal inference studies\nGuidance on writing introduction and method sections for research papers\n\n\n\n\n\n(Tyler J. VanderWeele 2022) link\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models"
  },
  {
    "objectID": "scripts/course-outline.html#videos",
    "href": "scripts/course-outline.html#videos",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "&lt;iframe width=“560” height=“315” src=“https://www.youtube.com/watch?v=KNPYUVmY3NM frameborder=”0” allow=“accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture” allowfullscreen&gt;"
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The course outline provides a comprehensive overview of the topics covered each week, including important dates for assignments and exams. To access the course outline:\n\nNavigate to the Content tab located in the upper right-hand corner of the course platform.\n\n\n\n\nAll relevant links, including external resources, reading materials, and supplementary content, are provided within the course outline. Ensure to:\n\nCheck the course outline regularly for updates and additional resources.\n\n\n\n\n\n\n\nAccessing Lectures and Readings\n\n\n\n\nCourse Outline: Located under the Content tab in the upper right-hand corner of the course platform. This section contains a detailed schedule of topics, readings, and assignments.\nReadings: Directly linked within the Course Outline. Ensure to review these materials before attending the lectures.\nLecture Materials: For slides, video recordings, and other lecture materials, please navigate to the Content tab located in the upper right-hand corner of the course platform and select the relevant week."
  },
  {
    "objectID": "content/course-outline.html",
    "href": "content/course-outline.html",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\n\n\n\n(J. A. Bulbulia 2022) link\n\n\n\n\n\n\n\n\n\nUnderstanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data\n\n\n\n\n\n\n\n\nConfounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: simulation and ggdag\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6 link\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(J. A. Bulbulia 2023) link\n(Neal 2020) Chapter 3 link\n\n\n\n\n\n\n\n\n\nKey concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6-9 link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link\n\n\n\n\n\n\n\n\n\nAssessment covering key terms and concepts taught so far\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R for ATE estimation\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on ATE\n\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 4-5 link\n\n\n\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nKey concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents\n\n\n\n\n\n\n\n\nWorkflow for causal question formulation, population statement, and causal diagram creation\nEstimation techniques for ATE and CATE using R and documentation with LaTeX\n\n\n\n\n\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(J. A. Bulbulia 2024) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nPreparing your report analysis and manuscript: descriptive analysis and reporting\n\n\n\n\n\n\n\n\nSecond assessment covering advanced topics in causal inference\nTopics include ATE, CATE, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your report analysis and manuscript: causal analysis and reporting\n\n\n\n\n\n\n\n\nFactor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\nR exercises focusing on measurement theory applications and graphing\n\n\n\n\n\n\n\n\nEnhancing measurement and external validity in causal inference studies\nGuidance on writing introduction and method sections for research papers\n\n\n\n\n\n(Tyler J. VanderWeele 2022) link\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models\n\n\n\n\n\n\n\n\nAddressing missing data issues in research\nCreating and managing Quarto documents for research workflows\n\n\n\n\n\nNon-parametric causal estimation; inverse probability of censoring weights.\nHands on working with your manuscripts\n\n\n\n\n\n(Joseph A. Bulbulia et al. 2023) link\n\n\n\n\n\n\nYou assess a questions: do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "content/course-outline.html#week-1-feb-26---course-introduction---introduction-to-r",
    "href": "content/course-outline.html#week-1-feb-26---course-introduction---introduction-to-r",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Introduce course objectives and outline\nR setup\n\n\n\n\n\nGetting started with R/R-studio: installation and package management\n\n\n\n\n\n\n\n(J. A. Bulbulia 2022) link"
  },
  {
    "objectID": "content/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-causal-structures",
    "href": "content/course-outline.html#week-2-march-4---causal-diagrams-five-elementary-causal-structures",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Understanding causal diagrams: definitions and applications\nIntroduction to five elementary structures and four rules in causal inference\nIntroduction to R interface and data simulation\n\n\n\n\n\nBarrett M (2023). ggdag: Analyze and Create Elegant Directed Acyclic Graphs. R package version 0.2.7.9000, https://github.com/malcolmbarrett/ggdag\n“An Introduction to Directed Acyclic Graphs”, https://r-causal.github.io/ggdag/articles/intro-to-dags.html\n“Common Structures of Bias”, https://r-causal.github.io/ggdag/articles/bias-structures.html\n\n\n\n\n\nPractical exercises in R: Using the interface and simulating data"
  },
  {
    "objectID": "content/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "href": "content/course-outline.html#week-3-march-11---causal-diagrams-the-structures-of-confounding-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Confounding bias using causal diagrams\nApplication of regression and simulation in R\n\n\n\n\n\nPractical exercises in R: simulation and ggdag\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6 link\n\n\n\n\n(Suzuki, Shinozaki, and Yamamoto 2020) link\n(J. A. Bulbulia 2023) link\n(Neal 2020) Chapter 3 link"
  },
  {
    "objectID": "content/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "href": "content/course-outline.html#week-4-march-18---causal-diagrams-the-structures-of-interactioneffect-modification-measurement-bias-selection-bias",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of interaction, measurement bias, and selection bias understood through causal diagrams\nBoth External and Internal Validity clarified by Causal Graphs\nAdvanced regression and simulation exercises in R\n\n\n\n\n\nContinuation of regression and simulation exercises in R\n\n\n\n\n\n(Hernan and Robins 2024) Chapter 6-9 link\n\n\n\n\n(Miguel A. Hernán, Hernández-Díaz, and Robins 2004) link\n(M. A. Hernán 2017) link\n(Miguel A. Hernán and Cole 2009) link\n(Tyler J. VanderWeele and Hernán 2012) link"
  },
  {
    "objectID": "content/course-outline.html#week-5-march-25---quiztest-25",
    "href": "content/course-outline.html#week-5-march-25---quiztest-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment covering key terms and concepts taught so far"
  },
  {
    "objectID": "content/course-outline.html#week-6-april-15---causal-inference-ate",
    "href": "content/course-outline.html#week-6-april-15---causal-inference-ate",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Key concepts of Average Treatment Effect (ATE)\nApplication of regression and simulation in R for ATE estimation\n\n\n\n\n\n(Hernan and Robins 2024) Chapters 1-3 link\n\n\n\n\n(Neal 2020) Chapter 1-2 link\n\n\n\n\n\n\nRegression and simulation exercises in R focussed on ATE"
  },
  {
    "objectID": "content/course-outline.html#week-7-april-22---causal-inference-and-cate-effect-modification",
    "href": "content/course-outline.html#week-7-april-22---causal-inference-and-cate-effect-modification",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "(Hernan and Robins 2024) Chapters 4-5 link\n\n\n\n\n(Tyler J. VanderWeele and Robins 2007) link\n(Tyler J. VanderWeele 2009) link\n\n\n\n\n\nKey concepts of Conditional Average Treatment Effect (CATE)\nAdvanced regression and simulation exercises in R for CATE estimation\n\n\n\n\n\nPreparing your report analysis and manuscript: setting up your workflow/ quarto documents"
  },
  {
    "objectID": "content/course-outline.html#week-8-april-29---causal-inference-estimation-of-ate-and-cate",
    "href": "content/course-outline.html#week-8-april-29---causal-inference-estimation-of-ate-and-cate",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Workflow for causal question formulation, population statement, and causal diagram creation\nEstimation techniques for ATE and CATE using R and documentation with LaTeX\n\n\n\n\n\n(Tyler J. VanderWeele, Mathur, and Chen 2020) link\n\n\n\n\n(J. A. Bulbulia 2024) link\n(Hoffman et al. 2023) link\n\n\n\n\n\n\nPreparing your report analysis and manuscript: descriptive analysis and reporting"
  },
  {
    "objectID": "content/course-outline.html#week-9-may-6---testquiz-25",
    "href": "content/course-outline.html#week-9-may-6---testquiz-25",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Second assessment covering advanced topics in causal inference\nTopics include ATE, CATE, fundamental assumptions of causal inference, experiments, and real-world confounding\n\n\n\n\n\nPreparing your report analysis and manuscript: causal analysis and reporting"
  },
  {
    "objectID": "content/course-outline.html#week-10-may-13---measurement-matters",
    "href": "content/course-outline.html#week-10-may-13---measurement-matters",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Factor analysis, confirmatory factor analysis (CFA), multigroup CFA, partial invariance\nWorked example on configural, metric, and scalar equivalence\n\n\n\n\n\n(Fischer and Karl 2019) link\n\n\n\n\n(Vijver et al. 2021) link\n(He and Vijver 2012) link\n(J. [et. al]. Harkness 2003) link\n\n\n\n\n\n\nR exercises focusing on measurement theory applications and graphing"
  },
  {
    "objectID": "content/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference",
    "href": "content/course-outline.html#week-11-may-20---measurement-external-validity-in-causal-inference",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Enhancing measurement and external validity in causal inference studies\nGuidance on writing introduction and method sections for research papers\n\n\n\n\n\n(Tyler J. VanderWeele 2022) link\n\n\n\n\n(J. A. Harkness, Van de Vijver, and Johnson 2003) link\n\n\n\n\n\n\nR programming using causal inference to examine failure modes in measurement models"
  },
  {
    "objectID": "content/course-outline.html#week-12-may-27---missing-data-workflows.",
    "href": "content/course-outline.html#week-12-may-27---missing-data-workflows.",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Addressing missing data issues in research\nCreating and managing Quarto documents for research workflows\n\n\n\n\n\nNon-parametric causal estimation; inverse probability of censoring weights.\nHands on working with your manuscripts\n\n\n\n\n\n(Joseph A. Bulbulia et al. 2023) link"
  },
  {
    "objectID": "content/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "href": "content/course-outline.html#week-13-june-3-no-seminar-comparative-report-due-40",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "You assess a questions: do cultural groups vary in response to interventions on well-beings?"
  },
  {
    "objectID": "index.html#course-outline-tab",
    "href": "index.html#course-outline-tab",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The Course Outline tab is specifically designed to give students an easy access to the comprehensive overview of the course, including topics covered each week, links to readings, important dates for assignments, exams, and a repository of necessary links and resources.\nTo access the Course Outline:\n\nNavigate to the Course Outline tab located prominently within the course platform.\n\n\n\n\n\n\n\nAccessing Lectures and Readings\n\n\n\n\nCourse Outline: Now conveniently located under the Course Outline tab. This section provides a detailed schedule of topics, readings, and assignments.\nReadings: Directly linked within the Course Outline tab. Make sure to review these materials to prepare for lectures and discussions.\nLecture Materials: For accessing slides, video recordings, and other lecture materials, refer to the Course Outline tab. All materials are organized weekly for easy navigation and access."
  },
  {
    "objectID": "index.html#contents-tab",
    "href": "index.html#contents-tab",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "The Contents tab offers direct access to weekly seminar and lab materials, including lecture outlines and lab resources.\n\nAccess it from the top right of the course platform by selecting the appropriate week.\nLab materials are available one week before the lecture; seminar review materials post-seminar."
  },
  {
    "objectID": "index.html#assignments-and-due-dates-1",
    "href": "index.html#assignments-and-due-dates-1",
    "title": "Psych 434: Conducting Research Across Cultures",
    "section": "",
    "text": "Assessment\nCLOs\nPercent\nDue\n\n\n\n\nClass participation\n1,2,3\n10\nWeekly\n\n\nTest 1\n2\n25\n25 March (w5)\n\n\nTest 2\n2\n25\n6 May (w9)\n\n\nResearch Report\n1,2,3\n40\n3 June (w12)\n\n\n\n\n\n\n\n\n\n\n\nAttendance and Participation:\n\n\nClass attendance and active participation.\n\n\nLab attendance and active participation.\n\n\n\n\n\n\n\n\n\n\n\n\nTest Guidelines\n\n\n\n\nTest duration is under an hour. The allocated time is nearly two hours.\nEach lecture starts and ends with key concept definitions and reviews for the test.\nR or RStudio knowledge isn’t part of the test. R support aims to enhance research report skills.\nTests are conducted in the lecture room.\nRequired: pen/pencil. Computers are not allowed.\n\n\n\n\nFirst Test (50 minutes, total time allowed: 1 hour 50 minutes):\n\nFocuses on revising core statistical and methodological concepts.\nAims to refresh basic statistical knowledge foundational for later course material.\n\nSecond Test (50 minutes, total time allowed: 1 hour 50 minutes):\n\nBuilds upon the first test’s concepts, emphasising the application of basic conceptual, statistical, and theoretical knowledge.\n\n\n\n\n\n\n\n\n\n\n\nResearch Report Instructions\n\n\n\n\nWe will supply the data.\nLab sessions are designed to support you in this assignment.\nWe assume no statistical background.\n\n\n\n\nTitle: “Causal Inference in Cultural Psychology: Examining Exposure Effects on Dimensions of Well-being Modified by Cultural or Sociodemographic Categories”.\nObjective:\n\nTo quantify the causal effect of a specific exposure on well-being dimensions, modified by sociodemographic categories (born_nz, eth_cat, big_doms, gen_cohort) using the NZAVS longitudinal synthetic dataset.\n\nInstructions:\n\nTheoretical Interest and Research Question:\n\nDescribe the significance of your chosen exposure and its potential impact on the selected outcomes, modified by the cultural or sociodemographic category.\nState the research question clearly.\n\nDirected Acyclic Graph (DAG):\n\nConstruct a DAG illustrating the relationships between exposure, outcomes, sociodemographic category, and potential bias sources. Ensure clarity in labelling.\n\nConfounding Control Strategy:\n\nOutline your strategy for confounding control, justifying the chosen confounders.\n\nMeasurement Biases:\n\nAddress and analyse measurement biases as relevant.\n\nAssumptions and Statistical Models:\n\nDiscuss the assumptions of your causal inference approach and your statistical model, including their limitations.\n\n\nRequirements:\n\nIntroduction: 1,500 words limit.\nConclusion: 1,500 words limit.\nMethod and Results sections should be concise; no specific word limit.\nUse any useful sources, citing appropriately to avoid academic misconduct.\nFollow APA style for citations and references.\nInclude tables/figures as needed.\nSubmit as a single PDF, including R code in an appendix.\n\nEvaluation Criteria:\n\nClarity of theoretical framework, research question, and design.\nValidity of confounding control strategy.\nDiscussion on assumptions and statistical models.\nOrganisation and presentation quality.\n\n\n\n\n\n\nExtensions:\n\nNegotiate a new due date by writing (email) before March 10th, 2024, if necessary.\n\nPenalties:\n\nLate submissions incur a one-grade-per-day penalty.\nOver-length assignments will be penalised.\n\nUnforeseeable Events:\n\nExtensions post-March 10 require evidence (e.g., medical certificate).\n\n\n\n\n\n\nBring a laptop with R and RStudio installed for data analysis sessions. Contact the instructor if you lack computer access.\nFor in-class tests, bring a writing utensil. Electronic devices are not permitted."
  },
  {
    "objectID": "content/01-content.html#base-r-graphs",
    "href": "content/01-content.html#base-r-graphs",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Base R Graphs",
    "text": "Base R Graphs\nAlthough ggplot2 is renowned for its flexibility and aesthetic appeal, Base R graphics remain a staple for straightforward and quick visualisations.\nBase R provides a set of plotting functions that are readily available without the need for additional packages, and it is speedy.\n\nBasic Plotting Functions\n\nplot(): the workhorse of Base R for creating scatter plots, line graphs, and more, with extensive customisation options.\nhist(): generates histograms to explore the distribution of a single continuous variable.\nboxplot(): useful for comparing distributions across groups, showing medians, quartiles, and outliers.\nbarplot(): Creates bar graphs for visualising categorical data.\n\n\n\nCreating a Basic Scatter Plot with Base R\nConsider simple scatter plot using the data we have just simulated\n\n# basic scatter plot with Base R\nplot(student_data$study_hours, student_data$score,\n    main = \"Scatter Plot of Scores vs. Study Hours\",\n    xlab = \"Study Hours\", ylab = \"Score\",\n    pch = 19, col = ifelse(student_data$passed == \"passed\", \"blue\", \"red\")\n)\n\n\n\n\n\n\n\n\nThis plot uses the plot function to create a scatter plot, with study hours on the x-axis and scores on the y-axis. The pch parameter specifies the symbol type, and col changes the colour based on whether the student passed or failed.\n\n\nGenerating a Histogram with Base R\nTo visualise the distribution of student scores:\n\n# histogram with Base R\nhist(student_data$score,\n    breaks = 5,\n    col = \"skyblue\",\n    main = \"Histogram of Student Scores\",\n    xlab = \"Scores\",\n    border = \"white\"\n)\n\n\n\n\n\n\n\n\nThis histogram provides a quick overview of the scores’ distribution, using the hist function with specified breaks, col for colour, and border for the colour of the histogram borders.\n\n\nGenerate Line Plot with Base R\n\n# must be numeric\nmonths_num &lt;- 1:length(study_data$month) # Simple numeric sequence\n\n# plot points with suppressed x-axis\nplot(months_num, study_data$study_hours,\n    type = \"p\", # Points\n    pch = 19, # Type of point\n    col = \"red\",\n    xlab = \"Month\",\n    ylab = \"Study Hours\",\n    main = \"Monthly Study Hours\",\n    xaxt = \"n\"\n) # Suppress the x-axis\n\n# add lines between points\nlines(months_num, study_data$study_hours,\n    col = \"blue\",\n    lwd = 1\n) # Line width\n\n# add custom month labels to the x-axis at appropriate positions\naxis(1, at = months_num, labels = study_data$month, las = 2) # `las=2` makes labels perpendicular to axis\n\n# Optional: adding a box around the plot for a minimalistic look\nbox()\n\n\n\n\n\n\n\n\n\n\nComparing Distributions with Box Plots\nBox plots in Base R can compare the score distributions across the pass/fail status:\n\n# Box plot with Base R\nboxplot(score ~ passed,\n    data = student_data,\n    main = \"Score Distribution by Pass/Fail Status\",\n    xlab = \"Status\", ylab = \"Scores\",\n    col = c(\"red\", \"blue\")\n)\n\n\n\n\n\n\n\n\nThis code uses the boxplot function to create box plots for scores, grouped by the pass/fail status, with custom colours for each group."
  },
  {
    "objectID": "content/01-content.html#lab-summary",
    "href": "content/01-content.html#lab-summary",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Lab Summary",
    "text": "Lab Summary\nCongratulations on completing Lab 1!\nThis session has laid the groundwork. We’ve covered a lot, but we’ll have a good deal of practice throughout the course.\n\nWhat We Have Learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use RStudio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nBasic R Data Structures such as:\n\nVectors and Matrices: You have learned to create and manipulate vectors and matrices, the simplest forms of data storage in R, which are crucial for handling numeric, character, and logical data types in a structured manner.\nData Frames: You’ve been introduced to data frames, a key data structure in R for storing tabular data. Data frames accommodate columns of different data types, making them highly versatile for data analysis and manipulation.\nFactors and Ordered Factors: Understanding factors and ordered factors has provided you with the tools to handle categorical data effectively, including the ability to manage and analyse data involving categorical variables with both unordered and ordered levels.\n\nBasics of ggplot2:\n\nYou’ve been equipped with the fundamentals of data visualisation using ggplot2, including how to create basic plots like bar charts, scatter plots, and line graphs. You’ve learned about the importance of aesthetics (aes) and geometries (geom_ functions) in creating visually appealing and informative graphics.\n\nCustomizing Plots:\n\nTechniques for enhancing plots with titles, axis labels, and custom colour schemes have been covered. You’ve practised making your visualisations more informative and engaging by customising plot aesthetics.\n\n\nHow to Build Skills?\n\nPractical Application:\n\nDo the hands-on exercises at home. They’ll help you apply what you have learned here.\n\n\nWhere to Get Help\nAs sure as night follows day, you will need help coding. Good resources:\n\nLarge Language Models (LLMs): OpenAI’s premium LLM (GPT-4) outperforms the free version (GPT-3.5) for complex queries. I don’t think LLM’s are quite ready for science, but they are remarkably helpful for coding, and for learning how to code. So please use them, with caution, but use them.\nStack Overflow: a valuable resource for coding advice and solutions.\nCross-validated is probably the best place to go for stats advice. (LLMs are currently not reliable for statisticis for which there is not much information – which is most of science.)\nDeveloper Websites and GitHub Pages: Directly engage with package developers and the community for insights and support.Parameters package discussion page offers insights and support directly from its developers and user community.\nYour tutors and lecturer. We care. We’re here to help you!\n\n\n\nRecommended Reading\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/ For now, only lecture 1 would be relevant. It’s worth noting that this lecture covers working with GitHub. GitHub is a useful tool, you should learn how to use it; however, it is not a requirement for the current course.\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Xie Y (2023). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.49, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "content/01-content.html#data-types-in-r",
    "href": "content/01-content.html#data-types-in-r",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Data Types in R",
    "text": "Data Types in R\nUnderstanding data types in R is essential. R supports several fundamental data types, including integers, characters, factors, and ordered factors. Each type has its specific use case and functions associated with it.\n\nIntegers\nIntegers are whole numbers without decimal points. In R, integers can be explicitly defined by adding an L suffix to the number.\n\n# define an integer\nx &lt;- 42L\n\nx\n\n[1] 42\n\n# check\nstr(x) # is integer\n\n int 42\n\n# convert to numeric\ny &lt;- as.numeric(x)\n\nstr(y)\n\n num 42\n\n\nIntegers are particularly useful when dealing with counts or indices that do not require fractional values.\n\n\nCharacters\nCharacter data types are used to represent text. In R, text strings are enclosed in quotes, either single (') or double (\").\n\n# define a character string\nname &lt;- \"Alice\"\n\nCharacters are essential for categorical data that does not fit into numerical categories, such as names, labels, and descriptions.\n\n\nFactors\nFactors are used to represent categorical data that can take on a limited number of values, known as levels. Factors are useful for statistical modeling as they explicitly define categories in the data.\n\n# Define a factor\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\"))\n\nFactors can improve efficiency and memory usage when dealing with categorical data, especially in large datasets. And they are useful when dealing with categorical variables (naturally).\n\nOrdered Factors\nOrdered factors are a special type of factor where the levels have an inherent order. They are defined similarly to factors but with an additional argument to denote the order.\n\n# ordered factors ---------------------------------------------------------\n# factors ordinary\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_factor_no_order &lt;- factor(education_levels, ordered = FALSE)\nstr(education_factor_no_order)\n\n Factor w/ 4 levels \"bachelor\",\"high school\",..: 2 1 3 4\n\n# factors with inherent order\neducation_factor &lt;- factor(education_levels, ordered = TRUE)\neducation_factor\n\n[1] high school bachelor    master      ph.d.      \nLevels: bachelor &lt; high school &lt; master &lt; ph.d.\n\n# another way to do the same\neducation_ordered_explicit &lt;- factor(education_levels, levels = education_levels, ordered = TRUE)\n\nOrdered factors allow for logical comparisons based on their order, which is particularly useful in analyses where the order of categories matters, such as ordinal regression.\n\n\nOperations with Ordered Factors\nOrdered factors support logical comparisons that consider the order of the levels.\n\n# comparison of ordered factors\nedu1 &lt;- ordered(\"bachelor\", levels = education_levels)\nedu2 &lt;- ordered(\"master\", levels = education_levels)\nedu2 &gt; edu1 # logical comparison\n\n[1] TRUE\n\n# modifying ordered factors\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\n\n\n\nChecking Data with Ordered Factors\nYou can view the structure and summary of ordered factors just as with regular factors, but the output will indicate the order.\n\n# view the structure\nstr(education_ordered_explicit)\n\n Ord.factor w/ 4 levels \"high school\"&lt;..: 1 2 3 4\n\n# summary to see the distribution\nsummary(education_ordered_explicit)\n\nhigh school    bachelor      master       ph.d. \n          1           1           1           1 \n\n\n\n\nModifying Ordered Factors\nIf you need to change the order of levels or add new levels, you can re-factor the variable using factor() or ordered() and specify the new levels.\n\n# modifying ordered factors\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5"
  },
  {
    "objectID": "content/01-content.html#factors-1",
    "href": "content/01-content.html#factors-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Factors",
    "text": "Factors\nFactors are used to represent categorical data.\n\ngender &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\"))\n\nhead(gender)\n\n[1] Male   Female Female Male  \nLevels: Female Male"
  },
  {
    "objectID": "content/01-content.html#strings",
    "href": "content/01-content.html#strings",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Strings",
    "text": "Strings\nStrings are sequences of characters.\n\n# sequences of characters\nyou &lt;- \"world!\"\ngreeting &lt;- paste(\"hello,\", you)\n# hello world\ngreeting\n\n[1] \"hello, world!\""
  },
  {
    "objectID": "content/01-content.html#vectors",
    "href": "content/01-content.html#vectors",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Vectors",
    "text": "Vectors\nVectors are one of R’s most fundamental data structures, essential for storing and manipulating a sequence of data elements.\nVectors are homogenous, meaning all elements in a vector must be of the same type (e.g., all numeric, all character, etc.).\nVectors in R can be created using the c() function, which stands for concatenate or combine:\n\n# numeric vector\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\n\n# character vector\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# logical vector\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\nManipulating Vectors\nR allows you to perform operations on vectors in a very intuitive way. Operations are vectorised, meaning they are applied element-wise:\n\n# arithmetic operations\nvector_sum &lt;- numeric_vector + 10 # Adds 10 to each element\n# show\nvector_sum\n\n[1] 11 12 13 14 15\n\n# vector mutliplication\nvector_multiplication &lt;- numeric_vector * 2 # Multiplies each element by 2\n# show\nvector_multiplication\n\n[1]  2  4  6  8 10\n\n# logical operations\nvector_greater_than_three &lt;- numeric_vector &gt; 3 # Returns a logical vector\n\n# show\nvector_greater_than_three\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nYou can access elements of a vector by using square brackets [ ] with an index or a vector of indices:\n\n#  the first element of numeric_vector\nfirst_element &lt;- numeric_vector[1]\n\n# show\nfirst_element\n\n[1] 1\n\n# multiple elements\nsome_elements &lt;- numeric_vector[c(2, 4)] # Gets the 2nd and 4th elements\n\n# show\nfirst_element\n\n[1] 1\n\n\n\n\nFunctions with vectors\nR provides a rich set of functions for statistical computations and manipulations that work with vectors:\n\n# statistical summary\nvector_mean &lt;- mean(numeric_vector)\nvector_mean\n\n[1] 3\n\nvector_sum &lt;- sum(numeric_vector)\nvector_sum\n\n[1] 15\n\n# sorting\nsorted_vector &lt;- sort(numeric_vector)\nsorted_vector\n\n[1] 1 2 3 4 5\n\n# unique values\nunique_vector &lt;- unique(character_vector)\nunique_vector\n\n[1] \"apple\"  \"banana\" \"cherry\""
  },
  {
    "objectID": "content/01-content.html#data-frames",
    "href": "content/01-content.html#data-frames",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Data Frames",
    "text": "Data Frames\n\nCreate Data Frames\nData frames can be created using the data.frame() function, specifying each column and its values. Here’s a simple example:\n\n# clear any previous `df` object\nrm(df)\ndf &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\"),\n    age = c(25, 30, 35),\n    gender = c(\"female\", \"male\", \"male\")\n)\n# check structure\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\ntable(df$gender)\n\n\nfemale   male \n     1      2 \n\ntable(df$age)\n\n\n25 30 35 \n 1  1  1 \n\ntable(df$name)\n\n\n  alice     bob charlie \n      1       1       1 \n\n\nIn this example, df is a data frame with three columns (name, age, gender) and three rows, each representing a different individual.\n\n\nAccess Data Frame Elements\nThere are often several ways to do the same thing in R. You can access the elements of a data frame in several ways:\n\nBy column name: use the $ operator followed by the column name.\n\n\nnames &lt;- df$name # extracts the `name` column\nnames\n\n[1] \"alice\"   \"bob\"     \"charlie\"\n\n\n\nBy row and column: Use the [row, column] indexing. Rows or columns can be specified by number or name.\n\n\n# access data frame elements\nnames &lt;- df$name\nnames\n\n[1] \"alice\"   \"bob\"     \"charlie\"\n\nsecond_person &lt;- df[2, ]\nsecond_person\n\n  name age gender\n2  bob  30   male\n\nage_column &lt;- df[, \"age\"]\nage_column\n\n[1] 25 30 35\n\n\n\nUsing subset() Function: To extract subsets of the data frame based on conditions.\n\n\n# use subset()\nvery_old_people &lt;- subset(df, age &gt; 25) # extracts rows where `age` is greater than 18\nvery_old_people\n\n     name age gender\n2     bob  30   male\n3 charlie  35   male\n\nsummary(very_old_people$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   31.25   32.50   32.50   33.75   35.00 \n\nmean(very_old_people$age)\n\n[1] 32.5\n\nmin(very_old_people$age)\n\n[1] 30\n\n\n\n\nExplore Your Data Frames\nFunctions such as head(), tail(), and str() help you explore the first few rows, last few rows, and the structure of the data frame, respectively.\n\nhead(df) # first six rows\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\ntail(df) # last six rows\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df) # structure of the data frame\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\n\n\n\nManipulating Data Frames\nData frames can be manipulated in various ways:\n\nAdding Columns: You can add new columns using the $ operator.\n\n\n# adds a new column \"employed\"\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n\n# show\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n\n\n\nAdding rows: Use the rbind() function to add new rows.\n\n\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n\n# show\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  28 female     TRUE\n\n\n\nModifying values: Access the element or column and assign it a new value.\n\n\n# note brackets\n# changes diana's age to 26\ndf[4, \"age\"] &lt;- 26\n\n# view row\ndf[4, ]\n\n   name age gender employed\n4 diana  26 female     TRUE\n\n\n\nRemoving columns or rows: set columns to NULL to remove them, or use - with row or column indices.\n\n\nhead(df)\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE\n\n# remove employed column\ndf$employed &lt;- NULL\n\n# check\ndf\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n4   diana  26 female\n\n# remove fourth row (Diana)\ndf &lt;- df[-4, ] # removes the fourth row\n\n# show\ndf\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\n\n\n\nAdd Rows with rbind()\nThe rbind() function in R stands for “row bind”. It is used to combine data frames or matrices by rows. This function is particularly useful when you want to add new observations or records to an existing data frame.\n\n#  adding a new row\nnew_person &lt;- data.frame(name = \"eve\", age = 32, gender = \"female\")\ndf &lt;- rbind(df, new_person)\n\n# verify the row addition\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n4     eve  32 female\n\n\nWhen using rbind(), ensure that the columns in the data frames being combined match in both name and order. If they do not match, you may encounter errors or unexpected results.\n\n\nAdding Columns with cbind()\nConversely, the cbind() function in R stands for “column bind”. It is used to combine data frames or matrices by columns. This function allows you to add new variables to an existing data frame.\n\n# example of adding a new column\ndf$occupation &lt;- c(\"engineer\", \"doctor\", \"artist\", \"doctor\") # direct assignment\n\n# or using cbind for a separate vector\noccupation_vector &lt;- c(\"engineer\", \"doctor\", \"artist\", \"doctor\")\ndf &lt;- cbind(df, occupation_vector)\n\n# verify the column addition\nhead(df)\n\n     name age gender occupation occupation_vector\n1   alice  25 female   engineer          engineer\n2     bob  30   male     doctor            doctor\n3 charlie  35   male     artist            artist\n4     eve  32 female     doctor            doctor\n\n\nAs with rbind(), when using cbind(), it is crucial that the data frames or vectors being combined have compatible dimensions. For cbind(), the number of rows must match.\n\n\nConsiderations for rbind() and cbind()\nWhile rbind() and cbind() are straightforward and powerful functions for combining data, they have some limitations:\n\nMatching Column or Row Names: for rbind(), the column names between the data frames need to match exactly. For cbind(), the row numbers must be equal.\nFactor Levels: when binding factors with different levels, R will unify the levels, which can sometimes lead to unexpected results. Be mindful of factor levels when using these functions.\nPerformance: we will use a different approach next week (and following), draing on thedplyr package, which will connect more easily to our workflows.\n\nBy understanding and utilizing rbind() and cbind(), you can efficiently manipulate the structure of your data frames, adding flexibility to your data analysis workflows in R.\n\n\nView Data Structure (summary(), str(), head(), tail())\n\nsummary(): Provides a summary of an object’s structure.\nstr(): Displays the structure of an object.\nhead(): Shows the first few rows of a data frame or the first elements of a vector.\ntail(): Shows the last few rows of a data frame or the last elements of a vector.\n\n\n# iris is a preloaded dataset\nstr(iris) # displays structure of scores_df\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris) # summary statistics\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nhead(iris) # first few rows\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris) # last few rows\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\n\n\nmean()\n\nCalculates the arithmetic mean of a numerical object.\n\n\nset.seed(12345)\n\n# we will cover R’s powerful simulation functions like `rnorm()`next week\nvector &lt;- rnorm(n = 40, mean = 0, sd = 1)\nmean(vector) # note the sampling error here\n\n[1] 0.2401853\n\n\n\n\nsd()\n\nComputes the standard deviation, which measures the amount of variation or dispersion of a set of values.\n\n\nsd(vector) # replace 'vector' with your numerical vector\n\n[1] 1.038425\n\n\n\n\nmin() and max()\n\nThese functions return a numerical object’s minimum and maximum values, respectively.\n\n\nmin(vector) # minimum value\n\n[1] -1.817956\n\nmax(vector) # maximum value\n\n[1] 2.196834\n\n\n\n\ntable()\n\nGenerates a frequency table of an object, useful for categorical data. It counts the number of occurrences of each unique element.\n\n\n#  seed for reproducibility\nset.seed(12345)\nstudent_data &lt;- data.frame(\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n    score = sample(80:100, 8, replace = TRUE),\n    stringsasfactors = FALSE\n)\n\n# determine pass/fail\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\n# convert 'passed' to factor\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\n\n# simulate study hours\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)\n# table for categorical data analysis\ngender &lt;- sample(c(\"male\", \"female\"), size = 100, replace = TRUE, prob = c(0.5, 0.5))\neducation_level &lt;- sample(c(\"high school\", \"bachelor\", \"master\"), size = 100, replace = TRUE, prob = c(0.4, 0.4, 0.2))\ndf_table_example &lt;- data.frame(gender, education_level)\ntable(df_table_example)\n\n        education_level\ngender   bachelor high school master\n  female       14          18     11\n  male         23          20     14\n\n\n\n\nCross-Tabulation with table()\n\ntable() can also be used for cross-tabulation, providing a way to analyse the relationship between two or more factors.\n\n\ntable(df_table_example$gender, df_table_example$education_level) # crosstab\n\n        \n         bachelor high school master\n  female       14          18     11\n  male         23          20     14\n\n\nThis produces a contingency table showing the counts at each combination of factor1 and factor2 levels.\n\n\nSummary Statistics\nUse summary() to get a summary of each column.\n\n# show\nsummary(df)\n\n     name                age           gender           occupation       \n Length:4           Min.   :25.00   Length:4           Length:4          \n Class :character   1st Qu.:28.75   Class :character   Class :character  \n Mode  :character   Median :31.00   Mode  :character   Mode  :character  \n                    Mean   :30.50                                        \n                    3rd Qu.:32.75                                        \n                    Max.   :35.00                                        \n occupation_vector \n Length:4          \n Class :character  \n Mode  :character"
  },
  {
    "objectID": "content/01-content.html#summary-of-todays-lab",
    "href": "content/01-content.html#summary-of-todays-lab",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Summary of Today’s Lab",
    "text": "Summary of Today’s Lab\nCongratulations on completing Lab 1!\nThis session has laid the groundwork. We have covered a lot, but we’ll have a good deal of practice throughout the course to reinforce the learning.\n\nWhat We Have Learned\n\nHow to install and setup R:\n\nYou’ve successfully installed R and RStudio, setting up your workstation for statistical analysis.\n\nHow to install and use R-Studio:\n\nYou’ve familiarised yourself with the RStudio interface, including the console, source editor, environment tab, and other utilities for effective data analysis.\n\nBasic R operations:\n\nYou’ve practided using R for basic arithmetic operations, understanding how to execute simple commands in the console.\n\nBasic R Data Structures such as:\n\nVectors and Matrices: You have learned to create and manipulate vectors and matrices, the simplest forms of data storage in R, which are crucial for handling numeric, character, and logical data types in a structured manner.\nData Frames: You’ve been introduced to data frames, a key data structure in R for storing tabular data. Data frames accommodate columns of different data types, making them highly versatile for data analysis and manipulation.\nFactors and Ordered Factors: Understanding factors and ordered factors has provided you with the tools to handle categorical data effectively, including the ability to manage and analyse data involving categorical variables with both unordered and ordered levels.\n\nBasics of ggplot2:\n\nYou’ve been equipped with the fundamentals of data visualisation using ggplot2, including how to create basic plots like bar charts, scatter plots, and line graphs. You’ve learned about the importance of aesthetics (aes) and geometries (geom_ functions) in creating visually appealing and informative graphics.\n\nCustomizing Plots:\n\nTechniques for enhancing plots with titles, axis labels, and custom colour schemes have been covered. You’ve practised making your visualisations more informative and engaging by customising plot aesthetics.\n\n\nHow to Build Skills?\n\nPractical Application:\n\nDo the hands-on exercises at home. They’ll help you apply what you have learned here.\n\n\nWhere to Get Help\nAs sure as night follows day, you will need help coding. Good resources:\n\nLarge Language Models (LLMs): LLMs are trained on extensive datasets. They are extremely good coding tutors. Open AI’s GPT-4 considerably outperforms GPT-3.5. However GPT 3.5 should be good enough. Gemini has a two-month free trial. LLM’s are rapidly evolving. However, presently, to use these tools, and to spot their errors, you will need to know how to code. Which is fortunate because coding makes you smarter!\n\nNote: you will not be assessed for R-code. Help from LLM’s for coding does not consitute a breach of academic integrity in this course. Your tests are in-class; no LLM’s allowed. For your final report, you will need to cite all sources, and how you used them, including LLMs.\n\nStack Overflow:: an outstanding resource for most problems. Great community.\nCross-validated the best place to go for stats advice. (LLM’s are only safe for standard statistics. They do not perform well for causal inference.)\nDeveloper Websites and GitHub Pages: Tidyverse\nYour tutors and course coordinator. We care. We’re here to help you!\n\n\n\nRecommended Reading\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/.\nMaterials from a previous course on learning R can be accessed here. https://go-bayes.github.io/psych-447/\nJohannes Karl’s Video\n\n\n\nJohannas Karl on Getting Started In R\n\n\n\nPackages\n\nreport::cite_packages()\n\n  - Chang W (2023). _extrafont: Tools for Using Fonts_. R package version 0.19, &lt;https://CRAN.R-project.org/package=extrafont&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Xie Y (2023). _tinytex: Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents_. R package version 0.49, &lt;https://github.com/rstudio/tinytex&gt;. Xie Y (2019). \"TinyTeX: A lightweight, cross-platform, and easy-to-maintain LaTeX distribution based on TeX Live.\" _TUGboat_, *40*(1), 30-32. &lt;https://tug.org/TUGboat/Contents/contents40-1.html&gt;."
  },
  {
    "objectID": "laboratory/Readme.html",
    "href": "laboratory/Readme.html",
    "title": "PSYC 434 Conducting Research Across Cultures: Trimester 1, 2024",
    "section": "",
    "text": "Structure is as follows:\n\n\n01-lab.R - introduction to R script: intsallation/basic commands/graphing"
  },
  {
    "objectID": "laboratory/Readme.html#labs",
    "href": "laboratory/Readme.html#labs",
    "title": "PSYC 434 Conducting Research Across Cultures: Trimester 1, 2024",
    "section": "",
    "text": "Structure is as follows:\n\n\n01-lab.R - introduction to R script: intsallation/basic commands/graphing"
  },
  {
    "objectID": "content/01-content.html#appendix-b-solutions",
    "href": "content/01-content.html#appendix-b-solutions",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix B: Solutions",
    "text": "Appendix B: Solutions\n\nSolutions 1 and 2\n\nExercises 1 and 2 have no solutions. Installation worked or it did not! If you have trouble, please see your tutor or instructor.\n\n\n\nSolution Exercise 3: Basic Operations and Data Structure Manipulation\n\n# e.g. create vectors\nvector_a &lt;- c(2, 4, 6, 8)\nvector_b &lt;- c(1, 3, 5, 7)\n\n# operations\nsum_vector &lt;- vector_a + vector_b\ndiff_vector &lt;- vector_a - vector_b\ndouble_vector_a &lt;- vector_a * 2\nhalf_vector_b &lt;- vector_b / 2\n\n# view\nsum_vector\n\n[1]  3  7 11 15\n\ndiff_vector\n\n[1] 1 1 1 1\n\ndouble_vector_a\n\n[1]  4  8 12 16\n\nhalf_vector_b\n\n[1] 0.5 1.5 2.5 3.5\n\n# Mean and Standard Deviation\nmean_a &lt;- mean(vector_a)\nsd_a &lt;- sd(vector_a)\nmean_b &lt;- mean(vector_b)\nsd_b &lt;- sd(vector_b)\n\n# view\nmean_a\n\n[1] 5\n\nsd_a\n\n[1] 2.581989\n\nmean_b\n\n[1] 4\n\nsd_b\n\n[1] 2.581989\n\n\n\n\nSolution 4: Working with Data Frames\n\n# create data frame\nstudent_data &lt;- data.frame(\n    id = 1:4,\n    name = c(\"alice\", \"bob\", \"charlie\", \"diana\"),\n    score = c(88, 92, 85, 95),\n    stringsAsFactors = FALSE\n)\n\n# add `passed` column\nstudent_data$passed &lt;- student_data$score &gt;= 90\n\n# subset students who passed\npassed_students &lt;- student_data[student_data$passed == TRUE, ]\n\n# explore data frame\nsummary(student_data)\n\n       id           name               score         passed       \n Min.   :1.00   Length:4           Min.   :85.00   Mode :logical  \n 1st Qu.:1.75   Class :character   1st Qu.:87.25   FALSE:2        \n Median :2.50   Mode  :character   Median :90.00   TRUE :2        \n Mean   :2.50                      Mean   :90.00                  \n 3rd Qu.:3.25                      3rd Qu.:92.75                  \n Max.   :4.00                      Max.   :95.00                  \n\nhead(student_data)\n\n  id    name score passed\n1  1   alice    88  FALSE\n2  2     bob    92   TRUE\n3  3 charlie    85  FALSE\n4  4   diana    95   TRUE\n\nstr(student_data)\n\n'data.frame':   4 obs. of  4 variables:\n $ id    : int  1 2 3 4\n $ name  : chr  \"alice\" \"bob\" \"charlie\" \"diana\"\n $ score : num  88 92 85 95\n $ passed: logi  FALSE TRUE FALSE TRUE\n\n\n\n\nSolution 5: Logical Operations and Subsetting\n\n# subset data based on score\nmean_score &lt;- mean(student_data$score)\nstudents_above_mean &lt;- student_data[student_data$Score &gt; mean_score, ]\n\n# add attendance and subset\nattendance &lt;- c(\"present\", \"absent\", \"present\", \"present\")\nstudent_data$Attendance &lt;- attendance\npresent_students &lt;- student_data[student_data$Attendance == \"present\", ]\n\n\n\nSolution 6: Cross-Tabulation and Analysis\n\n# create factor variables\nfruit &lt;- factor(c(\"apple\", \"banana\", \"apple\", \"orange\", \"banana\"))\ncolour &lt;- factor(c(\"red\", \"yellow\", \"green\", \"orange\", \"green\"))\n\n# create data frame\nfruit_data &lt;- data.frame(fruit, colour)\n\n# cross-tabulation\nfruit_color_table &lt;- table(fruit_data$fruit, fruit_data$colour)\nprint(fruit_color_table)\n\n        \n         green orange red yellow\n  apple      1      0   1      0\n  banana     1      0   0      1\n  orange     0      1   0      0\n\n# interpretation: Apple has the most colour variety with 2 colours (Red, Green).\n\n\n\nSolution 7: Visualization with ggplot2\n\n# install and load ggplot2\nif (!require(ggplot2)) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# create bar plot\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n    labs(title = \"Student Scores\", x = \"Name\", y = \"Score\") +\n    theme_minimal()"
  },
  {
    "objectID": "content/01-content.html#appendix-b-other-data-types-you-may-encounter",
    "href": "content/01-content.html#appendix-b-other-data-types-you-may-encounter",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Appendix B: Other Data Types You May Encounter",
    "text": "Appendix B: Other Data Types You May Encounter\n\nArrays and Matrices\nArrays are multi-dimensional data structures, while matrices are two-dimensional.\n\nmatrix_1 &lt;- matrix(1:9, nrow = 3) # creates a 3x3 matrix\narray_1 &lt;- array(1:12, dim = c(2, 3, 2)) # creates a 2x3x2 array\n\n\n\nConvert Matrix to Data Frame\nA data.frame is used for storing tabular data.\n\n# change matrix to array:\ndf_matrix_1 &lt;- data.frame(matrix_1)\n\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ X1: int  1 2 3\n $ X2: int  4 5 6\n $ X3: int  7 8 9\n\nhead(df_matrix_1)\n\n  X1 X2 X3\n1  1  4  7\n2  2  5  8\n3  3  6  9\n\n# change colnames\nnew_colnames &lt;- c(\"col_1\", \"col_2\", \"col_3\")\n\ncolnames(df_matrix_1) &lt;- new_colnames\n\n# check\nstr(df_matrix_1)\n\n'data.frame':   3 obs. of  3 variables:\n $ col_1: int  1 2 3\n $ col_2: int  4 5 6\n $ col_3: int  7 8 9\n\nhead(df_matrix_1)\n\n  col_1 col_2 col_3\n1     1     4     7\n2     2     5     8\n3     3     6     9\n\n\n\n\nWorking with Lists in R\n\nCreating lists\nTo create a list, you use the list() function.\n\n# Creating a simple list\nmy_list &lt;- list(name = \"John Doe\", age = 30, scores = c(90, 80, 70))\n\n# A list containing various types of elements, including another list\ncomplex_list &lt;- list(id = 1, name = \"Jane Doe\", preferences = list(color = \"blue\", hobby = \"reading\"))\n\n\n\nAccessing list elements\nList elements can be accessed using the [[ ]] notation for single elements, or the $ notation if you’re accessing named elements:\n\n# Accessing elements\nname &lt;- my_list$name # or my_list[[\"name\"]]\n\npreference_color &lt;- complex_list$preferences$color\n\n\n\nModifying lists\nLists can be modified by adding new elements, changing existing elements, or removing elements:\n\n# Adding a new element\nmy_list$gender &lt;- \"Male\"\n\n# Changing an existing element\nmy_list$age &lt;- 31\n\n# Removing an element\nmy_list$scores &lt;- NULL\n\n\n\nLists in Functions\nLists are often used as return values for functions that need to provide multiple pieces of data:\n\n# Function returning a list\ncalculate_stats &lt;- function(numbers) {\n    mean_val &lt;- mean(numbers)\n    sum_val &lt;- sum(numbers)\n    return(list(mean = mean_val, sum = sum_val))\n}\n\n# Using the function\nresults &lt;- calculate_stats(c(1, 2, 3, 4, 5))"
  },
  {
    "objectID": "slides/01-slides-lab.html#the-problem-is-confounding-what-can-we-do-about-it",
    "href": "slides/01-slides-lab.html#the-problem-is-confounding-what-can-we-do-about-it",
    "title": "Week 1: Intalling and Using R",
    "section": "The problem is confounding: what can we do about it?",
    "text": "The problem is confounding: what can we do about it?\n\nWeeks 2-4: Causal Diagrams"
  },
  {
    "objectID": "slides/01-slides-lab.html#what-have-we-learned",
    "href": "slides/01-slides-lab.html#what-have-we-learned",
    "title": "Week 1: Intalling and Using R",
    "section": "What have we learned?",
    "text": "What have we learned?\nKey Concepts ::: {.fragment .highlight-red} 1. Measurement = quantification 2. Validity = accuracy 3. Confounder = interference :::"
  },
  {
    "objectID": "slides/01-slides-lab.html#references",
    "href": "slides/01-slides-lab.html#references",
    "title": "Week 1: Intalling and Using R",
    "section": "References",
    "text": "References\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media. [Available online](https://r4ds.had.co.nz\nA helpful resource for learning R is Megan Hall’s lecture available at: https://meghan.rbind.io/talk/neair/.\nRStudio has compiled numerous accessible materials for learning R, which can be found here: https://education.rstudio.com/learn/beginner/."
  },
  {
    "objectID": "slides/01-slides-lab.html#introduction",
    "href": "slides/01-slides-lab.html#introduction",
    "title": "Week 1: Intalling and Using R",
    "section": "Introduction",
    "text": "Introduction\n\nWhy learn R?\nYou’ll need it for your final report.\nSupports your psychology coursework.\nEnhances your coding skills."
  },
  {
    "objectID": "slides/01-slides-lab.html#installing-r",
    "href": "slides/01-slides-lab.html#installing-r",
    "title": "Week 1: Intalling and Using R",
    "section": "Installing R",
    "text": "Installing R\n\nVisit the comprehensive r archive network (cran) at https://cran.r-project.org/\nSelect the version of r suitable for your operating system (windows, mac, or linux)\nDownload and install it by following the on-screen instructions"
  },
  {
    "objectID": "slides/01-slides-lab.html#installing-rstudio",
    "href": "slides/01-slides-lab.html#installing-rstudio",
    "title": "Week 1: Intalling and Using R",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nVisit rstudio download page at https://www.rstudio.com/products/rstudio/download/\nChoose the free version of rstudio desktop,\nDownload it for your operating system\nInstall and open"
  },
  {
    "objectID": "slides/01-slides-lab.html#create-new-project",
    "href": "slides/01-slides-lab.html#create-new-project",
    "title": "Week 1: Intalling and Using R",
    "section": "Create new project",
    "text": "Create new project\n\nfile &gt; new project\nChoose new directory\nSpecify the location where the project folder will be created\nClick create project"
  },
  {
    "objectID": "slides/01-slides-lab.html#exercise-1-install-tidyverse",
    "href": "slides/01-slides-lab.html#exercise-1-install-tidyverse",
    "title": "Week 1: Intalling and Using R",
    "section": "Exercise 1: Install tidyverse",
    "text": "Exercise 1: Install tidyverse\n\nOpen rstudio: launch rstudio on your computer\ntools &gt; install packages\nType tidyverse\nClick on the install button\nType library(tidyverse) in the console and press enter"
  },
  {
    "objectID": "slides/01-slides-lab.html#basic-r-commands",
    "href": "slides/01-slides-lab.html#basic-r-commands",
    "title": "Week 1: Intalling and Using R",
    "section": "Basic R commands",
    "text": "Basic R commands\n\nexecute code\n\nctrl + enter (windows/linux) or cmd + enter (mac).\n\nrstudio assignment operator shortcut:\n\nfor macos: option + - (minus key) inserts &lt;-.\nfor windows and linux: alt + - (minus key) inserts &lt;-.\n\n\n\nx &lt;- 10 # assigns the value 10 to x\ny &lt;- 5 # assigns the value 5 to y\n\n# this does the same\nx = 10\ny = 5\n\n# note what happens when we do this\n# 10 = 5 # not run\n\n# but we can do this\n10 == 5 # considered below\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides-lab.html#else",
    "href": "slides/01-slides-lab.html#else",
    "title": "Week 1: Intalling and Using R",
    "section": "else",
    "text": "else\n\n## Simulate data \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 1000\n  A &lt;- rnorm(n, 1) # simulates age,\n  H &lt;- rnorm(n , A) #  simulates happy as function of age\n  M &lt;- rnorm(n , A) + .2 * H # simulate marriage as a function of age + happiness\n\n  \n# Simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  A = A, \n  H = H,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + A, data = simdat_B)\nsim_B  # returns output\n}\n\n# Replication 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results\nparameters::pool_parameters(r_lm_B)\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |        95% CI | Statistic |     df |      p\n------------------------------------------------------------------------------\n(Intercept) |    4.12e-03 | 0.06 | [-0.12, 0.13] |      0.07 | 220.99 | 0.947 \nM           |        0.19 | 0.04 | [ 0.10, 0.27] |      4.42 | 234.19 | &lt; .001\nA           |        0.77 | 0.07 | [ 0.64, 0.91] |     11.17 | 211.13 | &lt; .001"
  },
  {
    "objectID": "slides/01-slides-lab.html#executing-code",
    "href": "slides/01-slides-lab.html#executing-code",
    "title": "Week 1: Intalling and Using R",
    "section": "Executing Code",
    "text": "Executing Code\n\nTo execute code in RStudio, use Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac)."
  },
  {
    "objectID": "slides/01-slides-lab.html#assignment-operator",
    "href": "slides/01-slides-lab.html#assignment-operator",
    "title": "Week 1: Intalling and Using R",
    "section": "Assignment Operator",
    "text": "Assignment Operator\nThe assignment operator in R is &lt;-. This operator assigns values to variables.\n\nx &lt;- 10 # assigns the value 10 to x\ny &lt;- 5  # assigns the value 5 to y\n\nAlternative assignment:\n\nx = 10\ny = 5\n\nComparing values:\n\n10 == 5 # returns FALSE\n\n[1] FALSE"
  },
  {
    "objectID": "slides/01-slides-lab.html#rstudio-assignment-operator-shortcut",
    "href": "slides/01-slides-lab.html#rstudio-assignment-operator-shortcut",
    "title": "Week 1: Intalling and Using R",
    "section": "RStudio Assignment Operator Shortcut",
    "text": "RStudio Assignment Operator Shortcut\n\nFor macOS: Option + - inserts &lt;-.\nFor Windows and Linux: Alt + - inserts &lt;-."
  },
  {
    "objectID": "slides/01-slides-lab.html#keyboard-shortcuts",
    "href": "slides/01-slides-lab.html#keyboard-shortcuts",
    "title": "Week 1: Intalling and Using R",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\nExplore keyboard shortcuts in RStudio through Tools -&gt; Keyboard Shortcuts Help."
  },
  {
    "objectID": "slides/01-slides-lab.html#concatenation",
    "href": "slides/01-slides-lab.html#concatenation",
    "title": "Week 1: Intalling and Using R",
    "section": "Concatenation",
    "text": "Concatenation\nThe c() function combines multiple elements into a vector.\n\nnumbers &lt;- c(1, 2, 3, 4, 5) # a vector of numbers\nprint(numbers)\n\n[1] 1 2 3 4 5"
  },
  {
    "objectID": "slides/01-slides-lab.html#arithmetic-operations",
    "href": "slides/01-slides-lab.html#arithmetic-operations",
    "title": "Week 1: Intalling and Using R",
    "section": "Arithmetic Operations",
    "text": "Arithmetic Operations\nAddition and subtraction in R:\n\nsum &lt;- x + y\nprint(sum)\n\n[1] 15\n\ndifference &lt;- x - y\nprint(difference)\n\n[1] 5"
  },
  {
    "objectID": "slides/01-slides-lab.html#multiplication-and-division",
    "href": "slides/01-slides-lab.html#multiplication-and-division",
    "title": "Week 1: Intalling and Using R",
    "section": "Multiplication and Division",
    "text": "Multiplication and Division\n\n# Scalar operations\nproduct &lt;- x * y\nquotient &lt;- x / y\n\n\n# vector multiplication and division\nvector1 &lt;- c(1, 2, 3)\nvector2 &lt;- c(4, 5, 6)\n\n# Vector operations\nvector_product &lt;- vector1 * vector2\nvector_division &lt;- vector1 / vector2\n\nBe cautious with division by zero:\n\nresult &lt;- 10 / 0 # Inf\nzero_division &lt;- 0 / 0 # NaN\n\nInteger division and modulo operation:\n\ninteger_division &lt;- 10 %/% 3\nremainder &lt;- 10 %% 3"
  },
  {
    "objectID": "slides/01-slides-lab.html#logical-operators",
    "href": "slides/01-slides-lab.html#logical-operators",
    "title": "Week 1: Intalling and Using R",
    "section": "Logical Operators",
    "text": "Logical Operators\nExamples of NOT, NOT EQUAL, and EQUAL operations:\n\nx_not_y &lt;- x != y\nx_equal_10 &lt;- x == 10\n\nOR and AND operations:\n\nvector_or &lt;- c(TRUE, FALSE) | c(FALSE, TRUE)\nsingle_or &lt;- TRUE || FALSE\n\nvector_and &lt;- c(TRUE, FALSE) & c(FALSE, TRUE)\nsingle_and &lt;- TRUE && FALSE"
  },
  {
    "objectID": "slides/01-slides-lab.html#integers",
    "href": "slides/01-slides-lab.html#integers",
    "title": "Week 1: Intalling and Using R",
    "section": "Integers",
    "text": "Integers\n\nWhole numbers without decimal points, defined with an L suffix\n\n\nx &lt;- 42L\nstr(x) # check type\n\n int 42\n\n\n\nConversion to numeric\n\n\ny &lt;- as.numeric(x)\nstr(y)\n\n num 42"
  },
  {
    "objectID": "slides/01-slides-lab.html#characters",
    "href": "slides/01-slides-lab.html#characters",
    "title": "Week 1: Intalling and Using R",
    "section": "Characters",
    "text": "Characters\n\nText strings enclosed in quotes\n\n\nname &lt;- \"alice\""
  },
  {
    "objectID": "slides/01-slides-lab.html#factors",
    "href": "slides/01-slides-lab.html#factors",
    "title": "Week 1: Intalling and Using R",
    "section": "Factors",
    "text": "Factors\n\nRepresent categorical data with limited values\n\n\ncolors &lt;- factor(c(\"red\", \"blue\", \"green\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ordered-factors",
    "href": "slides/01-slides-lab.html#ordered-factors",
    "title": "Week 1: Intalling and Using R",
    "section": "Ordered Factors",
    "text": "Ordered Factors\n\nFactors with and without inherent order\n\n\neducation_levels &lt;- c(\"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_factor_no_order &lt;- factor(education_levels, ordered = FALSE)\neducation_factor &lt;- factor(education_levels, ordered = TRUE)\neducation_ordered_explicit &lt;- factor(education_levels, levels = education_levels, ordered = TRUE)\n\n\nOperations with ordered factors\n\n\nedu1 &lt;- ordered(\"bachelor\", levels = education_levels)\nedu2 &lt;- ordered(\"master\", levels = education_levels)\nedu2 &gt; edu1 # logical comparison\n\n[1] TRUE\n\n\n\nModifying ordered factors\n\n\nnew_levels &lt;- c(\"primary school\", \"high school\", \"bachelor\", \"master\", \"ph.d.\")\neducation_updated &lt;- factor(education_levels, levels = new_levels, ordered = TRUE)\nstr(education_updated)\n\n Ord.factor w/ 5 levels \"primary school\"&lt;..: 2 3 4 5\n\ntable(education_updated)\n\neducation_updated\nprimary school    high school       bachelor         master          ph.d. \n             0              1              1              1              1"
  },
  {
    "objectID": "slides/01-slides-lab.html#strings",
    "href": "slides/01-slides-lab.html#strings",
    "title": "Week 1: Intalling and Using R",
    "section": "Strings",
    "text": "Strings\n\nSequences of characters\n\n\nyou &lt;- 'world!'\ngreeting &lt;- paste(\"hello,\", you)\n# hello world\ngreeting\n\n[1] \"hello, world!\""
  },
  {
    "objectID": "slides/01-slides-lab.html#vectors",
    "href": "slides/01-slides-lab.html#vectors",
    "title": "Week 1: Intalling and Using R",
    "section": "Vectors",
    "text": "Vectors\n\nFundamental data structure in R\n\n\nnumeric_vector &lt;- c(1, 2, 3, 4, 5)\ncharacter_vector &lt;- c(\"apple\", \"banana\", \"cherry\")\nlogical_vector &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\nManipulating vectors\n\n\nvector_sum &lt;- numeric_vector + 10\nvector_multiplication &lt;- numeric_vector * 2\nvector_greater_than_three &lt;- numeric_vector &gt; 3"
  },
  {
    "objectID": "slides/01-slides-lab.html#table-function",
    "href": "slides/01-slides-lab.html#table-function",
    "title": "Week 1: Intalling and Using R",
    "section": "table() Function",
    "text": "table() Function\n\nGenerates frequency tables for categorical data\n\n\ntable(vector_greater_than_three)\n\nvector_greater_than_three\nFALSE  TRUE \n    3     2"
  },
  {
    "objectID": "slides/01-slides-lab.html#dataframes",
    "href": "slides/01-slides-lab.html#dataframes",
    "title": "Week 1: Intalling and Using R",
    "section": "Dataframes",
    "text": "Dataframes\n\nCreating and manipulating data frames\n\n\n# clear previous `df` object (if any)\nrm(df)\n\nWarning in rm(df): object 'df' not found\n\ndf &lt;- data.frame(\n  name = c(\"alice\", \"bob\", \"charlie\"),\n  age = c(25, 30, 35),\n  gender = c(\"female\", \"male\", \"male\")\n)\n# look at structure\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\"\n\ntable(df$gender)\n\n\nfemale   male \n     1      2 \n\ntable(df$age)\n\n\n25 30 35 \n 1  1  1 \n\ntable(df$name)\n\n\n  alice     bob charlie \n      1       1       1"
  },
  {
    "objectID": "slides/01-slides-lab.html#accessing-data-frame-elements",
    "href": "slides/01-slides-lab.html#accessing-data-frame-elements",
    "title": "Week 1: Intalling and Using R",
    "section": "Accessing Data Frame Elements",
    "text": "Accessing Data Frame Elements\n\nBy column name and row/column indexing\n\n\n# by column name\nnames &lt;- df$name\n# by row and column\nsecond_person &lt;- df[2, ]\nage_column &lt;- df[, \"age\"]"
  },
  {
    "objectID": "slides/01-slides-lab.html#using-subset-function",
    "href": "slides/01-slides-lab.html#using-subset-function",
    "title": "Week 1: Intalling and Using R",
    "section": "Using subset() Function",
    "text": "Using subset() Function\n\nExtracting rows based on conditions\n\n\nvery_old_people &lt;- subset(df, age &gt; 25)\nsummary(very_old_people$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.00   31.25   32.50   32.50   33.75   35.00 \n\nmean(very_old_people$age)\n\n[1] 32.5\n\nmin(very_old_people$age)\n\n[1] 30"
  },
  {
    "objectID": "slides/01-slides-lab.html#exploring-data-frames",
    "href": "slides/01-slides-lab.html#exploring-data-frames",
    "title": "Week 1: Intalling and Using R",
    "section": "Exploring Data Frames",
    "text": "Exploring Data Frames\n\nUsing head(), tail(), and str()\n\n\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\ntail(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\""
  },
  {
    "objectID": "slides/01-slides-lab.html#manipulating-data-frames",
    "href": "slides/01-slides-lab.html#manipulating-data-frames",
    "title": "Week 1: Intalling and Using R",
    "section": "Manipulating Data Frames",
    "text": "Manipulating Data Frames\n\nAdding and modifying columns and rows\n\n\n# Adding columns\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n# Adding rows\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# Modifying values\ndf[4, \"age\"] &lt;- 26\ndf\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE"
  },
  {
    "objectID": "slides/01-slides-lab.html#rbind-and-cbind",
    "href": "slides/01-slides-lab.html#rbind-and-cbind",
    "title": "Week 1: Intalling and Using R",
    "section": "rbind() and cbind()",
    "text": "rbind() and cbind()\n\nAdding rows and columns to data frames\n\n\n# add rows with `rbind()`\nnew_person &lt;- data.frame(name = \"eve\", age = 32, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# add columns with `cbind()`\noccupation_vector &lt;- c(\"engineer\", \"doctor\", \"artist\", \"teacher\", \"doctor\")\ndf &lt;- cbind(df, occupation_vector)\ndf\n\n     name age gender employed occupation_vector\n1   alice  25 female     TRUE          engineer\n2     bob  30   male     TRUE            doctor\n3 charlie  35   male    FALSE            artist\n4   diana  26 female     TRUE           teacher\n5     eve  32 female     TRUE            doctor"
  },
  {
    "objectID": "slides/01-slides-lab.html#data-structure-view",
    "href": "slides/01-slides-lab.html#data-structure-view",
    "title": "Week 1: Intalling and Using R",
    "section": "Data Structure View",
    "text": "Data Structure View\n\nUsing summary(), str(), head(), and tail()\n\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides/01-slides-lab.html#statistical-functions",
    "href": "slides/01-slides-lab.html#statistical-functions",
    "title": "Week 1: Intalling and Using R",
    "section": "Statistical Functions",
    "text": "Statistical Functions\n\nmean(), sd(), min(), max(), and table()\n\n\n#  seed for reproducibility\nset.seed(12345)\nvector &lt;- rnorm(n = 40, mean = 0, sd = 1)\nmean(vector)  # calculates mean\n\n[1] 0.2401853\n\nsd(vector)  # computes standard deviation\n\n[1] 1.038425\n\nmin(vector)  # finds minimum value\n\n[1] -1.817956\n\nmax(vector)  # finds maximum value\n\n[1] 2.196834"
  },
  {
    "objectID": "slides/01-slides-lab.html#introduction-to-ggplot2",
    "href": "slides/01-slides-lab.html#introduction-to-ggplot2",
    "title": "Week 1: Intalling and Using R",
    "section": "Introduction to ggplot2",
    "text": "Introduction to ggplot2\n\nVisualizing data with ggplot2\n\n\n#  seed for reproducibility\nset.seed(12345)\n# ensure ggplot2 is installed and loaded\nif (!require(ggplot2)) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n# simulate student data\nstudent_data &lt;- data.frame(\n  name = c(\"alice\", \"bob\", \"charlie\", \"diana\", \"ethan\", \"fiona\", \"george\", \"hannah\"),\n  score = sample(80:100, 8, replace = TRUE),\n  stringsasfactors = FALSE\n)\nstudent_data$passed &lt;- ifelse(student_data$score &gt;= 90, \"passed\", \"failed\")\nstudent_data$passed &lt;- factor(student_data$passed, levels = c(\"failed\", \"passed\"))\nstudent_data$study_hours &lt;- sample(5:15, 8, replace = TRUE)"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-barplot",
    "href": "slides/01-slides-lab.html#ggplot2-barplot",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Barplot",
    "text": "ggplot2 Barplot\n\nbar plot showing score for each name\n\n\nggplot(student_data, aes(x = name, y = score)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nenhanced bar plot with titles, axis labels, and modified colours\n\n\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n  labs(title = \"student scores\", x = \"student name\", y = \"score\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-scatterplot",
    "href": "slides/01-slides-lab.html#ggplot2-scatterplot",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Scatterplot",
    "text": "ggplot2 Scatterplot\n\ncompare student scores against study hours\n\n\nggplot(student_data, aes(x = study_hours, y = score, color = passed)) +\n  geom_point(size = 4) +\n  labs(title = \"student scores vs. study hours\", x = \"study hours\", y = \"score\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-boxplot",
    "href": "slides/01-slides-lab.html#ggplot2-boxplot",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Boxplot",
    "text": "ggplot2 Boxplot\n\nvisualise the distribution of scores by pass/fail status\n\n\nggplot(student_data, aes(x = passed, y = score, fill = passed)) +\n  geom_boxplot() +\n  labs(title = \"score distribution by pass/fail status\", x = \"status\", y = \"score\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))\n\n# median (Q2/50th percentile): divides the dataset into two halves.\n# first quartile (Q1/25th percentile): lower edge indicating that 25% of the data falls below this value.\n# third quartile (Q3/75th percentile): upper edge of the box represents the third quartile, showing that 75% of the data is below this value.\n# interquartile range (IQR): height of the box represents the IQR: distance between the first and third quartiles (Q3 - Q1) / middle 50% of the data.\n# whiskers: The lines extending from the top and bottom of the box (the \"whiskers\") indicate the range of the data, typically to the smallest and largest values within 1.5 * IQR from the first and third quartiles, respectively. Points outside this range are often considered outliers and can be plotted individually.\n# outliers: points that lie beyond the whiskers"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-histogram",
    "href": "slides/01-slides-lab.html#ggplot2-histogram",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Histogram",
    "text": "ggplot2 Histogram\n\nunderstand the distribution of scores\n\n\nggplot(student_data, aes(x = score, fill = passed)) +\n  geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n  labs(title = \"histogram of scores\", x = \"score\", y = \"count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-lineplot",
    "href": "slides/01-slides-lab.html#ggplot2-lineplot",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Lineplot",
    "text": "ggplot2 Lineplot\n\n# prep data\nmonths &lt;- factor(month.abb[1:8], levels = month.abb[1:8])\nstudy_hours &lt;- c(0, 3, 15, 30, 35, 120, 18, 15)\nstudy_data &lt;- data.frame(month = months, study_hours = study_hours)\n\n# line plot\nggplot(study_data, aes(x = month, y = study_hours, group = 1)) +\n  geom_line(linewidth = 1, color = \"blue\") +\n  geom_point(color = \"red\", size = 1) +\n  labs(title = \"monthly study hours\", x = \"month\", y = \"study hours\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-slides-lab.html#modifying-data-frames",
    "href": "slides/01-slides-lab.html#modifying-data-frames",
    "title": "Week 1: Intalling and Using R",
    "section": "Modifying Data Frames",
    "text": "Modifying Data Frames\n\nAdding and modifying columns and rows\n\n\n# add columns\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n# add rows\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# modify values\ndf[4, \"age\"] &lt;- 26\ndf\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-scatter-plot",
    "href": "slides/01-slides-lab.html#base-r-scatter-plot",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Scatter Plot",
    "text": "Base R Scatter Plot\n\nscatter plot of scores vs. study hours\n\n\n# scatter plot\nplot(student_data$study_hours, student_data$score,\n     main = \"scatter plot of scores vs. study hours\",\n     xlab = \"study hours\", ylab = \"score\",\n     pch = 19, col = ifelse(student_data$passed == \"passed\", \"blue\", \"red\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-histogram",
    "href": "slides/01-slides-lab.html#base-r-histogram",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Histogram",
    "text": "Base R Histogram\n\nhistogram to visualise distribution of student scores\n\n\n# histogram \nhist(student_data$score,\n     breaks = 5,\n     col = \"skyblue\",\n     main = \"histogram of student scores\",\n     xlab = \"scores\",\n     border = \"white\")"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-boxplots",
    "href": "slides/01-slides-lab.html#base-r-boxplots",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Boxplots",
    "text": "Base R Boxplots\n\nbox plots for score distribution by pass/fail status\n\n\n# boxplot\nboxplot(score ~ passed, data = student_data,\n        main = \"score distribution by pass/fail status\",\n        xlab = \"status\", ylab = \"scores\",\n        col = c(\"red\", \"blue\"))\n\n# median (Q2/50th percentile): divides the dataset into two halves.\n# first quartile (Q1/25th percentile): lower edge indicating that 25% of the data falls below this value.\n# third quartile (Q3/75th percentile): upper edge of the box represents the third quartile, showing that 75% of the data is below this value.\n# interquartile range (IQR): height of the box represents the IQR: distance between the first and third quartiles (Q3 - Q1) / middle 50% of the data.\n# whiskers: The lines extending from the top and bottom of the box (the \"whiskers\") indicate the range of the data, typically to the smallest and largest values within 1.5 * IQR from the first and third quartiles, respectively. Points outside this range are often considered outliers and can be plotted individually.\n# outliers: points that lie beyond the whiskers"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-barplot",
    "href": "slides/01-slides-lab.html#base-r-barplot",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Barplot",
    "text": "Base R Barplot\n\nbarplot to visualise score distribution\n\n\n# prep data for the barplot\nscores_table &lt;- table(student_data$score)\nbarplot(scores_table,\n        main = \"Barplot of Scores\",\n        xlab = \"Scores\",\n        ylab = \"Frequency\",\n        col = \"skyblue\",\n        border = \"white\")"
  },
  {
    "objectID": "slides/01-slides-lab.html",
    "href": "slides/01-slides-lab.html",
    "title": "Week 1: Intalling and Using R",
    "section": "",
    "text": "Have R and R-studio Downloaded on your machine\nBe able to use R for basic analysis and graphing"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-line-plot",
    "href": "slides/01-slides-lab.html#base-r-line-plot",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Line Plot",
    "text": "Base R Line Plot\n\n# convert 'month' to a numeric scale for plotting positions\nmonths_num &lt;- 1:length(study_data$month) # Simple numeric sequence\n\n# Plotting points with suppressed x-axis\nplot(months_num, study_data$study_hours, \n     type = \"p\", # Points\n     pch = 19,   # Type of point\n     col = \"red\", \n     xlab = \"Month\", \n     ylab = \"Study Hours\", \n     main = \"Monthly Study Hours\",\n     xaxt = \"n\") # Suppress the x-axis\n\n# add lines between points\nlines(months_num, study_data$study_hours, \n      col = \"blue\", \n      lwd = 1) # Line width\n\n# add custom month labels to the x-axis at appropriate positions\naxis(1, at = months_num, labels = study_data$month, las=2) # `las=2` makes labels perpendicular to axis"
  },
  {
    "objectID": "slides/01-slides-lab.html#what-have-your-learned",
    "href": "slides/01-slides-lab.html#what-have-your-learned",
    "title": "Week 1: Intalling and Using R",
    "section": "What have your learned?",
    "text": "What have your learned?\n\nYou have Base R and R-studio Downloaded on your machine\nYou are able able to use R for basic analysis and graphing\nYou will need to practice, and will have lots of opporunity."
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-barplot-score-distributions",
    "href": "slides/01-slides-lab.html#base-r-barplot-score-distributions",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Barplot: Score Distributions",
    "text": "Base R Barplot: Score Distributions\n\n# prep data for the barplot\nscores_table &lt;- table(student_data$score)\nbarplot(scores_table,\n        main = \"Barplot of Scores\",\n        xlab = \"Scores\",\n        ylab = \"Frequency\",\n        col = \"skyblue\",\n        border = \"white\")"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-barplot-score-for-each-name",
    "href": "slides/01-slides-lab.html#ggplot2-barplot-score-for-each-name",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Barplot: score for each name",
    "text": "ggplot2 Barplot: score for each name\n\nggplot(student_data, aes(x = name, y = score)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\nenhanced bar plot with titles, axis labels, and modified colours\n\n\nggplot(student_data, aes(x = name, y = score, fill = passed)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\")) +\n  labs(title = \"student scores\", x = \"student name\", y = \"score\") +\n  theme_minimal()"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-scatterplot-student-scores-against-study-hours",
    "href": "slides/01-slides-lab.html#ggplot2-scatterplot-student-scores-against-study-hours",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Scatterplot: student scores against study hours",
    "text": "ggplot2 Scatterplot: student scores against study hours\n\nggplot(student_data, aes(x = study_hours, y = score, color = passed)) +\n  geom_point(size = 4) +\n  labs(title = \"student scores vs. study hours\", x = \"study hours\", y = \"score\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-boxplot-scores-by-passfail-status",
    "href": "slides/01-slides-lab.html#ggplot2-boxplot-scores-by-passfail-status",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Boxplot: scores by pass/fail status",
    "text": "ggplot2 Boxplot: scores by pass/fail status\n\nggplot(student_data, aes(x = passed, y = score, fill = passed)) +\n  geom_boxplot() +\n  labs(title = \"score distribution by pass/fail status\", x = \"status\", y = \"score\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#ggplot2-histogram-distribution-of-scores",
    "href": "slides/01-slides-lab.html#ggplot2-histogram-distribution-of-scores",
    "title": "Week 1: Intalling and Using R",
    "section": "ggplot2 Histogram: distribution of scores",
    "text": "ggplot2 Histogram: distribution of scores\n\nggplot(student_data, aes(x = score, fill = passed)) +\n  geom_histogram(binwidth = 5, color = \"black\", alpha = 0.7) +\n  labs(title = \"histogram of scores\", x = \"score\", y = \"count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"failed\" = \"red\", \"passed\" = \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-scatter-plot-scores-vs.-study-hours",
    "href": "slides/01-slides-lab.html#base-r-scatter-plot-scores-vs.-study-hours",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Scatter Plot: scores vs. study hours",
    "text": "Base R Scatter Plot: scores vs. study hours\n\n# scatter plot\nplot(student_data$study_hours, student_data$score,\n     main = \"scatter plot of scores vs. study hours\",\n     xlab = \"study hours\", ylab = \"score\",\n     pch = 19, col = ifelse(student_data$passed == \"passed\", \"blue\", \"red\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#base-r-boxplots-distribution-by-passfail",
    "href": "slides/01-slides-lab.html#base-r-boxplots-distribution-by-passfail",
    "title": "Week 1: Intalling and Using R",
    "section": "Base R Boxplots: Distribution by Pass/Fail",
    "text": "Base R Boxplots: Distribution by Pass/Fail\n\n# boxplot\nboxplot(score ~ passed, data = student_data,\n        main = \"score distribution by pass/fail status\",\n        xlab = \"status\", ylab = \"scores\",\n        col = c(\"red\", \"blue\"))"
  },
  {
    "objectID": "slides/01-slides-lab.html#install-r",
    "href": "slides/01-slides-lab.html#install-r",
    "title": "Week 1: Intalling and Using R",
    "section": "Install R",
    "text": "Install R\n\nVisit the comprehensive r archive network (cran) at https://cran.r-project.org/\nSelect the version of r suitable for your operating system (windows, mac, or linux)\nDownload and install it by following the on-screen instructions"
  },
  {
    "objectID": "slides/01-slides-lab.html#install-rstudio",
    "href": "slides/01-slides-lab.html#install-rstudio",
    "title": "Week 1: Intalling and Using R",
    "section": "Install RStudio",
    "text": "Install RStudio\n\nVisit rstudio download page at https://www.rstudio.com/products/rstudio/download/\nChoose the free version of rstudio desktop,\nDownload it for your operating system\nInstall and open"
  },
  {
    "objectID": "slides/01-slides-lab.html#execut-code",
    "href": "slides/01-slides-lab.html#execut-code",
    "title": "Week 1: Intalling and Using R",
    "section": "Execut Code",
    "text": "Execut Code\n\nUse Ctrl + Enter (Windows/Linux) or Cmd + Enter (Mac)."
  },
  {
    "objectID": "slides/01-slides-lab.html#access-data-frame-elements",
    "href": "slides/01-slides-lab.html#access-data-frame-elements",
    "title": "Week 1: Intalling and Using R",
    "section": "Access Data Frame Elements",
    "text": "Access Data Frame Elements\n\nBy column name and row/column indexing\n\n\n# by column name\nnames &lt;- df$name\n# by row and column\nsecond_person &lt;- df[2, ]\nage_column &lt;- df[, \"age\"]"
  },
  {
    "objectID": "slides/01-slides-lab.html#explore-data-frames",
    "href": "slides/01-slides-lab.html#explore-data-frames",
    "title": "Week 1: Intalling and Using R",
    "section": "Explore Data Frames",
    "text": "Explore Data Frames\n\nUsing head(), tail(), and str()\n\n\nhead(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\ntail(df)\n\n     name age gender\n1   alice  25 female\n2     bob  30   male\n3 charlie  35   male\n\nstr(df)\n\n'data.frame':   3 obs. of  3 variables:\n $ name  : chr  \"alice\" \"bob\" \"charlie\"\n $ age   : num  25 30 35\n $ gender: chr  \"female\" \"male\" \"male\""
  },
  {
    "objectID": "slides/01-slides-lab.html#modify-data-frames",
    "href": "slides/01-slides-lab.html#modify-data-frames",
    "title": "Week 1: Intalling and Using R",
    "section": "Modify Data Frames",
    "text": "Modify Data Frames\n\nAdd and modify columns and rows\n\n\n# add columns\ndf$employed &lt;- c(TRUE, TRUE, FALSE)\n# add rows\nnew_person &lt;- data.frame(name = \"diana\", age = 28, gender = \"female\", employed = TRUE)\ndf &lt;- rbind(df, new_person)\n# modify values\ndf[4, \"age\"] &lt;- 26\ndf\n\n     name age gender employed\n1   alice  25 female     TRUE\n2     bob  30   male     TRUE\n3 charlie  35   male    FALSE\n4   diana  26 female     TRUE"
  },
  {
    "objectID": "slides/01-slides.html#what-might-go-wrong-1",
    "href": "slides/01-slides.html#what-might-go-wrong-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What might go wrong?",
    "text": "What might go wrong?\nAge might a common cause of both marriage and happiness"
  },
  {
    "objectID": "slides/01-slides.html#what-else-might-go-wrong-1",
    "href": "slides/01-slides.html#what-else-might-go-wrong-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "What else might go wrong?",
    "text": "What else might go wrong?\nAge might a common cause of both marriage and happiness"
  },
  {
    "objectID": "content/01-content.html#slide-deck",
    "href": "content/01-content.html#slide-deck",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slide deck:",
    "text": "Slide deck:"
  },
  {
    "objectID": "content/01-content.html#slides-1",
    "href": "content/01-content.html#slides-1",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Slides",
    "text": "Slides\nPREVIEW\n\n\n\nOpen in browser here"
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-literature-review",
    "href": "slides/01-slides.html#conduct-a-literature-review",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a Literature Review",
    "text": "Conduct a Literature Review\n\nWhat would this tell us?\n\nwhat other researchers have found.\n\nWhat would this not tell us?\n\nwhat other researchers have not found.\nwhat other researchers got wrong."
  },
  {
    "objectID": "slides/01-slides.html#conduct-a-survey-with-a-large-and-diverse-sample-of-individuals",
    "href": "slides/01-slides.html#conduct-a-survey-with-a-large-and-diverse-sample-of-individuals",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Conduct a Survey with a Large and Diverse Sample of Individuals",
    "text": "Conduct a Survey with a Large and Diverse Sample of Individuals\n\nWhere to begin?"
  },
  {
    "objectID": "slides/01-slides.html#our-approach",
    "href": "slides/01-slides.html#our-approach",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Our Approach:",
    "text": "Our Approach:\nExternal Validity\n\nDefinition: the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods.\nImportance: we want to know if our findings carry beyond the people in our study\nChallenges: how can we know?\n\nInternal Validity\n\nDefinition: the degree to which a study can demonstrate that a causal relationship exists between the ‘independent’ and ‘dependent’ variables, free of confounding.\nWe will use the term ‘outcome’ and ‘treatment’ in place of independent and dependent. This is because causality occurs in time.\nImportance: we cannot interpret the associations we recover if they do not have a causal interpretation.\nChallenges: internal validity requires balance in the confounders that might afffect the treatement and outcome, and observational data do not give us ballance.\nThe Special Challenges in Cross-Cultural Research:\n\nThe concept of measurement itself has causal underpinnings (VanderWeele 2022).\nIn cross-cultural research, we have stronger challengs from measurement validity, which, as we shall see, encompasses both external and internal validity.\nWe consider measurment validity within classical framework of external and internal validity\nEthical challenges, which science needs to consider.\nForemost among these ethical challenges, I believe, is doing our best to get inference right!"
  },
  {
    "objectID": "slides/01-slides.html#the-hope",
    "href": "slides/01-slides.html#the-hope",
    "title": "Asking questions in cross-cultural psychology",
    "section": "The Hope",
    "text": "The Hope\n\nThere has been tremendous progress in the health and computer sciences in addressing problems of external and internal validity from a causal perspective\nYou will be among the first to apply these methods to questions in cross-cultural psychology.\nIn doing so, much of what has been confusing to you about psychological research design, data-analysis, ultimately scientific inference will be clearer."
  },
  {
    "objectID": "slides/01-slides-lab.html#where-to-get-help",
    "href": "slides/01-slides-lab.html#where-to-get-help",
    "title": "Week 1: Intalling and Using R",
    "section": "Where to Get Help",
    "text": "Where to Get Help\n\nLarge Language Models (LLMs): LLMs are trained on extensive datasets. They are extremely good coding tutors. Open AI’s GPT-4 considerably outperforms GPT-3.5. However GPT 3.5 should be good enough. Gemini has a two-month free trial. LLM’s are rapidly evolving. However, presently, to use these tools, and to spot their errors, you will need to know how to code. Which is fortunate because coding makes you smarter!\n\nNote: you will not be assessed for R-code. Help from LLM’s for coding does not consitute a breach of academic integrity in this course. Your tests are in-class; no LLM’s allowed. For your final report, you will need to cite all sources, and how you used them, including LLMs.\n\nStack Overflow: an outstanding resource for most problems. Great community.\nCross-validated the best place to go for stats advice. (LLM’s are only safe for standard statistics. They do not perform well for causal inference.)\nDeveloper Websites and GitHub Pages: Tidyverse\nYour tutors and course coordinator. We care. We’re here to help you!"
  },
  {
    "objectID": "slides/01-slides.html#many-psychologists-will-simply-control-for-income-but-what-if-happiness-and-marriage-cause-income",
    "href": "slides/01-slides.html#many-psychologists-will-simply-control-for-income-but-what-if-happiness-and-marriage-cause-income",
    "title": "Asking questions in cross-cultural psychology",
    "section": "Many psychologists will simply “control for” income, but what if happiness and marriage cause income?",
    "text": "Many psychologists will simply “control for” income, but what if happiness and marriage cause income?\n\nSimulate data in which happiness and marriage cause income\nControlling for income makes it look as though happiness and marriage are negatively related.\nBut we simulated data with no relationship!\nThis is called ‘collider bias’, we will consider how it works in the upcoming weeks.\nMeantime, even if we measure people without error, cross-cultural studies might select people with different incomes, leading to bias even in the absence of measurement error bias (and even if we do not control for income!)\n\n\n\nCode\n## simulate data \n# reproducability \nset.seed(123)\nsim_fun_B = function() {\n  n &lt;- 10000\n  H &lt;- rnorm(n, 1) # simulates Happiness,\n  M &lt;- rnorm(n , 1) #  simulates Marriage, nothing to do with happiness\n  I &lt;- rnorm(n , .2 * H + .5 * M) # simulate marriage as a function of age + happiness\n\n  \n# simulate dataframe from function\nsimdat_B &lt;- data.frame(\n  H = H, \n  I = I,\n  M = M) \n\n#  model in which marriage \"predicts\" happiness controlling for age\nsim_B &lt;- lm(H ~ M + I, data = simdat_B)\nsim_B  # returns output\n}\n\n# replicate 100 times\nr_lm_B &lt;- NA\nr_lm_B = replicate(100, sim_fun_B(), simplify = FALSE )\n\n# print model results: marriage looks negatively related to happiness!\nparameters::pool_parameters(r_lm_B)\n\n\n# Fixed Effects\n\nParameter   | Coefficient |   SE |         95% CI | Statistic |     df |      p\n-------------------------------------------------------------------------------\n(Intercept) |        0.96 | 0.02 | [ 0.92,  1.00] |     46.96 | 322.51 | &lt; .001\nM           |       -0.10 | 0.02 | [-0.13, -0.07] |     -6.11 | 324.10 | &lt; .001\nI           |        0.19 | 0.01 | [ 0.17,  0.22] |     13.71 | 320.53 | &lt; .001"
  },
  {
    "objectID": "content/02-content.html#causal-diagrams-or-causal-directed-acyclic-graphs",
    "href": "content/02-content.html#causal-diagrams-or-causal-directed-acyclic-graphs",
    "title": "Causal diagrams: Five elementary structures",
    "section": "Causal Diagrams – or “Causal Directed Acyclic Graphs”",
    "text": "Causal Diagrams – or “Causal Directed Acyclic Graphs”"
  },
  {
    "objectID": "content/02-content.html#for-more-information-about-the-packages-used-here",
    "href": "content/02-content.html#for-more-information-about-the-packages-used-here",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "For more information about the packages used here:",
    "text": "For more information about the packages used here:\n\nggplot2: A system for declaratively creating graphics, based on The Grammar of Graphics.\nParameters package: Provides utilities for processing model parameters and their metrics.\nReport package: Facilitates the automated generation of reports from statistical models.\nMontgomery, Nyhan, and Torres (2018) describes confounding in experiments - well worth a read.\n\n\nMontgomery, Jacob M., Brendan Nyhan, and Michelle Torres. 2018. “How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.” American Journal of Political Science 62 (3): 760–75. https://doi.org/10.1111/ajps.12357."
  },
  {
    "objectID": "content/02-content.html#appendix-a-i-lied",
    "href": "content/02-content.html#appendix-a-i-lied",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix A: I lied",
    "text": "Appendix A: I lied\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#review",
    "href": "content/02-content.html#review",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Review",
    "text": "Review\n\nPsychological research begins with two questions:\n\n\n\nWhat do I want to know?\nFor which population does this knowledge generalise?\n\n\nThis course considers how to ask this psychological questions that pertain to populations with different characteristics\n\nIn psychological research, we typically ask questions about the causes and consequences of thought and behaviour - “What if?” questions (Hernan and Robins 2024).\nThe following concepts help us to describe two distinct failure modes in psychological research when asking “What if?” questions:\n\n\nThe Concept of External Validity: the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods. That is, we want to know if our findings carry beyond the sample population to the target population. We fail when our results do not generalise as we think. More fundamental, we fail when we have not clearly defined our question or our target population.\nThe Concept of Internal Validity: the extent to which the associations we obtain from data reflect causality. In psychological science, we use the terms “independent variable” and “dependent variable.” Sometimes we use the terms “exogenous variable” and “endogenous variable.” Sometimes we use the term “predictor variable” to describe the “dependent” or “endogenous” variable. These words are confusing. When asking “What if?” questions, we want to understand what would happen were we to intervene. In this course, we will use the term “treatment” or equivalently the term “exposure” to denote the intervention; we will use the term “outcome” to denote the effect of an intervention.1\n\n\nDefinition 1 We say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as it is defined at baseline.\n\n\nDefinition 2 We say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as it is defined at baseline.\n\n\nDuring the first part of the course, our primary focus will be on challenges to internal validity from confounding bias.\n\nThe concept of “confounding bias” helps to clarify what it is at stake when evaluating the internal validity of a study. As we shall see, there are several equivalent definitions of “confounding bias.” The definition that we will examine today is:\n\nDefinition 3 We say there is confounding-bias if there is an open back-door path between the treatment and outcome, or if that path between the treatment and outcome is blocked.\n\nOur purpose will be to clarify the meaning of the terminology in this definition, and along the way, to clarify how five elementary graphical structures and four rules allow us to determine whether and how to obtain internal validity from causal assumptions encoded in a causal diagram."
  },
  {
    "objectID": "content/02-content.html#download-your-laboratory-r-script-here-1",
    "href": "content/02-content.html#download-your-laboratory-r-script-here-1",
    "title": "Causal diagrams: Five elementary structures",
    "section": "Download Your Laboratory R Script Here",
    "text": "Download Your Laboratory R Script Here\nAssuming you have installed R and RStudio:\n\nDownload the R script for Lab 01 by clicking the link below. This script contains the code you will work with during your laboratory session.\nAfter downloading, open RStudio.\nIn RStudio, create a new R script file by going to File &gt; New File &gt; R Script.\nCopy the contents of the downloaded .R file and paste them into the new R script file you created in RStudio.\nSave the new R script file in your project directory for easy access during the lab.\n\nDownload the R script for Lab 01"
  },
  {
    "objectID": "content/02-content.html#footnotes",
    "href": "content/02-content.html#footnotes",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“What if?” questions implicitly invoke the idea of intervening on the world. “If we did this, then what would happen to that…?” Our preferred terminology reflects our interest in the effects of interventions.↩︎"
  },
  {
    "objectID": "content/02-content.html#introduction-to-causal-diagrams-or-causal-directed-acyclic-graphs",
    "href": "content/02-content.html#introduction-to-causal-diagrams-or-causal-directed-acyclic-graphs",
    "title": "Causal diagrams: Five elementary structures",
    "section": "Introduction to Causal Diagrams – or “Causal Directed Acyclic Graphs”",
    "text": "Introduction to Causal Diagrams – or “Causal Directed Acyclic Graphs”\n\n\n\n\n\n\nFigure 1: Naming Conventions\n\n\n\n\n\n\n\n\n\nFigure 2: Nodes, Edges, Conditioning Conventions\n\n\n\n\n\n\n\n\n\nFigure 3: Five elementary structures\n\n\n\n\n\n\n\n\n\nFigure 4: Four rules of confounding control"
  },
  {
    "objectID": "content/02-content.html#introduction-to-causal-diagrams.",
    "href": "content/02-content.html#introduction-to-causal-diagrams.",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Introduction to Causal Diagrams.",
    "text": "Introduction to Causal Diagrams.\nCausal diagrams, also called causal graphs, Directed Acyclic Graphs, and Causal Directed Acyclic Graphs, are graphical tools whose primary purpose is to enable investigators to detect confounding biases.\nRemarkably, causal diagrams are rarely used in psychology!\nBefore describing how causal diagrams work, we first define the meanings of their symbols. Note there is no single convention for creating causal diagrams, so it is important that we are clear when defining our meanings.\n\nThe meaning of our symbols\nThe conventions that describe the meanings of our symbols are given in Figure 1.\n\n\n\n\n\n\nFigure 1: Our variable naming conventions. This figure is adapted from (Bulbulia 2023)\n\nBulbulia, J. A. 2023. “Causal Diagrams (Directed Acyclic Graphs): A Practical Guide.”\n\n\n\nFor us:\nX denotes a variable without reference to its role;\nA denotes the “treatment” or “exposure” variable. This is the variable for which we seek to understand the effect of intervening on it. It is the “cause;”\nY denotes the outcome or response of an intervention. It is the “effect.” Last week we considered whether marriage A causes happiness Y.\nY(a) denotes the counterfactual or potential state of Y in response to setting the level of the exposure to a specific level, A=a. As we will consider in the second half of the course, to consistently estimate causal effects we will need to evaluate counterfactual or potential states of the world. Keeping to our example, we will need to do more than evaluate marriage and happiness in people over time. We will need to evaluate how happy the unmarried people would have been had they been married and how happy the married people would have been had they not been married. Of course, these events cannot be directly observed. Thus to address fundamental questions in psychology, we need to contrast counterfactual states of the world. This might seem like science fiction; however, we are already familiar with methods for obtaining such counterfactual contrasts – namely, randomised controlled experiments! We will return to this concept later, but for now, it will be useful for you to understand the notation.\nL denotes a measured confounder or set of confounders is defined as a variable which, if conditioned upon, closes an open back-door path between the treatment A and the outcome Y. Consider the scenario where happiness at time 0 (L) affects both the probability of getting married at time 1 (A) and one’s happiness at time 2 (Y). In this case, L serves as a confounder because it influences both the treatment (marriage at time 1) and the outcome (happiness at time 2), potentially opening a back-door path that confounds the estimated effect of marriage on happiness.\nTo accurately estimate the causal effect of marriage on happiness, then, it is essential to control for L. With cross-sectional data, such control might be difficult.\nU denotes an unmeasured confounder – that is a variable that may affect both the treatment and the outcome, but for which we have no direct measurement. Suppose cultural upbringing affects both whether someone gets married and whether they are happy. If this variable is not measured, we cannot accurately estimate a causal effect of marriage on happiness.\nM denotes a mediator or a variable along the path from exposure to outcome. For example, perhaps marriage causes wealth and wealth causes happiness. As we shall see, conditioning on “wealth” when estimating the effect of marriage on happiness will make it seem that marriage does not cause happiness when it does, through wealth.\n\\bar{X} denotes a sequence of variables, for example, a sequence of treatments. Imagine we were interested in the causal effect of marriage and remarriage on well-being. In this case, there are two treatments A_0 and A_1 and four potential contrasts. For the scenario of marriage and remarriage affecting well-being, we denote the potential outcomes as Y(a_0, a_1), where a_0 and a_1 represent the specific values taken by A_0 and A_1, respectively. Given two treatments, A_0 and A_1, four primary contrasts of interest correspond to the different combinations of these treatments. These contrasts allow us to compare the causal effects of being married versus not and remarried versus not on well-being. The potential outcomes under these conditions can be specified as follows:\n\nY(0, 0): The potential outcome when there is no marriage.\nY(0, 1): The potential outcome when there is marriage.\nY(1, 0): The potential outcome when there is divorce.\nY(1, 1): The potential outcome from marriage prevalence.\n\nEach of these outcomes allows for a specific contrast to be made, comparing the well-being under different scenarios of marriage and remarriage. Which do we want to contrast? Note, the question about ‘the causal effects of marriage on happiness’ is ambiguous because we have not stated the causal contrast we are interested in.\n\\mathcal{R} denotes a randomisation or a chance event.\n\n\nElements of our Causal Graphs\nThe conventions that describe components of our causal graphs are given in Figure 2.\n\n\n\n\n\n\nFigure 2: Nodes, Edges, Conditioning Conventions. This figure is adapted from (Bulbulia 2023)\n\nBulbulia, J. A. 2023. “Causal Diagrams (Directed Acyclic Graphs): A Practical Guide.”\n\n\n\n\nTime indexing\nIn our causal diagrams, we will implement two conventions to accurately depict the temporal order of events.\nFirst, the layout of a causal diagram will be structured from left to right to reflect the sequence of causality as it unfolds in reality. This orientation is crucial because causal diagrams must inherently be acyclic and because causality itself is inherently temporal.\nSecond, we will enhance the representation of the event sequence within our diagrams by systematically indexing our nodes according to the relative timing of events. If an event represented by X_0 precedes another event represented by X_1, the indexing will indicate this chronological order.\n\n\nRepresenting uncertainty in timing explicitly\nIn settings in which the sequence of events is ambiguous or cannot be definitively known, particularly in the context of cross-sectional data where all measurements are taken at a single point in time, we adopt a specific convention to express causality under uncertainty: X_{\\phi t}. This notation allows us to propose a temporal order without clear, time-specific measurements, acknowledging our speculation.\nFor instance, when the timing between events is unclear, we denote an event that is presumed to occur first as X_{\\phi 0} and a subsequent event as X_{\\phi 1}, indicating a tentative ordering where X_{\\phi 0} is thought to precede X_{\\phi 1}. However, it is essential to underscore that this notation signals our uncertainty regarding the actual timing of events; our measurements do not give us the confidence to assert this sequence definitively.\n\n\nArrows\nAs indicated in Figure 2, black arrows denote causality, red arrows reveal an open backdoor path, dashed black arrows denote attenuation, and red dashed arrows denote bias in a true causal association between A and Y. Finally, a blue arrow with a circle point denotes effect-measure modification, also known as “effect modification.” We might be interested in treatment effect heterogeneity without evaluating the causality in the sources of this heterogeneity. For example, we cannot typically imagine any intervention in which people could be randomised into cultures. However, we may be interested in whether the effects of an intervention that might be manipulable, such as marriage, differ by culture. To clarify this interest, we require a non-causal arrow.\n\\mathcal{R}\\to A denotes a random treatment assignment.\n\n\nBoxes\nWe use a black box to denote conditioning that reduces confounding or that is inert.\nWe use a red box to describe settings in which conditioning on a variable introduces confounding bias.\nOccasionally we will use a dashed circle do denote a latent variable, that is, a variable that is either not measured or not conditioned upon.\n\n\nTerminology for Conditional Independence\nThe bottom panel of Figure 2 shows some mathematical notation. Do not be alarmed, we are safe! Part 1 of the course will not require more complicated math than this notation. And we shall see that the notation is a compact way to describe intuitions that can be expressed less compactly in words:\n\nStatistical Independence (\\coprod): in the context of causal inference, statistical independence between the treatment and potential outcomes, denoted as A \\coprod Y(a), means the treatment assignment is independent of the potential outcomes. This assumption is critical for estimating causal effects without bias.\nStatistical Dependence (\\cancel\\coprod): conversely, \\cancel\\coprod denotes statistical dependence, indicating that the distribution of one variable is influenced by the other. For example, A \\cancel\\coprod Y(a) implies that the treatment assignment is related to the potential outcomes, potentially introducing bias into causal estimates.\nConditioning (|): conditioning, denoted by the vertical line |, allows for specifying contexts or conditions under which independence or dependence holds.\n\nConditional Independence (A \\coprod Y(a)|L): This means that once we account for a set of variables L, the treatment and potential outcomes are independent. This condition is often the basis for strategies aiming to control for confounding.\nConditional Dependence (A \\cancel\\coprod Y(a)|L): States that potential outcomes and treatments are not independent after conditioning on L, indicating a need for careful consideration in the analysis to avoid biased causal inferences."
  },
  {
    "objectID": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-regression",
    "href": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-regression",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Lab – Regression in R: Simulate The Five Fundamental Structures of Regression",
    "text": "Lab – Regression in R: Simulate The Five Fundamental Structures of Regression"
  },
  {
    "objectID": "content/02-content.html#part-1-seminar",
    "href": "content/02-content.html#part-1-seminar",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Part 1: Seminar",
    "text": "Part 1: Seminar\n\nOverview\n\nUnderstand basic features of causal diagrams: definitions and applications\nIntroduction to the five elementary causal structures\nLab: Gentle Introduction to Simulation and Regression (TBC over the upcoming weeks)\n\n\n\nReview\n\nPsychological research begins with two questions:\n\n\n\nWhat do I want to know?\nFor which population does this knowledge generalise?\n\n\nThis course considers how to ask psychological questions that pertain to populations with different characteristics\n\nIn psychological research, we typically ask questions about the causes and consequences of thought and behaviour - “What if?” questions (Hernan and Robins 2024).\nThe following concepts help us to describe two distinct failure modes in psychological research when asking “What if?” questions:\n\n\nThe Concept of External Validity: the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods. That is, we want to know if our findings carry beyond the sample population to the target population. We fail when our results do not generalise as we think. More fundamentally, we fail when we have not clearly defined our question or our target population.\nThe Concept of Internal Validity: the extent to which the associations we obtain from data reflect causality. In psychological science, we use “independent variable” and “dependent variable.” Sometimes we use the terms “exogenous variable” and “endogenous variable.” Sometimes we use the term “predictor variable” to describe the “dependent” or “endogenous” variable. These words are confusing. When asking “What if?” questions, we want to understand what would happen if we intervened. In this course, we will use the term “treatment” or, equivalently the term “exposure” to denote the intervention; we will use the term “outcome” to denote the effect of an intervention.1\n\n\nDuring the first part of the course, our primary focus will be on challenges to internal validity from confounding bias."
  },
  {
    "objectID": "content/02-content.html#definitions",
    "href": "content/02-content.html#definitions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Definitions",
    "text": "Definitions\n\nDefinition 1 We say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as defined at baseline.\n\n\nDefinition 2 We say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as defined at baseline.\n\nThe concept of “confounding bias” helps to clarify what it is at stake when evaluating the internal validity of a study. As we shall see, there are several equivalent definitions of “confounding bias,” which we will describe during the upcoming weeks.\nThe definition of confounding bias that we will examine today is:\n\nDefinition 3 We say there is confounding bias if there is an open back-door path between the treatment and outcome or if the path between the treatment and outcome is blocked.\n\nToday, our purpose will be to clarify the meaning of each term in this definition. To that end, we will introduce the five elementary graphical structures employed in causal diagrams. We will then explain the four elementary rules that allow investigators to identify causal effects from the asserted relations in a causal diagram. First, what are causal diagrams?"
  },
  {
    "objectID": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-causality",
    "href": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-causality",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Lab – Regression in R: Simulate The Five Fundamental Structures of Causality",
    "text": "Lab – Regression in R: Simulate The Five Fundamental Structures of Causality"
  },
  {
    "objectID": "content/02-content.html#excercise-1",
    "href": "content/02-content.html#excercise-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Excercise 1",
    "text": "Excercise 1\n\nModify the simulation parameters to change each group’s mean and standard deviation. Observe how these changes affect the distribution.\nGo to the histogram. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/02-content.html#appendix-a-how-anova-can-deliever-the-functionality-of-linear-regressions",
    "href": "content/02-content.html#appendix-a-how-anova-can-deliever-the-functionality-of-linear-regressions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix A: How ANOVA can deliever the functionality of Linear Regressions",
    "text": "Appendix A: How ANOVA can deliever the functionality of Linear Regressions\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#exercise-2-linear-regression-analysis-with-simulated-data",
    "href": "content/02-content.html#exercise-2-linear-regression-analysis-with-simulated-data",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Exercise 2: Linear Regression Analysis with Simulated Data",
    "text": "Exercise 2: Linear Regression Analysis with Simulated Data\n\nTask 1: Simulating Continuous Treatment Variable\n\n# library for enhanced model reporting\nlibrary(parameters)\n\n# set seed for reproducibility\nset.seed(123) # choose a seed number for consistency\n\n# define the number of observations\nn &lt;- 100 # total observations\n\n# simulate continuous treatment variable A\na &lt;- rnorm(n, mean = 50, sd = 10) # mean = 50, sd = 10 for A\n\n# specify the effect size of A on Y\nbeta_a &lt;- 2 # explicit effect size\n\n# simulate outcome variable Y including an error term\ny &lt;- 5 + beta_a * a + rnorm(n, mean = 0, sd = 20) # Y = intercept + beta_a*A + error\n\n# create a dataframe\ndf &lt;- data.frame(a = a, y = y)\n\n# view the structure and first few rows of the data frame\nstr(df)\nhead(df)\n\n\n\nTask 2: Exploring the Simulated Data\nBefore moving on to regression analysis, ensure students understand the structure and distribution of the simulated data. Encourage them to use summary(), plot(), and other exploratory data analysis functions.\n\n\nTask 3: Regression Analysis of Continuous Treatment Effect\n\n# perform linear regression of Y on A\nfit &lt;- lm(y ~ a, data = df)\n\n# display the regression model summary\nsummary(fit)\n\n# report the model in a reader-friendly format\nreport_fit &lt;- report::report(fit)\nprint(report_fit)\n\n# optionally, visualize the relationship\nplot(df$a, df$y, main = \"Scatterplot of Y vs. A\", xlab = \"Treatment (A)\", ylab = \"Outcome (Y)\")\nabline(fit, col = \"red\")"
  },
  {
    "objectID": "content/02-content.html#appendix-b-how-anova-can-deliever-the-functionality-of-linear-regressions",
    "href": "content/02-content.html#appendix-b-how-anova-can-deliever-the-functionality-of-linear-regressions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix B: How ANOVA can deliever the functionality of Linear Regressions",
    "text": "Appendix B: How ANOVA can deliever the functionality of Linear Regressions\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n           diff        lwr        upr     p adj\n2-1  -0.8030143  -9.560281   7.954253 0.9739971\n3-1 117.6732276 108.915961 126.430495 0.0000000\n3-2 118.4762419 109.718975 127.233509 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#appendix-c-glossary",
    "href": "content/02-content.html#appendix-c-glossary",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix C: Glossary",
    "text": "Appendix C: Glossary\nGlossary of key terms in causal inference"
  },
  {
    "objectID": "content/02-content.html#part-1-seminar-1",
    "href": "content/02-content.html#part-1-seminar-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Part 1: Seminar",
    "text": "Part 1: Seminar\n\nOverview\n\nUnderstand basic features of causal diagrams: definitions and applications\nIntroduction to the five elementary causal structures\nLab: Simulation and Regression\n\n\n\nReview\n\nPsychological research begins with two questions:\n\n\n\nWhat do I want to know?\nFor which population does this knowledge generalise?\n\n\nThis course considers how to ask this psychological questions that pertain to populations with different characteristics\n\nIn psychological research, we typically ask questions about the causes and consequences of thought and behaviour - “What if?” questions (Hernan and Robins 2024).\nThe following concepts help us to describe two distinct failure modes in psychological research when asking “What if?” questions:\n\n\nThe Concept of External Validity: the extent to which the findings of a study can be generalised to other situations, people, settings, and time periods. That is, we want to know if our findings carry beyond the sample population to the target population. We fail when our results do not generalise as we think. More fundamental, we fail when we have not clearly defined our question or our target population.\nThe Concept of Internal Validity: the extent to which the associations we obtain from data reflect causality. In psychological science, we use the terms “independent variable” and “dependent variable.” Sometimes we use the terms “exogenous variable” and “endogenous variable.” Sometimes we use the term “predictor variable” to describe the “dependent” or “endogenous” variable. These words are confusing. When asking “What if?” questions, we want to understand what would happen were we to intervene. In this course, we will use the term “treatment” or equivalently the term “exposure” to denote the intervention; we will use the term “outcome” to denote the effect of an intervention.2\n\n\nDuring the first part of the course, our primary focus will be on challenges to internal validity from confounding bias."
  },
  {
    "objectID": "content/02-content.html#definitions-1",
    "href": "content/02-content.html#definitions-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Definitions",
    "text": "Definitions\n\nDefinition 4 We say internal validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the sample population as it is defined at baseline.\n\n\nDefinition 5 We say external validity is compromised if the association between the treatment and outcome in a study does not consistently reflect causality in the target population as it is defined at baseline.\n\nThe concept of “confounding bias” helps to clarify what it is at stake when evaluating the internal validity of a study. As we shall see, there are several equivalent definitions of “confounding bias,” which we will describe during the upcoming weeks.\nThe definition of confounding bias that we will examine today is:\n\nDefinition 6 We say there is confounding-bias if there is an open back-door path between the treatment and outcome, or if that path between the treatment and outcome is blocked.\n\nToday, our purpose will be to clarify the meaning of each term in this definition. To that end, we will introduce the five elementary graphical structures and explain the four elementary rules that allow investigators to to move from the causal assumptions encoded in a causal diagram to an assessment of the challenges and opportunities for obtaining internal validity from data."
  },
  {
    "objectID": "content/02-content.html#introduction-to-causal-diagrams.-1",
    "href": "content/02-content.html#introduction-to-causal-diagrams.-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Introduction to Causal Diagrams.",
    "text": "Introduction to Causal Diagrams.\nCausal diagrams, also called causal graphs, Directed Acyclic Graphs, and Causal Directed Acyclic Graphs, are graphical tools whose primary purpose is to enable investigators to detect confounding biases.\nTheir primary purpose is to equip investigators with strategies for addressing the demands of internal validity: the associations we obtain in our analysis consistently estimate causal associations for the population of interest. We will assume that the population of interest – the target population – is the sample population at baseline. We do this to better distinguish between the concepts of internal and external validity. Later, we will relax this assumption.\nWe begin with by stating variable naming conventions that we will use throughout this course.\n\n\n\n\n\n\nFigure 5: Variable naming conventions\n\n\n\nThe symbol X generically denotes a variable, without reference to its role.\nA denotes the “treatment” or “exposure” variable. This is the variable for which we seek to understand the effect of intervening on it. It is the “cause.”\nY denotes the outcome or response of an intervention. It is the “effect”\nY(a) denotes the counterfactual or potential state of the outcome Y in response to setting the level of the exposure to a specific level, A=a. As we will consider in the second half of the course, to consistently estimate causal effects we will need to evaluate counterfactual or potential states of the world. Don’t worry about them for now – all will become clear!\nL denotes a measured confounder or set of confounders is defined as a variable which, if conditioned upon, closes an open back-door path between the treatment A and the outcome Y.\n\n\n\n\n\n\nFigure 6: Nodes, Edges, Conditioning Conventions\n\n\n\n\n\n\n\n\n\nFigure 7: Five elementary structures\n\n\n\n\n\n\n\n\n\nFigure 8: Four rules of confounding control"
  },
  {
    "objectID": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-causality-1",
    "href": "content/02-content.html#lab-regression-in-r-simulate-the-five-fundamental-structures-of-causality-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Lab – Regression in R: Simulate The Five Fundamental Structures of Causality",
    "text": "Lab – Regression in R: Simulate The Five Fundamental Structures of Causality"
  },
  {
    "objectID": "content/02-content.html#simulating-data-in-r-outcome-treatment-1",
    "href": "content/02-content.html#simulating-data-in-r-outcome-treatment-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating Data in R: Outcome ~ Treatment",
    "text": "Simulating Data in R: Outcome ~ Treatment\n\nStep 1: Set Up Your R Environment\nEnsure R or RStudio is installed and open.\n\n\nStep 2: Set a Seed for Reproducibility\nTo ensure that your simulated data can be reproduced exactly. Again, it is good practice to set a seed before generating random data. This makes your analyses and simulations replicable.\nquarto-executable-code-5450563D\nset.seed(123) # use any number to set the seed\n\n\nStep 3: Simulate Continuous Data: One Variable\nTo simulate continuous data, you can use functions like rnorm() for normal distributions, runif() for uniform distributions, etc. Here we simulate 100 normally distributed data points with a mean of 50 and a standard deviation of 10:\nquarto-executable-code-5450563D\nn &lt;- 100 # number of observations\nmean &lt;- 50\nsd &lt;- 10\ndata_continuous &lt;- rnorm(n, mean, sd)\n\n# view\nhead(data_continuous)\n\n# view\nhist(data_continuous)\n\n\nStep 4: Simulating Categorical Data\nCategorical data can be simulated using the sample() function. Here, we simulate a binary variable (gender) with two levels for 100 observations. There is equal probability of assignment.\nquarto-executable-code-5450563D\nlevels &lt;- c(\"Male\", \"Female\")\ndata_categorical &lt;- sample(levels, n, replace = TRUE)\n\n# view\nhead(data_categorical)\nTo generate categories with unequal probabilities, you can use the sample() function by specifying the prob parameter, which defines the probability of selecting each level. This allows for simulating categorical data where the distribution between categories is not uniform.\nBelow is an example that modifies your initial code to create a categorical variable with unequal probabilities for “Male” and “Female”. Here is an example with unequal probabilities:\nquarto-executable-code-5450563D\n#| echo: true\n#| eval: false\n\n# Define levels and number of observations\nlevels &lt;- c(\"Male\", \"Female\")\nn &lt;- 100 # total number of observations\n\n# Generate categorical data with unequal probabilities\ndata_categorical_unequal &lt;- sample(levels, n, replace = TRUE, prob = c(0.3, 0.7))\n\n# View the first few elements\nhead(data_categorical_unequal)\nIn this example, the prob parameter is set to c(0.3, 0.7), indicating a 30% probability for “Male” and a 70% probability for “Female”. This results in a simulated dataset where approximately 30% of the observations are “Male” and 70% are “Female”, reflecting the specified unequal probabilities. Adjust the probabilities as needed to fit the scenario you wish to simulate.\nset.seed(123) #  reproducibility\ngroupA_scores &lt;- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores &lt;- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\nscores_df &lt;- data.frame(Group = rep(c(\"A\", \"B\"), each = 100), Scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\nstr(scores_df)\n\n# summary of columns\nsummary(scores_df)\n\n# top rows\nhead(scores_df)\n\n# bottom rows\ntail(scores_df)"
  },
  {
    "objectID": "content/02-content.html#visualising-simulated-data-1",
    "href": "content/02-content.html#visualising-simulated-data-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Visualising simulated data",
    "text": "Visualising simulated data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n```{r visualize-data, fig.cap=“Score Distribution by Group”} if (!require(ggplot2)) { install.packages(“ggplot2”) library(ggplot2) } else { library(ggplot2) }"
  },
  {
    "objectID": "content/02-content.html#excercise-1-1",
    "href": "content/02-content.html#excercise-1-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Excercise 1",
    "text": "Excercise 1\n\nModify the simulation parameters to change each group’s mean and standard deviation. Observe how these changes affect the distribution.\nGo to the histogram. Experiment with different bin widths. In your own words, how do large and small numbers speak differently to the data? When might you use one histogram and not another."
  },
  {
    "objectID": "content/02-content.html#simulating-data-for-familiar-statistical-tests-1",
    "href": "content/02-content.html#simulating-data-for-familiar-statistical-tests-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating data for familiar statistical tests",
    "text": "Simulating data for familiar statistical tests\nquarto-executable-code-5450563D\n# simulate some data\ndata &lt;- rnorm(100, mean = 5, sd = 1) # 100 random normal values with mean = 5\n\n# perform one-sample t-test\n# testing if the mean of the data is reliably different from 4\nt.test(data, mu = 4)\nquarto-executable-code-5450563D\n# simulate data for two groups\ngroup1 &lt;- rnorm(50, mean = 5, sd = 1) # 50 random normal values, mean = 5\ngroup2 &lt;- rnorm(50, mean = 5.5, sd = 1) # 50 random normal values, mean = 5.5\n\n# two-sample t-test\nt.test(group1, group2)\nquarto-executable-code-5450563D\n# simulate pre-test and post-test scores\npre_test &lt;- rnorm(30, mean = 80, sd = 10)\npost_test &lt;- rnorm(30, mean =  pre_test + 5, sd = 5) # assume an increase\n\n# perform paired t-test\nt.test(pre_test, post_test, paired = TRUE)"
  },
  {
    "objectID": "content/02-content.html#exercise-2-linear-regression-analysis-with-simulated-data-1",
    "href": "content/02-content.html#exercise-2-linear-regression-analysis-with-simulated-data-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Exercise 2: Linear Regression Analysis with Simulated Data",
    "text": "Exercise 2: Linear Regression Analysis with Simulated Data\n\nTask 1: Simulating Continuous Treatment Variable\nquarto-executable-code-5450563D\n#| echo: true\n#| eval: false\n\n# library for enhanced model reporting\nlibrary(parameters)\n\n# set seed for reproducibility\nset.seed(123) # choose a seed number for consistency\n\n# define the number of observations\nn &lt;- 100 # total observations\n\n# simulate continuous treatment variable A\na &lt;- rnorm(n, mean = 50, sd = 10) # mean = 50, sd = 10 for A\n\n# specify the effect size of A on Y\nbeta_a &lt;- 2 # explicit effect size\n\n# simulate outcome variable Y including an error term\ny &lt;- 5 + beta_a * a + rnorm(n, mean = 0, sd = 20) # Y = intercept + beta_a*A + error\n\n# create a dataframe\ndf &lt;- data.frame(a = a, y = y)\n\n# view the structure and first few rows of the dataframe\nstr(df)\nhead(df)\n\n\nTask 2: Exploring the Simulated Data\nBefore moving on to regression analysis, ensure students understand the structure and distribution of the simulated data. Encourage them to use summary(), plot(), and other exploratory data analysis functions.\n\n\nTask 3: Regression Analysis of Continuous Treatment Effect\nquarto-executable-code-5450563D\n#| echo: true\n#| eval: false\n\n# perform linear regression of Y on A\nfit &lt;- lm(y ~ a, data = df)\n\n# display the regression model summary\nsummary(fit)\n\n# report the model in a reader-friendly format\nreport_fit &lt;- report::report(fit)\nprint(report_fit)\n\n# optionally, visualize the relationship\nplot(df$a, df$y, main = \"Scatterplot of Y vs. A\", xlab = \"Treatment (A)\", ylab = \"Outcome (Y)\")\nabline(fit, col = \"red\")"
  },
  {
    "objectID": "content/02-content.html#equivalence-of-anova-and-regression-1",
    "href": "content/02-content.html#equivalence-of-anova-and-regression-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Equivalence of ANOVA and Regression",
    "text": "Equivalence of ANOVA and Regression\nWe will simulate data in R to show that a one-way ANOVA is a special case of linear regression with categorical predictors. We will give some reasons for preferring regression (in some settings)."
  },
  {
    "objectID": "content/02-content.html#method-1",
    "href": "content/02-content.html#method-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Method",
    "text": "Method\nFirst, we simulate a dataset with one categorical independent variable with three levels (groups) and a continuous outcome (also called a “dependant”) variable. This setup allows us to apply both ANOVA and linear regression for comparison.\nquarto-executable-code-5450563D\n# nice tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n\nset.seed(321) # reproducibility\nn &lt;- 90 # total number of observations\nk &lt;- 3 # number of groups\n\n# simulate independent variable (grouping factor)\ngroup &lt;- factor(rep(1:k, each = n/k))\n\n# inspect\nstr(group)\n\n# simulate outcome variable\nmeans &lt;- c(100, 100, 220) # Mean for each group\nsd &lt;- 15 # Standard deviation (same for all groups)\n\n# generate random data\ny &lt;- rnorm(n, mean = rep(means, each = n/k), sd = sd)\n\n\n# make data frame\ndf_1 &lt;- cbind.data.frame(y, group)\n\nanova_model &lt;- aov(y ~ group, data = df_1)\n# summary(anova_model)\ntable_anova &lt;- model_parameters(anova_model)\n\n# report the model\nreport::report(anova_model)\nNext, we analyse the same data using linear regression. In R, regression models automatically convert categorical variables into dummy variables.\nquarto-executable-code-5450563D\n# for tables (just installed)\nlibrary(parameters)\n\n# regression model \nfit &lt;- lm(y ~ group, data = df_1)\n\n# uncomment if you want an ordinary summary\n# summary(regression_model)\n\ntable_fit &lt;- parameters::model_parameters(fit)\n\n# print table\ntable_fit\nquarto-executable-code-5450563D\nlibrary(parameters)\nlibrary(report)\n\n# report the model\nreport_fit &lt;- report_parameters(fit)\n\n#print\nreport_fit"
  },
  {
    "objectID": "content/02-content.html#upshot-1",
    "href": "content/02-content.html#upshot-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Upshot",
    "text": "Upshot\nANOVA partitions variance into between-group and within-group components, while regression models the mean of the dependent variable as a linear function of the independent (including categorical) variables. For many questions, ANOVA is appropriate, however, when we are comparing groups, we often want a finer-grained interpretation. Regression is built for obtaining this finer grain understanding. We will return to regression over the next few weeks and use regression to hone your skills in R. Later, Along the way, you’ll learn more about data visualisation, modelling, and reporting.\nquarto-executable-code-5450563D\n# graph the output of the parameters table\n# visualisation\nplot(table_fit)\n\nExercise 2\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\nSimulate Data:\n\nGenerate two continuous variables, Y and A, with n = 100 observations each.\nThe variable A should have a mean of 50 and a standard deviation (sd) of 10.\n\nDefine the Relationship:\n\nSimulate the variable Y such that it is linearly related to A with a specified effect size. The effect size of A on Y must be explicitly defined as 2.\n\nIncorporate an Error Term:\n\nWhen simulating Y, include an error term with a standard deviation (sd) of 20 to introduce variability.\n\nRegression Analysis:\n\nUse the lm() function in R to regress Y on A.\nEnsure the regression model captures the specified effect of A on Y.\n\nReport the Results:\n\nOutput the regression model summary to examine the coefficients, including the effect of A on Y, and assess the model’s overall fit and significance.\n\n\nHere is a template to get you started. Copy the code and paste it into your R script.\nquarto-executable-code-5450563D\n#| echo: true\n#| eval: false\n\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn &lt;-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A &lt;-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)\n\nStep 5: Simulating Data Frames\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\nquarto-executable-code-5450563D\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame &lt;- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\nNote that you can sample probabilistically for your groups\nquarto-executable-code-5450563D\nn &lt;- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))"
  },
  {
    "objectID": "content/02-content.html#more-complexity-1",
    "href": "content/02-content.html#more-complexity-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "More complexity",
    "text": "More complexity\nquarto-executable-code-5450563D\n# set the number of observations\nn &lt;- 100\n\n# simulate the 'Age' variable\nmean_age &lt;- 30\nsd_age &lt;- 5\nAge &lt;- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept &lt;- 20000   # Intercept for the income equation\nbeta_age &lt;- 1500     # Coefficient for the effect of age on income\nerror_sd &lt;- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome &lt;- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex &lt;- data.frame(Age, Income)\n\nStep 7: Visualising Simulated Data\nVisualising your simulated data can help understand its distribution and relationships. Use the ggplot2 package for this:\nquarto-executable-code-5450563D\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")"
  },
  {
    "objectID": "content/02-content.html#practice-1",
    "href": "content/02-content.html#practice-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Practice",
    "text": "Practice\nSimulating data is a powerful method to understand statistical concepts and data manipulation. Let’s simulate a simple dataset representing scores from two cultural groups.\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations!"
  },
  {
    "objectID": "content/02-content.html#what-you-have-learned-1",
    "href": "content/02-content.html#what-you-have-learned-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "What You Have Learned",
    "text": "What You Have Learned\n\nData simulation:\n\nYou’ve learned to simulate datasets in R. This is a foundational skill for exploring statistical concepts and data manipulation techniques. Congratulations!\n\nData visualisation:\n\nYou’ve begun data visualising data through boxplots and histograms and coefficient plots, which is crucial for analysing and communicating statistical findings.\n\nStatistical tests: You’ve conducted basic statistical tests, including t-tests and ANOVA, gaining insights into comparing means across groups.\nUnderstanding ANOVA and regression:\n\nYou’ve explored the equivalence of ANOVA and regression analysis, learning how these methods can be applied to analyse and interpret data effectively."
  },
  {
    "objectID": "content/02-content.html#for-more-information-about-the-packages-used-here-1",
    "href": "content/02-content.html#for-more-information-about-the-packages-used-here-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "For more information about the packages used here:",
    "text": "For more information about the packages used here:\n\nggplot2: A system for declaratively creating graphics, based on The Grammar of Graphics.\nParameters package: Provides utilities for processing model parameters and their metrics.\nReport package: Facilitates the automated generation of reports from statistical models."
  },
  {
    "objectID": "content/02-content.html#appendix-b-how-anova-can-deliever-the-functionality-of-linear-regressions-1",
    "href": "content/02-content.html#appendix-b-how-anova-can-deliever-the-functionality-of-linear-regressions-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix B: How ANOVA can deliever the functionality of Linear Regressions",
    "text": "Appendix B: How ANOVA can deliever the functionality of Linear Regressions\nWe can get group comparisons with ANOVA, for example:\nquarto-executable-code-5450563D\n# Conduct Tukey's HSD test for post-hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\nplot(tukey_post_hoc)\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#appendix-c-glossary-1",
    "href": "content/02-content.html#appendix-c-glossary-1",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix C: Glossary:",
    "text": "Appendix C: Glossary:\nCreating a table format for your comprehensive glossary in a markdown document, given the extensive list, it’s practical to focus on key terms for brevity and clarity. Here’s a condensed table representation:\n| Term                                   | Definition                                                                                                                                                   |\n|----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **Acyclic**                            | In causal diagrams, no variable can be an ancestor or descendant of itself; no feedback loops allowed.                                                       |\n| **Adjustment Set**                     | Variables conditioned to block all backdoor paths between treatment ($A$) and outcome ($Y$), ensuring unbiased causal estimates.                             |\n| **Ancestor (Parent)**                  | A node affecting others downstream in the causal chain, preceding its descendants in time.                                                                   |\n| **Arrow**                              | Represents direct causation in causal DAGs, pointing from cause to effect.                                                                                   |\n| **Average Treatment Effect (ATE)**     | The difference in expected outcomes between treated and untreated units across a population.                                                                 |\n| **Backdoor Path**                      | Paths introducing potential confounding bias between treatment and outcome, requiring blocking for unbiased estimation.                                      |\n| **Causal Contrast**                    | The difference in expected outcomes under different treatment levels across a population.                                                                    |\n| **Causal Contrast Scale**              | The metric for quantifying causal contrasts, chosen based on outcome type and research question.                                                             |\n| **Causal Diagram (Causal DAG)**        | Graphs representing causal relationships; must be acyclic and capture all confounding sources.                                                               |\n| **Causal Estimand**                    | The causal contrast of interest specifying intervention, outcome, contrast scale, and target population; given before analysis.                             |\n| **Causal Path**                        | Asserts a change in the parent node will induce a change in its child.                                                                                       |\n| **Censoring**                          | Incomplete data in longitudinal studies due to unobserved relevant events, sample attrition, including right, left, and interval censoring.                  |\n| **Collider**                           | A variable where two causal paths meet head-to-head, leading to non-causal associations between any unassociated parents.                                    |\n| **Conditional Average Treatment Effect (CATE)** | The treatment effect for specific subgroups, defined by explicit characteristics.                                                                          |\n| **Conditioning**                       | Adjusting for variables in analysis to distinguish causal effects from associations.                                                                         |\n| **Confounding**                        | Treatment and outcome are associated independently of causality or disassociated in the presence of causality.                                               |\n| **Confounder**                         | A variable that, when conditioned upon, reduces or eliminates confounding; its role is relative to the causal question and conditioning strategy.             |\n| **Counterfactual or Potential outcomes** | Hypothetical outcomes under different treatment conditions to be contrasted, only one may be realised for each observed unit.                              |\n| **d-separation**                       | Backdoor paths are blocked, indicating conditional independence.                                                                                             |\n| **Descendant (Child)**                 | A node causally influenced by a prior node (Parent). A child is a direct descendant.                                                                         |\n| **Effect-Measure Modifier/Effect-Modifier** | A variable that affects the magnitude or direction of a causal effect.                                                                                    |\n| **Estimator**                          | Algorithm to compute a statistical estimand from data.                                                                                                       |\n| **External Validity/Target Validity**  | The generalisability of study findings to the prespecified target population; assumes internal validity.                                                     |\n| **Heterogeneous Treatment Effects**    | Variation in treatment effects across subgroups or contexts.                                                                                                 |\n| **Identification Problem**             | Estimating causal effects accurately by adjusting for confounding.                                                                                           |\n| **Incident Exposure Effect**           | Describes the effect of initiating a new treatment.                                                                                                          |\n| **Indirect Effect (Mediated Effect)**  | The effect portion transmitted through a mediator in the causal pathway.                                                                                     |\n| **Instrumental Variable**              | Associated with treatment but affecting the outcome only through the treatment, used for estimating causal effects amidst confounding.                        |\n| **Intention-to-Treat Effect**          | The effect of treatment assignment, analysed based on initial treatment assignment, reflecting real-world effectiveness but possibly obscuring mechanisms.    |\n| **Internal Validity**                  | The extent to which causal associations in the study population are accurately identified.                                                                    |\n| **Inverse Probability of Censoring Weights (IPCW)** | Adjust for bias from attrition in longitudinal studies.                                                                       |\n| **Inverse Probability of Treatment Weights (IPTW)** | Creates a pseudo-population to obtain treatment balance across conditions.                                                                              |\n| **Loss-to-follow-up**                  | Participant attrition.                                                                                                                                       |\n| **Marginal effect**                    | Synonymous with Average Treatment Effect.                                                                                                                    |\n| **Mediator**                           | A variable through which a treatment affects an outcome.                                                                                                     |\n| **Node**                               | Represents a variable in a causal diagram\n:::"
  },
  {
    "objectID": "content/02-content.html#the-five-elementary-structures-of-causality",
    "href": "content/02-content.html#the-five-elementary-structures-of-causality",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Five Elementary Structures of Causality",
    "text": "The Five Elementary Structures of Causality\nJudea Pearl proved that all elementary structures of causality can be represented graphically (Pearl 2009). Figure 3 presents this five elementary structures.\n\nPearl, Judea. 2009. Causality. Cambridge University Press.\n\n\n\n\n\n\nFigure 3: Five elementary structures. This figure is adapted from (Bulbulia 2023).\n\nBulbulia, J. A. 2023. “Causal Diagrams (Directed Acyclic Graphs): A Practical Guide.”\n\n\n\nThe structures are as follows:\n\nTwo Variables:\n\nCausality Absent: There is no causal effect between variables A and B. They do not influence each other, denoted as A \\coprod B, indicating they are statistically independent.\nCausality: Variable A causally affects variable B. This relationship suggests an association between them, denoted as A \\cancel\\coprod B, indicating they are statistically dependent.\n\nThree Variables:\n\nFork: Variable A causally affects both B and C. Variables B and C are conditionally independent given A, denoted as B \\coprod C | A. This structure implies that knowing A removes any association between B and C due to their common cause.\nChain: A causal chain exists where C is affected by B, which in turn is affected by A. Variables A and C are conditionally independent given B, denoted as A \\coprod C | B. This indicates that B mediates the effect of A on C, and knowing B breaks the association between A and C.\nCollider: Variable C is affected by both A and B, which are independent. However, conditioning on C induces an association between A and B, denoted as A \\cancel\\coprod B | C. This structure is unique because it suggests that A and B, while initially independent, become associated when we account for their common effect C.\n\n\nOnce we understand the basic relationships between two variables, we can build upon these to create more complex relationships. These structures help us see how statistical independences and dependencies emerge from the data, allowing us to clarify the causal relationships we presume exist. Such clarity is crucial for ensuring that confounders are balanced across treatment groups, given all measured confounders, so that Y(a) \\coprod A | L.\nYou might wonder, “If not from the data, where do our assumptions about causality come from?” This question will come up repeatedly throughout the course. The short answer is that our assumptions are based on existing knowledge. This reliance on current knowledge might seem counterintuitive for buiding scientific knowledge-— shouldn’t we use data to build knowledge, not the other way around? Yes, but it is not that straightforward. Data often hold the answers we’re looking for but can be ambiguous. When the causal structure is unclear, it is important to sketch out different causal diagrams, explore their implications, and, if necessary, conduct separate analyses based on these diagrams.\nOtto Neurath, an Austrian philosopher and a member of the Vienna Circle, famously used the metaphor of a ship that must be rebuilt at sea to describe the process of scientific theory and knowledge development.\n\nDuhem has shown … that every statement about any happening is saturated with hypotheses of all sorts and that these in the end are derived from our whole world-view. We are like sailors who on the open sea must reconstruct their ship but are never able to start afresh from the bottom. Where a beam is taken away a new one must at once be put there, and for this the rest of the ship is used as support. In this way, by using the old beams and driftwood, the ship can be shaped entirely anew, but only by gradual reconstruction. (Neurath 1973, 199)\n\nNeurath, Otto. 1973. “Anti-Spengler.” In Empiricism and Sociology, edited by Marie Neurath and Robert S. Cohen, 158–213. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-94-010-2525-6_6.\n\nThis quotation emphasises the iterative process that accumulates scientific knowledge; new insights are cast from the foundation of existing knowledge. Causal diagrams are at home in Neurath’s boat. The tradition of science that believes that knowledge develops from the results of statistical tests applied to data should be resisted. The data alone typically do not contain the answers we seek."
  },
  {
    "objectID": "content/02-content.html#the-four-rules-of-confounding-control",
    "href": "content/02-content.html#the-four-rules-of-confounding-control",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "The Four Rules of Confounding Control",
    "text": "The Four Rules of Confounding Control\nFigure 4 describe the four elementary rules of confounding control:\n\n\n\n\n\n\nFigure 4: Four rules of confounding control\n\n\n\n\nCondition on Common Cause or its Proxy: this rule applies to settings in which the treatment (A) and the outcome (Y) share common causes. By conditioning on these common causes, we block the open backdoor paths that could introduce bias into our causal estimates. Controlling for these common causes (or their proxies) helps tp isolate the specific effect of A on Y. (We do not draw a path from $ A Y$ because we do not assume this path.)\nDo Not Condition on a Mediator: this rule applies to settings in which the variable L is a mediator of A \\to Y. Here, conditioning on a mediator will bias the total causal effect estimate. Later in the course, we will discuss the assumptions required for causal mediation. For now, if we are interested in total effect estimates, we must not condition on a mediator. Here we draw the path from A \\to Y to ensure that if such a path exists, it will not become biased from our conditioning strategy.\nDo Not Condition on a Collider: this rule applies to settings in which we L is a common effect of A and Y. Conditioning on a collider may invoke a spurious association. Last week we considered an example in which marriage caused wealth and happiness caused wealth. Conditioning on wealth in this setting will induce an association between happiness and marriage. Why? If we know the outcome, wealth, then we know there are at least two ways of wealth. Among those wealthy but low on happiness, we can predict that they are more likely to be married, for how else would they be wealthy? Similarly, among those who are wealthy and are not married, we can predict that they are happy, for how else would they be wealthy if not through marriage? These relationships are predictable entirely without a causal association between marriage and happiness!\nProxy Rule: Conditioning on a Descendent Is Akin to Conditioning on Its Parent: this rule applies to settings in which we L’ is an effect from another variable L. The graph considers when L’ is downstream of a collider. For example, suppose we condition on home ownership, which is an effect of wealth. Such conditioning will open up a non-causal path without causation because home ownership is a proxy for wealth. Consider, if someone owns a house but is not married, they are more likely to be happy, for how else could they accumulate the wealth required for home ownership? Likewise, if someone is unhappy and owns a house, we can infer that they are more likely to be married because how else would they be wealthy? Conditioning on a proxy for a collider here is akin to conditioning on the collider itself.\n\nHowever, we can also use the proxy rule to reduce bias. Return to the earlier example in which there is an unmeasured common cause of marriage and happiness, which we called “cultural upbringing” Suppose we have not measured this variable but have measured proxies for this variable, such as country of birth, childhood religion, number of languages one speaks, and others. By controlling for baseline values of these proxies, we can exert more control over unmeasured confounding. Even if bias is not eliminated, we should reduce bias wherever possible, which includes not introducing new biases, such as mediator bias, along the way. Later in the course, we will teach you how to perform sensitivity analyses to verify the robustness of your results to unmeasured confounding. Sensitivity analysis is critical because where the data are observational, we cannot entirely rule out unmeasured confounding."
  },
  {
    "objectID": "content/02-content.html#appendix-b-how-anova-can-deliver-the-functionality-of-linear-regressions",
    "href": "content/02-content.html#appendix-b-how-anova-can-deliver-the-functionality-of-linear-regressions",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix B: How ANOVA can deliver the functionality of Linear Regressions",
    "text": "Appendix B: How ANOVA can deliver the functionality of Linear Regressions\nWe can get group comparisons with ANOVA, for example:\n\n# Conduct Tukey's HSD test for post hoc comparisons\ntukey_post_hoc &lt;- TukeyHSD(anova_model)\n\n# Display the results\nprint(tukey_post_hoc)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ group, data = df_1)\n\n$group\n          diff        lwr       upr     p adj\n2-1   3.381631  -4.906624  11.66989 0.5958959\n3-1 121.072862 112.784607 129.36112 0.0000000\n3-2 117.691231 109.402975 125.97949 0.0000000\n\nplot(tukey_post_hoc)\n\n\n\n\n\n\n\n\nRegression and ANOVA are equivalent"
  },
  {
    "objectID": "content/02-content.html#lab-regression-in-r-graphing-and-elements-of-simulation",
    "href": "content/02-content.html#lab-regression-in-r-graphing-and-elements-of-simulation",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Lab – Regression in R, Graphing, and Elements of Simulation",
    "text": "Lab – Regression in R, Graphing, and Elements of Simulation"
  },
  {
    "objectID": "content/02-content.html#simulating-outcomes-from-treatments",
    "href": "content/02-content.html#simulating-outcomes-from-treatments",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Simulating Outcomes from Treatments",
    "text": "Simulating Outcomes from Treatments\nIn this example, the prob parameter is set to c(0.3, 0.7), indicating a 30% probability for “Male” and a 70% probability for “Female”. This results in a simulated dataset where approximately 30% of the observations are “Male” and 70% are “Female”, reflecting the specified unequal probabilities. Adjust the probabilities as needed to fit the scenario you wish to simulate.\n\n#|fig-cap: \"Box plot of simulated scores by groups.\"\n# note: running this code will do no harm if the libraries are already installed.\n# install (if necessary) libraries\n# load libraries\n# libraries\n# graphs\nif (!require(ggplot2)) {\n  install.packages(\"ggplot2\")\n  library(ggplot2)\n} else {\n  library(ggplot2)\n}\n\n# data wrangling\nif (!require(tidyverse)) {\n  install.packages(\"tidyverse\")\n  library(tidyverse)\n} else {\n  library(tidyverse)\n}\n\n# tables\nif (!require(parameters)) {\n  install.packages(\"parameters\")\n  library(parameters)\n} else {\n  library(parameters)\n}\n\n# reporting\nif (!require(report)) {\n  install.packages(\"report\")\n  library(report)\n} else {\n  library(report)\n}\n\n\n# predictive graphs\nif (!require(ggeffects)) {\n  install.packages(\"ggeffects\")\n  library(ggeffects)\n} else {\n  library(ggeffects)\n}\n\n# assembling graphs\nif (!require(patchwork)) {\n  install.packages(\"patchwork\")\n  library(patchwork)\n} else {\n  library(patchwork)\n}\n\n# if libraries are already install, start here\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(parameters)\nlibrary(report)\n\n\nset.seed(123) # reproducibility\ngroupA_scores &lt;- rnorm(100, mean = 100, sd = 15) # simulate scores for group A\ngroupB_scores &lt;- rnorm(100, mean = 105, sd = 15) # simulate scores for group B\n\n# ombine into a data frame\ndf_scores &lt;- data.frame(group = rep(c(\"A\", \"B\"), each = 100), scores = c(groupA_scores, groupB_scores))\n\n# commands to view data\n #str(df_scores)\n\n# summary of columns\n#summary(df_scores)\n\n# top rows (uncomment)\n# head(df_scores)\n\n# bottom rows  (uncomment)\n# tail(df_scores)\n\n# check structure of data  (uncomment)\n# str(df_scores)\n\n\n# make group a factor (not strictly necessary here, but useful in outher applications)\ndf_scores_1 &lt;- df_scores |&gt; \n  mutate(group = as.factor(group))\n\nhead(df_scores_1)\n\n  group    scores\n1     A  91.59287\n2     A  96.54734\n3     A 123.38062\n4     A 101.05763\n5     A 101.93932\n6     A 125.72597"
  },
  {
    "objectID": "content/02-content.html#visualising-our-simulated-data",
    "href": "content/02-content.html#visualising-our-simulated-data",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Visualising Our Simulated Data",
    "text": "Visualising Our Simulated Data\nUnderstanding your data visually is as important as the statistical analysis itself. Let’s create a simple plot to compare the score distributions between the two groups.\n\n# plot your data\nggplot(df_scores_1, aes(x = group, y = scores, fill = group)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Score Distribution by Group\", x = \"Group\", y = \"Scores\")\n\n\n\n\nScore Distribution by Group"
  },
  {
    "objectID": "content/02-content.html#understanding-regression-using-data-simulation",
    "href": "content/02-content.html#understanding-regression-using-data-simulation",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Understanding Regression Using Data Simulation",
    "text": "Understanding Regression Using Data Simulation\n\nSimulating Continuous Treatment Variable and Outcome Variable\n\n# library for enhanced model reporting\nset.seed(123)\n\nlibrary(parameters)\nlibrary(report)\n\n\n# set seed for reproducibility\nset.seed(123) # choose a seed number for consistency\n\n# define the number of observations\nn &lt;- 100 # total observations\n\n# simulate continuous treatment variable A\ntreatment &lt;- rnorm(n, mean = 50, sd = 10) # mean = 50, sd = 10 for A\n\n# specify the effect size of A on Y\nbeta_a &lt;- 2 # explicit effect size\n\n# simulate outcome variable Y including an error term\noutcome &lt;- 5 + beta_a * treatment + rnorm(n, mean = 0, sd = 20) # Y = intercept + beta_a*A + error\n\n# create a dataframe\ndf &lt;- data.frame(treatment = treatment,outcome = outcome)\n\n# view the structure and first few rows of the data frame\nstr(df)\n\n'data.frame':   100 obs. of  2 variables:\n $ treatment: num  44.4 47.7 65.6 50.7 51.3 ...\n $ outcome  : num  79.6 105.5 131.2 99.5 88.6 ...\n\nhead(df)\n\n  treatment   outcome\n1  44.39524  79.58236\n2  47.69823 105.53412\n3  65.58708 131.24033\n4  50.70508  99.45932\n5  51.29288  88.55338\n6  67.15065 138.40075\n\n\n\n\nRun Linear Model\nBefore moving on to regression analysis, ensure students understand the structure and distribution of the simulated data. Encourage them to use summary(), plot(), and other exploratory data analysis functions.\n\nRegression Analysis of Continuous Treatment Effect\n\nset.seed(123)\n\n# perform linear regression of Y on A\nfit &lt;- lm(outcome ~ treatment, data = df)\n\n# display the regression model summary\nsummary(fit)\n\n\nCall:\nlm(formula = outcome ~ treatment, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.15 -13.67  -1.75  11.61  65.81 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   8.1911    11.0530   0.741     0.46    \ntreatment     1.8951     0.2138   8.865  3.5e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.41 on 98 degrees of freedom\nMultiple R-squared:  0.4451,    Adjusted R-squared:  0.4394 \nF-statistic:  78.6 on 1 and 98 DF,  p-value: 3.497e-14\n\n# report the model in a reader-friendly format\nreport_fit &lt;- report::report(fit)\n\n\n# uncomment to print the model report\n# print(report_fit)\n\n# use ggeffects to view predicted values\nlibrary(ggeffects)\n\npredicted_values &lt;- ggeffects::ggemmeans(fit,\n                     terms = c(\"treatment\"))\n\n# plot see\nplot(predicted_values,  dot_alpha = 0.35,   show_data = TRUE, jitter = .1)\n\n\n\n\nGraph of regression line showing uncertainty."
  },
  {
    "objectID": "content/02-content.html#exercise-2",
    "href": "content/02-content.html#exercise-2",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Exercise 2",
    "text": "Exercise 2\nPerform a linear regression analysis using R. Follow the detailed instructions below to simulate the necessary data, execute the regression, and report your findings:\n\nSimulate Data:\n\nGenerate two continuous variables, Y and A, with n = 100 observations each.\nThe variable A should have a mean of 50 and a standard deviation (sd) of 10.\n\nDefine the Relationship:\n\nSimulate the variable Y such that it is linearly related to A with a specified effect size. The effect size of A on Y must be explicitly defined as 2.\n\nIncorporate an Error Term:\n\nWhen simulating Y, include an error term with a standard deviation (sd) of 20 to introduce variability.\n\nRegression Analysis:\n\nUse the lm() function in R to regress Y on A.\nEnsure the regression model captures the specified effect of A on Y.\n\nReport the Results:\n\nOutput the regression model summary to examine the coefficients, including the effect of A on Y, and assess the model’s overall fit and significance.\n\n\nHere is a template to get you started. Copy the code and paste it into your R script.\n\nlibrary(parameters)\n#  seed for reproducibility\nset.seed( ) # numbers go in brackets\n\n# number of observations\nn &lt;-   # number goes here\n\n# simulate data for variable A with specified mean and sd\nA &lt;- rnorm(n, \n           mean = , # set your number here \n           sd = )# set your number here \n\n# define the specified effect size of A on Y\nbeta_A &lt;-   # define your effect with a number here \n\n\n# simulate data and make data frame in one step\n\ndf_3 &lt;- data.frame(\n  # simulate data for variable A with specified mean and sd\n  A = A, # from above\n  Y = 5 + beta_A * A + rnorm(n, mean = 0, sd = 20) #  effect is intercept + ...\n)\n\n# view\nhead(df_3)\nstr(df_3)\n\n#  linear regression of Y on A\nfit_3 &lt;- lm(Y ~ A, data = df_3)\n\n#  results (standard code)\n# summary(model)\n\n# time saving reports\nparameters::model_parameters(fit_3)\nreport(fit_3)"
  },
  {
    "objectID": "content/02-content.html#appendix-c-adding-complexity-in-simulation",
    "href": "content/02-content.html#appendix-c-adding-complexity-in-simulation",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix C: Adding Complexity in Simulation",
    "text": "Appendix C: Adding Complexity in Simulation\n\nSimulating Data… Continued\nData frames are used in R to store data tables. To simulate a dataset with both continuous and categorical data, you can combine the above steps:\n\n# create a data frame with simulated data for ID, Gender, Age, and Income\ndata_frame &lt;- data.frame(\n  # generate a sequence of IDs from 1 to n\n  ID = 1:n,\n  \n  # randomly assign 'Male' or 'Female' to each observation\n  Gender = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  \n  # simulate 'Age' data: normally distributed with mean 30 and sd 5\n  Age = rnorm(n, mean = 30, sd = 5),\n  \n  # simulate 'Income' data: normally distributed with mean 50000 and sd 10000\n  Income = rnorm(n, mean = 50000, sd = 10000)\n)\n\nNote that you can sample probabilistically for your groups\n\nn &lt;- 100 # total number of observations\n\n# sample 'Gender' with a 40/60 proportion for Male/Female\nGender = sample(c(\"Male\", \"Female\"), n, replace = TRUE, prob = c(0.4, 0.6))\n\n\n\nMore complexity\n\n# set the number of observations\nn &lt;- 100\n\n# simulate the 'Age' variable\nmean_age &lt;- 30\nsd_age &lt;- 5\nAge &lt;- rnorm(n, mean = mean_age, sd = sd_age)\n\n# define coefficients explicitly\nintercept &lt;- 20000   # Intercept for the income equation\nbeta_age &lt;- 1500     # Coefficient for the effect of age on income\nerror_sd &lt;- 10000    # Standard deviation of the error term\n\n# simulate 'Income' based on 'Age' and defined coefficients\nIncome &lt;- intercept + beta_age * Age + rnorm(n, mean = 0, sd = error_sd)\n\n# create a data frame to hold the simulated data\ndata_complex &lt;- data.frame(Age, Income)\n\n\n\nVisualising Simulated Data\nVisualising your simulated data can help understand its distribution and relationships. Use the ggplot2 package for this:\n\nlibrary(ggplot2)\nggplot(data_complex, aes(x = Age, y = Income)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Simulated Age vs. Income\", x = \"Age\", y = \"Income\")"
  },
  {
    "objectID": "content/02-content.html#appendix-d-causal-inference-glossary",
    "href": "content/02-content.html#appendix-d-causal-inference-glossary",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Appendix D: Causal Inference Glossary",
    "text": "Appendix D: Causal Inference Glossary\nNote, as of yet, we have only encountered several of the terms in this glossary.\nGlossary of key terms in causal inference"
  },
  {
    "objectID": "content/02-content.html#combination-plots",
    "href": "content/02-content.html#combination-plots",
    "title": "Causal diagrams: Five Elementary Structures",
    "section": "Combination Plots",
    "text": "Combination Plots\nWe can create and combine individual plots, as showin in Figure 5, as follows:\n\n# regression --------------------------------------------------------------\n# graph the output of the parameters table and assign to object\ncoefficient_plot &lt;- plot(table_fit)\n\n\n# ggeffects plot - create predictive plot.\n# this gives the expected values by group \npredictive_plot &lt;- plot(\n  ggeffects::ggpredict(fit_regression, terms = \"group\"),\n  dot_alpha = 0.35,\n  show_data = TRUE,\n  jitter = .1,\n  colors =  \"reefs\"\n) +\n  scale_y_continuous(limits = c(0, 260)) + # change y axis\n  labs(title = \"Predictive Graph\", x = \"Treatment Group\", y = \"Response\")\n\n# view (uncomment to see it alone)\n#predictive_plot\n\n# show all color palettes (uncomment to see colour options)\n# show_pals()\n\n\n# multiple plots using `patchwork`\nlibrary(patchwork)\n\n# create a plot\nmy_first_combination_plot &lt;- coefficient_plot / predictive_plot  +\n  plot_annotation(title = \"Coefficient and Predictive plots with two panels \",\n                  tag_levels = \"A\")\n\n# save a plot to your directory, make sure to create one called \"figs\"\n\n## check directory (uncomments)\n# here::here()\n\n# save (change values if necessary )\nggsave(\n  my_first_combination_plot,\n  path = here::here(\"figs\"),\n  width = 12,\n  height = 8,\n  units = \"in\",\n  filename = \"my_first_combination_plot.jpeg\",\n  device = 'jpeg',\n  limitsize = FALSE,\n  dpi = 600\n)\n\n# view\nmy_first_combination_plot\n\n\n\n\n\n\n\nFigure 5: This is a combined plot"
  }
]